{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "drop_prob = 0.1\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "num_epochs = 8\n",
    "max_seq_length = 300\n",
    "ffn_hidden = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # if there is one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Number of threads PyTorch will use: 11\n"
     ]
    }
   ],
   "source": [
    "# Get the number of CPU cores\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "# Set PyTorch to use all available CPU threads\n",
    "torch.set_num_threads(num_cores)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set the number of threads explicitly\n",
    "print(f\"Number of threads PyTorch will use: {torch.get_num_threads()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden).to(device)  # Move layer to device\n",
    "        self.linear2 = nn.Linear(hidden, d_model).to(device)  # Move layer to device\n",
    "        self.relu = nn.ReLU().to(device)  # Move activation to device\n",
    "        self.dropout = nn.Dropout(p=drop_prob).to(device)  # Move dropout to device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)  # Move input to device\n",
    "        x = self.linear1(x)\n",
    "        # print(f\"x after first linear layer: {x.size()}\")\n",
    "        x = self.relu(x)\n",
    "        # print(f\"x after activation: {x.size()}\")\n",
    "        x = self.dropout(x)\n",
    "        # print(f\"x after dropout: {x.size()}\")\n",
    "        x = self.linear2(x)\n",
    "        # print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Compute the position encodings\n",
    "        even_i = torch.arange(0, d_model, 2).float().to(device)\n",
    "        denominator = torch.pow(10000, even_i / d_model).to(device)\n",
    "        position = torch.arange(max_seq_length).reshape(max_seq_length, 1).to(device)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2).to(device)\n",
    "        self.position_encoding = torch.flatten(stacked, start_dim=1, end_dim=2)  # Shape: [max_seq_length, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)  # Move input to device\n",
    "\n",
    "        # Get the batch size and sequence length\n",
    "        batch_size, seq_len, d_model = x.size(0), x.size(1), x.size(2)\n",
    "\n",
    "        # Dynamically slice the positional encoding\n",
    "        if seq_len > self.max_seq_length:\n",
    "            raise ValueError(f\"Sequence length ({seq_len}) exceeds max_seq_length ({self.max_seq_length})!\")\n",
    "        position_encoding = self.position_encoding[:seq_len, :]  # Shape: [seq_len, d_model]\n",
    "        #print(f\"Sliced position encoding shape: {position_encoding.shape}\")\n",
    "        \n",
    "        # Expand positional encoding to match batch size\n",
    "        position_encoding = position_encoding.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: [batch_size, seq_len, d_model]\n",
    "        #print(f\" Broadcasted position encoding shape: {position_encoding.shape}\")\n",
    "\n",
    "        # Add positional encoding to the input\n",
    "        output = x + position_encoding\n",
    "\n",
    "        #print(f\" Output tensor shape after addition: {output.shape}\")\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "\n",
    "    # Step 1: Compute the scaled dot-product (Q * K^T)\n",
    "    d_k = q.size(-1)  # This is the dimension of the keys (and queries)\n",
    "    scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # Step 2: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        # Adjust the mask shape\n",
    "        if mask.ndimension() == 4:\n",
    "            if mask.shape[1] == 1:  # Handle case like (batch_size, 1, seq_len_q, seq_len_k)\n",
    "                mask = mask.expand(-1, q.size(1), -1, -1)  # Broadcast to match num_heads\n",
    "            elif mask.shape == scaled.shape:\n",
    "                print(\"Mask is already correctly shaped for attention.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected 4D mask shape: {mask.shape}\")\n",
    "        elif mask.ndimension() == 2:\n",
    "            if mask.shape[0] == q.size(0):  # Padding mask: (batch_size, seq_len)\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)  # Shape: (batch_size, 1, 1, seq_len)\n",
    "                mask = mask.expand(-1, q.size(1), q.size(2), -1)  # Shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "            elif mask.shape[0] == mask.shape[1]:  # Causal mask: (seq_len, seq_len)\n",
    "                mask = mask.unsqueeze(0).unsqueeze(1)  # Shape: (1, 1, seq_len, seq_len)\n",
    "                mask = mask.expand(q.size(0), q.size(1), -1, -1)  # Shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected 2D mask shape: {mask.shape}\")\n",
    "        elif mask.ndimension() == 3:\n",
    "            # Explicit case for (batch_size, seq_len_q, seq_len_k)\n",
    "            mask = mask.unsqueeze(1)  # Shape: (batch_size, 1, seq_len_q, seq_len_k)\n",
    "            mask = mask.expand(-1, q.size(1), -1, -1)  # Shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected mask dimensions: {mask.shape}\")\n",
    "        \n",
    "        # Add the mask (assumes additive masking with -inf for masked positions)\n",
    "        scaled = scaled + mask\n",
    "\n",
    "    # Step 3: Apply softmax to get attention weights\n",
    "    attention = F.softmax(scaled, dim=-1)  # Shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    # Step 4: Multiply attention weights with values (Q * Attention * V)\n",
    "    values = torch.matmul(attention, v)  # Shape: (batch_size, num_heads, seq_len_q, d_v)\n",
    "\n",
    "    return attention, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_inference(q, k, v, mask=None):\n",
    "    # Ensure all inputs are in the same data type (float32 for attention mechanism)\n",
    "    dtype = torch.float32  # Use float32 for consistency in attention computations\n",
    "    #print(f\"Initial data types - q: {q.dtype}, k: {k.dtype}, v: {v.dtype}\")\n",
    "\n",
    "    # Cast q, k, v to float32 if they are not already\n",
    "    q = q.to(dtype)\n",
    "    k = k.to(dtype)\n",
    "    v = v.to(dtype)\n",
    "    #print(f\"Data types after casting - q: {q.dtype}, k: {k.dtype}, v: {v.dtype}\")\n",
    "\n",
    "    # Step 1: Compute the scaled dot-product (Q * K^T)\n",
    "    d_k = q.size(-1)  # Dimension of keys (and queries)\n",
    "    #print(f\"Query shape: {q.shape}, Key shape: {k.shape}, Value shape: {v.shape}, d_k: {d_k}\")\n",
    "\n",
    "    # Make sure q and k have compatible shapes for multiplication\n",
    "    assert q.size(-1) == k.size(-1), f\"Shape mismatch: {q.shape} vs {k.shape}\"\n",
    "\n",
    "    scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    #print(f\"Scaled dot product shape: {scaled.shape}\")\n",
    "\n",
    "    # Step 2: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        #print(f\"Mask provided, mask shape: {mask.shape}\")\n",
    "        # Apply mask (same as before)\n",
    "        mask = mask.to(dtype)\n",
    "        scaled = scaled + mask\n",
    "        #print(f\"Scaled with mask applied, shape: {scaled.shape}\")\n",
    "\n",
    "    # Step 3: Apply softmax to get attention weights\n",
    "    attention = F.softmax(scaled, dim=-1)  # Shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    #print(f\"Attention shape: {attention.shape}\")\n",
    "\n",
    "    # Step 4: Multiply attention weights with values (Q * Attention * V)\n",
    "    values = torch.matmul(attention, v)  # Shape: (batch_size, num_heads, seq_len_q, d_v)\n",
    "    #print(f\"Values shape: {values.shape}\")\n",
    "\n",
    "    return attention, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # 512\n",
    "        self.num_heads = num_heads  # 8\n",
    "        self.head_dim = d_model // num_heads  # 64\n",
    "        self.qkv_layer = nn.Linear(d_model, 3*d_model).to(device)  # 512 x 1536\n",
    "        self.linear_layer = nn.Linear(d_model, d_model).to(device)  # 512 x 512\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x.to(device)  # Move input to device\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device)\n",
    "\n",
    "        #print(f\"x.shape = {x.shape}\")\n",
    "        batch_size, max_seq_length, d_model = x.size()\n",
    "        #print(f\"x.size() = {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        #print(f\"x after going through qkv layer - {qkv.size()}\")\n",
    "        qkv_reshape = qkv.reshape(batch_size, max_seq_length, self.num_heads, 3*self.head_dim)\n",
    "        #print(\"--- Entering MultiHeadedAttention ---\")\n",
    "        #print(f\"qkv reshaped shape is {qkv_reshape.size()}\")\n",
    "        qkv_permute = qkv_reshape.permute(0,2,1,3)\n",
    "        #print(f\"QKV Permuted Shape is - {qkv_permute.size()}\")\n",
    "        #print(\"--- Dividing QKV into individual tensors ---\")\n",
    "        q, k, v = qkv_permute.chunk(3, dim=-1)\n",
    "        #print(\"Now q, k, v tensors are passed into the Attention Calculation\")\n",
    "        attention, values = scaled_dot_product(q, k, v, mask=mask)\n",
    "        #print(\"Now we get the Attention Scores and New Values\")\n",
    "        #print(f\"Values shape we get from scaled dot product attention is - {values.size()}\")\n",
    "        #print(\"Now we concatinate the values received from scaled dot product.\")\n",
    "        values_reshaped = values.permute(0,2,1,3).reshape(batch_size, max_seq_length, self.num_heads * self.head_dim)\n",
    "        #print(f\"New Values from Multiheaded attention output shape is - {values_reshaped.size()}\")\n",
    "        output = self.linear_layer(values_reshaped)\n",
    "        #print(f\"Final Output shape from multiheaded attention heads - {output.size()}\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape)).to(device)\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape)).to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.to(device)  # Move input to device\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        #print(f\"Shape of the Mean after preserving the dimensions - {mean.size()}\")\n",
    "        var = ((inputs - mean)**2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        #print(f\"Standard Deviation is {std.size()}\")\n",
    "        normalized_out = (inputs - mean) / std\n",
    "        normalized_out = (normalized_out * self.gamma) + self.beta\n",
    "        #print(\"Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\")\n",
    "        #print(f\"Shape of gamma and beta are - {self.gamma.size()} and {self.beta.size()}\")\n",
    "        #print(f\"Layer Normalized Output shape is {normalized_out.size()}\")\n",
    "        return normalized_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Putting it all together \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention_layer = MultiHeadAttention(num_heads=num_heads, d_model=d_model).to(device)\n",
    "        self.layer_normalizer1 = LayerNormalization(parameters_shape=[d_model]).to(device)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob).to(device)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob).to(device)\n",
    "        self.layer_normalizer2 = LayerNormalization(parameters_shape=[d_model]).to(device)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob).to(device)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x.to(device)  # Move input to device\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device)\n",
    "        \n",
    "        residual_x = x\n",
    "        #print(f\"Encoder Attention Mask shape: {mask.shape}\")\n",
    "        #print(mask)\n",
    "        #print(\"----- Attention Layer 1 ----\")\n",
    "        x = self.attention_layer(x, mask=mask)\n",
    "        #print(f\"Output shape from Multi Head Attention - {x.size()}\")\n",
    "        #print(\"---- Dropout 1 ----\")\n",
    "        x = self.dropout1(x)\n",
    "        #print(f\"x shape after passing through dropout Layer 1 - {x.size()}\")\n",
    "        #print(\"----- Add and Normalizer 1 ---\")\n",
    "        x = self.layer_normalizer1(residual_x + x)\n",
    "        #print(f\"x shape after passing through layer normalizer 1 - {x.size()}\")\n",
    "        residual_x = x\n",
    "        #print(\"---- Attention 2 through ffn ----\")\n",
    "        x = self.ffn(x)\n",
    "        #print(\"----- Dropout 2 -----\")\n",
    "        x = self.dropout2(x)\n",
    "        #print(\"----- Add and Normalizer 2 ---\")\n",
    "        x = self.layer_normalizer2(residual_x + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder Block\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, num_heads=num_heads, drop_prob=drop_prob).to(device) \n",
    "                                       for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x.to(device)  # Move input to device\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderBlock(d_model, ffn_hidden, num_heads,drop_prob, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = nn.Linear(d_model, 2*d_model).to(device)\n",
    "        self.q_layer = nn.Linear(d_model, d_model).to(device)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model).to(device)\n",
    "\n",
    "    def forward(self, x, y, mask=None):\n",
    "        x, y = x.to(device), y.to(device)  # Move inputs to device\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device)\n",
    "\n",
    "        batch_size, encoder_max_seq_length, d_model = x.size()\n",
    "        _, decoder_max_seq_len, _ = y.size()\n",
    "        #print(\"This is the input to the Decoder Layer Cross Attention from Masked Attention from\")\n",
    "        #print(f\"The shape of the input x which comes from Encoder is {x.size()}\")\n",
    "        #print(\"The shape of cross attention mask used in Multi Headed Cross Attention is -\", mask.shape)\n",
    "        \n",
    "        # Keys and values for cross attention will be coming from encoder layer\n",
    "        kv = self.kv_layer(x)\n",
    "        #print(\"The Keys and Values for Cross Attention will be coming from Encoder Layer\")\n",
    "        #print(f\"The shape of kv is {kv.size()}\")\n",
    "        #print(\"batch size: \", batch_size)\n",
    "        #print(\"max_seq_len:\", encoder_max_seq_length)\n",
    "        #print(\"num heads: \", self.num_heads)\n",
    "        #print(\"head_dim: \", self.head_dim)\n",
    "        \n",
    "        #print(\"Query matrix will be coming from masked self-attention head from decoder\")\n",
    "        q = self.q_layer(y)\n",
    "        #print(\"The y which is the query matrix will be coming from the masked self attention head from Decoder.\")\n",
    "        #print(f\"The shape of q is {q.size()}\")\n",
    "        #print(f\"The shape of kv coming from encoder layer is - {kv.size()}\")\n",
    "        \n",
    "        kv = kv.reshape(batch_size, encoder_max_seq_length, self.num_heads, 2*self.head_dim)\n",
    "        #print(\"kv reshaped shape is  - \", kv.shape)\n",
    "        q = q.reshape(batch_size, decoder_max_seq_len, self.num_heads, self.head_dim)\n",
    "        #print(\"q reshaped shape is  -\", q.shape)\n",
    "        # q shape is 30 x 200 x 8 x 64\n",
    "        # k, v shape is 30 x 200 x 8 x 128\n",
    "        # The shape we want is q = 30 x 8 x 200 x 64\n",
    "        # The shape we want for k, v = 30 x 8 x 200 x 128\n",
    "        kv = kv.permute(0,2,1,3)\n",
    "        q = q.permute(0,2,1,3)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        #print(\"kv and q after permuted and kv after chunk. Final shapes will be of q,k,v- \",q.shape, k.shape, v.shape)\n",
    "        # the shape of k, v = 30 x 8 x 200 x 64\n",
    "\n",
    "        attention, values = scaled_dot_product(q, k, v, mask=mask)\n",
    "        #print(f\"Values and Attention shapes are {values.size()} and {attention.size()}\")\n",
    "        values = values.permute(0,2,1,3).reshape(batch_size, decoder_max_seq_len, self.num_heads * self.head_dim)\n",
    "        #print(f\"The contextualized values will be reshaped by 'values.reshape(batch_size, max_seq_length, self.num_heads * self.head_dim)'\")\n",
    "        #print(f\"The contextualized values will be of shape {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        #print(f\"Values passing through a linear layer and returns output with shape - {out.size()}\")\n",
    "        return out  # 30 x 200 x 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, drop_prob, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads).to(device)\n",
    "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model]).to(device)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob).to(device)\n",
    "        self.encoder_decoder_cross_attention = MultiHeadedCrossAttention(d_model=d_model, num_heads=num_heads).to(device)\n",
    "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model]).to(device)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob).to(device)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob).to(device)\n",
    "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model]).to(device)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob).to(device)\n",
    "\n",
    "    def forward(self, x, y, decoder_self_attention_mask=None, cross_attention_mask=None):\n",
    "        x, y = x.to(device), y.to(device)  # Move inputs to device\n",
    "        if decoder_self_attention_mask is not None:\n",
    "            decoder_self_attention_mask = decoder_self_attention_mask.to(device)\n",
    "        if cross_attention_mask is not None:\n",
    "            cross_attention_mask = cross_attention_mask.to(device)\n",
    "\n",
    "        #print(f\"Initial input to DecoderLayer x: {x.shape}, y: {y.shape}\")  \n",
    "        #print(\"Mask shapes used in decoder layer are - \")\n",
    "        #print(\"The shape of decoder masked attention - \", decoder_self_attention_mask.shape)\n",
    "        #print(\"The shape of encoder decoder cross attention mask is - \", cross_attention_mask.shape)\n",
    "\n",
    "        residual_y = y\n",
    "        #print(\"----ENTERING MASKED ATTENTION----\")\n",
    "        y = self.self_attention(y, mask=decoder_self_attention_mask)\n",
    "        #print(f\"Output of masked self-attention: {y.shape}\")  \n",
    "\n",
    "        y = self.dropout1(y)\n",
    "        #print(f\"After dropout 1: {y.shape}\")\n",
    "\n",
    "        y = self.layer_norm1(residual_y + y)\n",
    "        #print(f\"After layer norm 1: {y.shape}\")\n",
    "\n",
    "        residual_y = y\n",
    "        y = self.encoder_decoder_cross_attention(x, y, mask=cross_attention_mask)\n",
    "        #print(f\"Output of cross attention: {y.shape}\")\n",
    "\n",
    "        y = self.dropout2(y)\n",
    "        #print(f\"After dropout 2: {y.shape}\")\n",
    "\n",
    "        y = self.layer_norm2(residual_y + y)\n",
    "        #print(f\"After layer norm 2: {y.shape}\")\n",
    "\n",
    "        residual_y = y\n",
    "        y = self.ffn(y)\n",
    "        #print(f\"After feed-forward network: {y.shape}\")\n",
    "\n",
    "        y = self.dropout3(y)\n",
    "        #print(f\"After dropout 3: {y.shape}\")\n",
    "\n",
    "        y = self.layer_norm3(residual_y + y)\n",
    "        #print(f\"Final output from DecoderLayer: {y.shape}\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(nn.Sequential):\n",
    "    #         self._modules is a dictionary of all the layers stored in the SequentialDecoder (inherited from nn.Sequential).\n",
    "    #         self._modules.values() gives a list of the layers in the order they were added.\n",
    "\n",
    "    def forward(self, x, y, decoder_self_attention_mask=None, cross_attention_mask=None):\n",
    "        x, y = x.to(device), y.to(device)  # Move inputs to device\n",
    "        if decoder_self_attention_mask is not None:\n",
    "            decoder_self_attention_mask = decoder_self_attention_mask.to(device)\n",
    "        if cross_attention_mask is not None:\n",
    "            cross_attention_mask = cross_attention_mask.to(device)\n",
    "\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, y, decoder_self_attention_mask, cross_attention_mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Sequential):\n",
    "    def __init__(self, d_model, num_heads, ffn_hidden, drop_prob, num_layers, device = device):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[\n",
    "            DecoderLayer(d_model=d_model,\n",
    "                         ffn_hidden=ffn_hidden,\n",
    "                         drop_prob=drop_prob,\n",
    "                         num_heads=num_heads).to(device)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, y, decoder_self_attention_mask=None, cross_attention_mask=None):\n",
    "        x, y = x.to(device), y.to(device)  # Move inputs to device\n",
    "        if decoder_self_attention_mask is not None:\n",
    "            decoder_self_attention_mask = decoder_self_attention_mask.to(device)\n",
    "        if cross_attention_mask is not None:\n",
    "            cross_attention_mask = cross_attention_mask.to(device)\n",
    "        \n",
    "        # x shape is 30 x 200 x 512\n",
    "        # y shape is 30 x 200 x 512\n",
    "        return self.layers(x, y, decoder_self_attention_mask, cross_attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    ffn_hidden=ffn_hidden,\n",
    "    drop_prob=drop_prob,\n",
    "    num_layers=num_layers,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/venu/Documents/Productivity/Pytorch Tutorials/en-te/English_Text.txt', \"r\") as english_file_te_trans:\n",
    "    source_eng_to_te_text = english_file_te_trans.readlines()\n",
    "\n",
    "with open('/Users/venu/Documents/Productivity/Pytorch Tutorials/en-te/Telugu_Text.txt', \"r\") as telugu_file:\n",
    "    telugu_text = telugu_file.readlines()\n",
    "\n",
    "with open('/Users/venu/Documents/Productivity/Pytorch Tutorials/en-hi/english_text.txt',\"r\") as english_file_hi_trans:\n",
    "    source_eng_to_hi_text = english_file_hi_trans.readlines()\n",
    "\n",
    "with open('/Users/venu/Documents/Productivity/Pytorch Tutorials/en-hi/hindi_text.txt', \"r\") as hindi_file:\n",
    "    hindi_text = hindi_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('But in BJP this does not happen. \\n', 'కానీ బీజేపీ అలా వివక్ష చూపించదు. \\n')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_eng_to_te_text[-1000], telugu_text[-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This trend is ominous.\\n', 'यह प्रवृत्ति देशघाती है।\\n')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_eng_to_hi_text[-1000], hindi_text[-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4946036, 4946036, 8568307, 8568307)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_eng_to_te_text), len(telugu_text), len(source_eng_to_hi_text), len(hindi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NUM = 42000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_eng_tel_trans = source_eng_to_te_text[-SAMPLE_NUM:]\n",
    "target_telugu = telugu_text[-SAMPLE_NUM:]\n",
    "source_eng_hin_trans = source_eng_to_hi_text[:SAMPLE_NUM]\n",
    "target_hindi = hindi_text[:SAMPLE_NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Shri Tomar also expressed gratitude to Prime Minister Shri Narendra Modi for bringing in long-awaited agricultural reforms by announcing legal amendments and ordinances which will empower the farmers and help them in getting remunerative prices for their produce. \\n',\n",
       "  'Buying a new phone? \\n'],\n",
       " ['దీర్ఘకాలికంగా ఎదురు చూస్తున్న వ్యవసాయ సంస్కరణలను తీసుకువచ్చిన ప్రధానమంత్రి నరేంద్ర మోదీకి కృతజ్ఞతలు తెలిపారు. \\n',\n",
       "  'మీరు కొత్త ఫోన్ కొనాలనుకుంటున్నారా? \\n'],\n",
       " ['In reply, Pakistan got off to a solid start.\\n',\n",
       "  'The European Union has seven principal decision-making bodies, its institutions: the European Parliament, the European Council, the Council of the European Union, the European Commission, the Court of Justice of the European Union, the European Central Bank and the European Court of Auditors.\\n'],\n",
       " ['जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\\n',\n",
       "  'यूरोपीय संघ के महत्वपूर्ण संस्थानों में यूरोपियन कमीशन, यूरोपीय संसद, यूरोपीय संघ परिषद, यूरोपीय न्यायलय एवं यूरोपियन सेंट्रल बैंक इत्यादि शामिल हैं।\\n'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_eng_tel_trans[:2], target_telugu[:2], source_eng_hin_trans[:2], target_hindi[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 42000, 42000, 42000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_eng_tel_trans), len(target_telugu), len(source_eng_hin_trans), len(target_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepend language tokens for telugu and spanish text\n",
    "\n",
    "target_sample_telugu_sentences = [\"<_te_>\" + sentence.strip() for sentence in target_telugu]\n",
    "target_sample_hindi_sentences = [\"<_hi_>\" + sentence.strip() for sentence in target_hindi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Shri Tomar also expressed gratitude to Prime Minister Shri Narendra Modi for bringing in long-awaited agricultural reforms by announcing legal amendments and ordinances which will empower the farmers and help them in getting remunerative prices for their produce. \\n',\n",
       "  'Buying a new phone? \\n',\n",
       "  'If you go offline now, %S downloads will be canceled. Are you sure you want to go offline? \\n'],\n",
       " ['<_te_>దీర్ఘకాలికంగా ఎదురు చూస్తున్న వ్యవసాయ సంస్కరణలను తీసుకువచ్చిన ప్రధానమంత్రి నరేంద్ర మోదీకి కృతజ్ఞతలు తెలిపారు.',\n",
       "  '<_te_>మీరు కొత్త ఫోన్ కొనాలనుకుంటున్నారా?',\n",
       "  '<_te_>ఇప్పుడు మీరు ఆఫ్\\u200cలైన్\\u200cలోకి వెళితే, %S డౌన్లోడులు రద్దవుతాయి. మీరు ఖచ్చితంగా వైదొలగాలని అనుకుంటున్నారా?'],\n",
       " ['In reply, Pakistan got off to a solid start.\\n',\n",
       "  'The European Union has seven principal decision-making bodies, its institutions: the European Parliament, the European Council, the Council of the European Union, the European Commission, the Court of Justice of the European Union, the European Central Bank and the European Court of Auditors.\\n',\n",
       "  'The Congress leader represents Sivaganga Lok Sabha segment from Tamil Nadu.\\n'],\n",
       " ['<_hi_>जिसके जवाब में पाक ने अच्छी शुरुआत की थी.',\n",
       "  '<_hi_>यूरोपीय संघ के महत्वपूर्ण संस्थानों में यूरोपियन कमीशन, यूरोपीय संसद, यूरोपीय संघ परिषद, यूरोपीय न्यायलय एवं यूरोपियन सेंट्रल बैंक इत्यादि शामिल हैं।',\n",
       "  '<_hi_>कांग्रेस नेता तमिलनाडु से शिवगंगा लोकसभा क्षेत्र का प्रतिनिधित्व करते हैं.'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_eng_tel_trans[:3], target_sample_telugu_sentences[:3], source_eng_hin_trans[:3], target_sample_hindi_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripping new line\n",
    "source_eng_tel_trans = [sentence.rstrip('\\n') for sentence in source_eng_tel_trans]\n",
    "target_sample_telugu_sentences = [sentence.rstrip('\\n') for sentence in target_sample_telugu_sentences]\n",
    "source_eng_hin_trans = [sentence.rstrip('\\n') for sentence in source_eng_hin_trans]\n",
    "target_sample_hindi_sentences = [sentence.rstrip('\\n') for sentence in target_sample_hindi_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of tokens in english to spanish english text - 1302\n",
      "Max length of tokens in english to telugu english text - 1217\n",
      "Max length of tokens in spanish sentences - 1098\n",
      "Max length of tokens in telugu sentences - 1177\n"
     ]
    }
   ],
   "source": [
    "# checking max lenght of english and telugu sentences\n",
    "max_len_en_tel_sentences = max(len(sentence) for sentence in source_eng_tel_trans)\n",
    "max_len_tel_sentences = max(len(sentence) for sentence in target_sample_telugu_sentences)\n",
    "max_len_eng_hin_sentences = max(len(sentence) for sentence in source_eng_hin_trans)\n",
    "max_len_hindi_sentences = max(len(sentence) for sentence in target_sample_hindi_sentences)\n",
    "\n",
    "print(f\"Max length of tokens in english to spanish english text - {max_len_en_tel_sentences}\")\n",
    "print(f\"Max length of tokens in english to telugu english text - {max_len_tel_sentences}\")\n",
    "print(f\"Max length of tokens in spanish sentences - {max_len_eng_hin_sentences}\")\n",
    "print(f\"Max length of tokens in telugu sentences - {max_len_hindi_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/r3k6h0y11431_5th45hwjmjr0000gn/T/ipykernel_1689/4225657690.py:8: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([english_telugu_lengths, sample_telugu_lengths], labels=['English', 'Telugu'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/r3k6h0y11431_5th45hwjmjr0000gn/T/ipykernel_1689/4225657690.py:18: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([english_hindi_lengths, sample_hindi_lengths], labels=['English', 'Hindi'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate sentence lengths\n",
    "english_telugu_lengths = [len(sentence) for sentence in source_eng_tel_trans]\n",
    "sample_telugu_lengths = [len(sentence) for sentence in target_sample_telugu_sentences]\n",
    "\n",
    "# Plot boxplots\n",
    "plt.boxplot([english_telugu_lengths, sample_telugu_lengths], labels=['English', 'Telugu'])\n",
    "plt.ylabel('Sentence Length')\n",
    "plt.title('Sentence Length Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Calculate sentence lengths\n",
    "english_hindi_lengths = [len(sentence) for sentence in source_eng_hin_trans]\n",
    "sample_hindi_lengths = [len(sentence) for sentence in target_sample_hindi_sentences]\n",
    "\n",
    "# Plot boxplots\n",
    "plt.boxplot([english_hindi_lengths, sample_hindi_lengths], labels=['English', 'Hindi'])\n",
    "plt.ylabel('Sentence Length')\n",
    "plt.title('Sentence Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97% of English sentences in English to Telugu dataset have length of 176.0 tokens or less\n",
      "97% of Telugu sentences in English to Telugu dataset have length of 171.0 tokens or less\n",
      "97% of English sentences in English to Hindi dataset have length of 266.0 tokens or less\n",
      "97% of Hindi sentences in English to Hindi dataset have length of 264.0 tokens or less\n"
     ]
    }
   ],
   "source": [
    "# restricting the max length sentences to be allowed to 97 percentile\n",
    "\n",
    "# Calculate desired percentile\n",
    "percentile = 97\n",
    "en_te_value = np.percentile(english_telugu_lengths, percentile)\n",
    "telugu_value = np.percentile(sample_telugu_lengths, percentile)\n",
    "en_hi_value = np.percentile(english_hindi_lengths, percentile)\n",
    "hindi_value = np.percentile(sample_hindi_lengths, percentile)\n",
    "\n",
    "\n",
    "print(f\"{percentile}% of English sentences in English to Telugu dataset have length of {en_te_value} tokens or less\")\n",
    "print(f\"{percentile}% of Telugu sentences in English to Telugu dataset have length of {telugu_value} tokens or less\")\n",
    "print(f\"{percentile}% of English sentences in English to Hindi dataset have length of {en_hi_value} tokens or less\")\n",
    "print(f\"{percentile}% of Hindi sentences in English to Hindi dataset have length of {hindi_value} tokens or less\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing sentences only if they have length less then the max sequence length\n",
    "def is_valid_length(sentence, max_seq_length):\n",
    "    return len(list(sentence)) < (max_seq_length - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 42000, 42000, 42000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_eng_tel_trans), len(target_sample_telugu_sentences), len(source_eng_hin_trans), len(target_sample_hindi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_english_telugu_sentence_indices = []\n",
    "\n",
    "for index in range(len(target_sample_telugu_sentences)):\n",
    "    valid_sample_eng_tel_sentences, valid_sample_telugu = source_eng_tel_trans[index], target_sample_telugu_sentences[index]\n",
    "    if is_valid_length(valid_sample_eng_tel_sentences, max_seq_length)\\\n",
    "    and is_valid_length(valid_sample_telugu, max_seq_length):\n",
    "        valid_english_telugu_sentence_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_english_hindi_sentence_indices = []\n",
    "\n",
    "for index in range(len(target_sample_hindi_sentences)):\n",
    "    valid_sample_eng_hin_sentences, valid_sample_hindi = source_eng_hin_trans[index], target_sample_hindi_sentences[index]\n",
    "    if is_valid_length(valid_sample_eng_hin_sentences, max_seq_length)\\\n",
    "    and is_valid_length(valid_sample_hindi, max_seq_length):\n",
    "        valid_english_hindi_sentence_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41790, 40866)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_english_telugu_sentence_indices), len(valid_english_hindi_sentence_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41790, 40866, 40866, 41790)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_valid_telugu_sentences = [target_sample_telugu_sentences[i] for i in valid_english_telugu_sentence_indices]\n",
    "sample_valid_hindi_sentences = [target_sample_hindi_sentences[i] for i in valid_english_hindi_sentence_indices]\n",
    "sample_valid_english_eng_hin_sentences = [source_eng_hin_trans[i] for i in valid_english_hindi_sentence_indices]\n",
    "sample_valid_english_eng_tel_sentences = [source_eng_tel_trans[i] for i in valid_english_telugu_sentence_indices]\n",
    "\n",
    "len(sample_valid_telugu_sentences), len(sample_valid_hindi_sentences), len(sample_valid_english_eng_hin_sentences), len(sample_valid_english_eng_tel_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<_te_>అన్ని ప్రజలు తర్వాత ఎప్పుడైనా సంతోషంగా నివసిస్తున్నట్లు.',\n",
       " 'Everyone lives happily ever after. ')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_valid_telugu_sentences[-2], sample_valid_english_eng_tel_sentences[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<_hi_>तब भी संकल्पनाओं के साम्राज्य में इससे आशातीत सामंजस्य प्रकट होता है।',\n",
       " 'It may nevertheless unfold unsuspected harmonics in the realm of concept.')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_valid_hindi_sentences[-2], sample_valid_english_eng_hin_sentences[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # appending <eos> at the end to the decoder inputs.\n",
    "# eos_token = \"<eos>\"\n",
    "\n",
    "# sample_valid_telugu_sentences = [sentence + \" \" + eos_token for sentence in sample_valid_telugu_sentences]\n",
    "# sample_valid_hindi_sentences = [sentence + \" \" + eos_token for sentence in sample_valid_hindi_sentences]\n",
    "\n",
    "# sample_valid_hindi_sentences[-2], sample_valid_telugu_sentences[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Sentences into Tokens Using Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Bert Tokenizer\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<_hi_>\", \"<_te_>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.eos_token = \"<eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added: ['<_hi_>', '<_te_>']\n",
      "Vocabulary size: 119547\n"
     ]
    }
   ],
   "source": [
    "print(\"Special tokens added:\", tokenizer.additional_special_tokens)\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.vocab_size += len(tokenizer.additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119549"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab_size = tokenizer.vocab_size + len(tokenizer.additional_special_tokens)\n",
    "total_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[119547]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['<_hi_>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[119548]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['<_te_>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing source and target languages\n",
    "\n",
    "tokenized_english_tel = tokenizer(\n",
    "    sample_valid_english_eng_tel_sentences,\n",
    "    padding = \"max_length\",\n",
    "    truncation = True,\n",
    "    max_length = max_seq_length,\n",
    "    return_tensors = \"pt\"\n",
    ")\n",
    "\n",
    "tokenized_english_hin = tokenizer(\n",
    "    sample_valid_english_eng_hin_sentences,\n",
    "    padding = \"max_length\",\n",
    "    truncation = True,\n",
    "    max_length = max_seq_length,\n",
    "    return_tensors = \"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing target telugu and hindi sentences\n",
    "\n",
    "tokenized_telugu = tokenizer(\n",
    "    sample_valid_telugu_sentences,\n",
    "    padding = \"max_length\",\n",
    "    truncation = True,\n",
    "    max_length = max_seq_length,\n",
    "    return_tensors = \"pt\"\n",
    ")\n",
    "\n",
    "tokenized_hindi = tokenizer(\n",
    "    sample_valid_hindi_sentences,\n",
    "    padding = \"max_length\",\n",
    "    truncation = True,\n",
    "    max_length = max_seq_length,\n",
    "    return_tensors = \"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In reply, Pakistan got off to a solid start.\n",
      "Tokenized sample English to Hindi Sentence: [CLS] <_hi_> जिसके जवाब में पाक ने अच्छी शुरुआत की थी. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(sample_valid_english_eng_hin_sentences[0])\n",
    "print(f\"Tokenized sample English to Hindi Sentence: {tokenizer.decode(tokenized_hindi[\"input_ids\"][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shri Tomar also expressed gratitude to Prime Minister Shri Narendra Modi for bringing in long-awaited agricultural reforms by announcing legal amendments and ordinances which will empower the farmers and help them in getting remunerative prices for their produce. \n",
      "Tokenized sample English to Telugu Sentence: [CLS] <_te_> దీర్ఘకాలికంగా ఎదురు చూస్తున్న వ్యవసాయ సంస్కరణలను తీసుకువచ్చిన ప్రధానమంత్రి నరేంద్ర మోదీకి కృతజ్ఞతలు తెలిపారు. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(sample_valid_english_eng_tel_sentences[0])\n",
    "print(f\"Tokenized sample English to Telugu Sentence: {tokenizer.decode(tokenized_telugu[\"input_ids\"][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<_te_>దీర్ఘకాలికంగా ఎదురు చూస్తున్న వ్యవసాయ సంస్కరణలను తీసుకువచ్చిన ప్రధానమంత్రి నరేంద్ర మోదీకి కృతజ్ఞతలు తెలిపారు.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_valid_telugu_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining English and Hindi/Telugu Sentences\n",
    "data = {\n",
    "    \"source\": sample_valid_english_eng_hin_sentences + sample_valid_english_eng_tel_sentences,\n",
    "    \"target\": sample_valid_hindi_sentences + sample_valid_telugu_sentences\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Preprocess Data using SentencePiece Tokenizer -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_sentences(sentences, sp, max_seq_length):\n",
    "\n",
    "#     # Tokenize and pad sequences\n",
    "#     tokenized_sentences = [sp.encode_as_ids(sentence) for sentence in sentences]\n",
    "#     padded_sentences = [\n",
    "#         ids[:max_seq_length] + [0] * (max_seq_length - len(ids)) # this pads the sentence to max_seq_length\n",
    "#         if len(ids) < max_seq_length  else ids[:max_seq_length]\n",
    "#         for ids in tokenized_sentences\n",
    "#     ]\n",
    "#     return torch.tensor(padded_sentences)\n",
    "# # preprocess english to hindi dataset\n",
    "# tokenized_english_hi = preprocess_sentences(sample_valid_english_eng_hin_sentences, sp= sp, max_seq_length= max_seq_length)\n",
    "# tokenized_hindi = preprocess_sentences(sample_valid_hindi_sentences, sp, max_seq_length)\n",
    "\n",
    "# # preprocess english to telugu dataset\n",
    "# tokenized_english_te = preprocess_sentences(sample_valid_english_eng_tel_sentences, sp, max_seq_length)\n",
    "# tokenized_telugu = preprocess_sentences(sample_valid_telugu_sentences, sp, max_seq_length)\n",
    "# # Now we combine the english sentences from source english to hindi and source english to telugu sentences and target sentences differently\n",
    "# combined_english = {\n",
    "#     \"input_ids\": torch.cat((tokenized_english_hi, tokenized_english_te)),\n",
    "#     \"attention_mask\": torch.cat((torch.ones_like(tokenized_english_hi), torch.ones_like(tokenized_english_te)))\n",
    "# }\n",
    "\n",
    "# combined_target = {\n",
    "#     \"input_ids\": torch.cat((tokenized_hindi, tokenized_telugu)),\n",
    "#     \"attention_mask\": torch.cat((torch.ones_like(tokenized_hindi), torch.ones_like(tokenized_telugu)))\n",
    "# }\n",
    "# hi_token_ids = sp.encode_as_ids(\"_<hi> \")\n",
    "# te_token_ids = sp.encode_as_ids(\"_<te> \")\n",
    "\n",
    "# print(f\"Token ID for _<hi> : {hi_token_ids}\")\n",
    "# print(f\"Token ID for _<te> : {te_token_ids}\")\n",
    "# hi_decoded = sp.decode_ids(hi_token_ids)\n",
    "# te_decoded = sp.decode_ids(te_token_ids)\n",
    "\n",
    "# print(f\"Decoded text for _<hi> : {hi_decoded}\")\n",
    "# print(f\"Decoded text for _<te> : {te_decoded}\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\", use_fast=True)\n",
    "# print(\"Updated Vocabulary Size:\", tokenizer.vocab_size)\n",
    "# # defining special tokens for language specific sentences before we actually tokenize the whole sentences\n",
    "# language_specific_tokens = {\"hi\":\"<hi>\", \"te\":\"<te>\"}\n",
    "\n",
    "# print(list(language_specific_tokens.values()))\n",
    "# # adding special tokens explicitly to the tokenizer - \n",
    "# tokenizer.add_special_tokens({\"additional_special_tokens\": list(language_specific_tokens.values())})\n",
    "# print(tokenizer.vocab_size)\n",
    "# print(list[language_specific_tokens.values()])\n",
    "# lang_token_ids = {tokenizer.convert_tokens_to_ids(token) for token in language_specific_tokens.values()}\n",
    "# print(\"Special Token IDs:\", lang_token_ids)\n",
    "# text = \"This is a <hi> Hindi sentence. <te> Telugu sentence.\"\n",
    "# tokenized_text = tokenizer(text)\n",
    "# print(tokenized_text.input_ids)\n",
    "# tokenizer.vocab_size\n",
    "# # Check if special tokens are in the tokenizer\n",
    "# print(f\"<hi> ID: {tokenizer.convert_tokens_to_ids('<hi>')}\")\n",
    "# print(f\"<te> ID: {tokenizer.convert_tokens_to_ids('<te>')}\")\n",
    "# print(f\"Special tokens map: {tokenizer.special_tokens_map}\")\n",
    "# print(f\"All special tokens: {tokenizer.all_special_tokens}\")\n",
    "# # English-to-Hindi dataset\n",
    "# tokenized_english_en_hi_sentence = tokenizer(sample_valid_english_eng_hin_sentences, \n",
    "#                                              padding= \"max_length\", \n",
    "#                                              truncation= True, \n",
    "#                                              max_length= max_seq_length, \n",
    "#                                              return_tensors= \"pt\")\n",
    "\n",
    "# tokenized_hindi_sentence = tokenizer(sample_valid_hindi_sentences, \n",
    "#                                      padding= \"max_length\", \n",
    "#                                      truncation= True, \n",
    "#                                      max_length= max_seq_length, \n",
    "#                                      return_tensors= \"pt\")\n",
    "# # English to Telugu Dataset\n",
    "# tokenized_english_en_te_sentence = tokenizer(sample_valid_english_eng_tel_sentences, \n",
    "#                                              padding= \"max_length\", \n",
    "#                                              truncation= True, \n",
    "#                                              max_length= max_seq_length, \n",
    "#                                              return_tensors= \"pt\")\n",
    "\n",
    "# tokenized_telugu_sentence = tokenizer(sample_valid_telugu_sentences, \n",
    "#                                       padding= \"max_length\", \n",
    "#                                       truncation= True, \n",
    "#                                       max_length= max_seq_length, \n",
    "#                                       return_tensors=\"pt\")\n",
    "# tokenized_english_en_hi_sentence.keys()\n",
    "# # Verify that the special tokens are added\n",
    "# lang_token_ids = {lang: tokenizer.convert_tokens_to_ids(token) for lang, token in language_specific_tokens.items()}\n",
    "# print(\"Language Token IDs:\", lang_token_ids)  # Ensure unique IDs for `<hi>` and `<te>`\n",
    "# # Tokenize a sample sentence with a prepended token\n",
    "# sample_sentence = \"<hi> This is a test sentence for Hindi.\"\n",
    "# sample_tokenized = tokenizer(sample_sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
    "\n",
    "# # Inspect input IDs\n",
    "# print(\"Tokenized Input IDs:\", sample_tokenized[\"input_ids\"])\n",
    "# print(\"Special Token ID for <hi>:\", lang_token_ids[\"hi\"])\n",
    "# lang_token_ids[\"te\"]\n",
    "# sample_te_sentence = \"<te> ఇది తెలుగులో పరీక్ష వాక్యం.\"\n",
    "# tokenized_te = tokenizer(sample_te_sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
    "# print(\"Special Token ID for <te>:\", lang_token_ids[\"te\"])\n",
    "# assert lang_token_ids[\"te\"] in tokenized_te[\"input_ids\"]\n",
    "# print(\"Vocabulary size:\", tokenizer.vocab_size)\n",
    "# # combined datasets\n",
    "# combined_english = {\n",
    "#     \"input_ids\": torch.cat((tokenized_english_en_hi_sentence[\"input_ids\"], tokenized_english_en_te_sentence[\"input_ids\"])),\n",
    "#     \"attention_mask\": torch.cat((tokenized_english_en_hi_sentence[\"attention_mask\"], tokenized_english_en_te_sentence[\"attention_mask\"]))\n",
    "# }\n",
    "\n",
    "# combined_target = {\n",
    "#     \"input_ids\": torch.cat((tokenized_hindi_sentence[\"input_ids\"], tokenized_telugu_sentence[\"input_ids\"])),\n",
    "#     \"attention_mask\": torch.cat((tokenized_hindi_sentence[\"attention_mask\"], tokenized_telugu_sentence[\"attention_mask\"]))\n",
    "# }\n",
    "# combined_english\n",
    "# # checking shapes\n",
    "# print(\"Encoder input shape:\", combined_english[\"input_ids\"].shape)\n",
    "# print(\"Decoder input shape:\", combined_target[\"input_ids\"].shape)\n",
    "# # Combined lists of source english and target hindi and telugu language sentences into single list to pass into custom dataset\n",
    "\n",
    "# # combined source english\n",
    "# combined_source_english_sentences = sample_valid_english_eng_hin_sentences + sample_valid_english_eng_tel_sentences\n",
    "# combined_target_sentences = sample_valid_hindi_sentences + sample_valid_telugu_sentences\n",
    "\n",
    "# # create data dictionary\n",
    "\n",
    "# data = {\n",
    "#     \"source\": combined_source_english_sentences,\n",
    "#     \"target\": combined_target_sentences\n",
    "# }\n",
    "# print(\"Language Token IDs:\", lang_token_ids)\n",
    "# print(\"Sample tokenized input IDs:\", sample_tokenized[\"input_ids\"])\n",
    "# assert lang_token_ids[\"hi\"] in sample_tokenized[\"input_ids\"]\n",
    "# print(\"Updated Vocabulary Size:\", tokenizer.vocab_size)\n",
    "# for token in language_specific_tokens.values():\n",
    "#     assert token in tokenizer.get_vocab(), f\"Token {token} not found in vocabulary!\"\n",
    "# print(\"Total source english sentences\", len(data[\"source\"]))\n",
    "# print(\"Total target hindi and telugu sentences\", len(data[\"target\"]))\n",
    "# for i in range(3):\n",
    "#     print(f\"Source Sentence {data[\"source\"][i]}\")\n",
    "#     print(f\"target sentence {data[\"target\"][i]}\")\n",
    "#     print(\"*********\")\n",
    "# for i in range(-4,-1):\n",
    "#     print(f\"Source Sentence {data[\"source\"][i]}\")\n",
    "#     print(f\"target sentence {data[\"target\"][i]}\")\n",
    "#     print(\"*********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, d_model, max_seq_length, pad_token_id, device):\n",
    "        \"\"\"\n",
    "        Initializes the TranslationDataset by preparing encoder and decoder inputs with proper padding.\n",
    "        \"\"\"\n",
    "        self.device = device  # Store device for later use\n",
    "\n",
    "        # Prepare data using the tokenizer\n",
    "        encoder_inputs = tokenizer(\n",
    "            text=data[\"source\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        decoder_inputs = tokenizer(\n",
    "            text=data[\"target\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Move tokenized data to device\n",
    "        encoder_inputs = {k: v.to(device) for k, v in encoder_inputs.items()}\n",
    "        decoder_inputs = {k: v.to(device) for k, v in decoder_inputs.items()}\n",
    "\n",
    "        # Extract inputs and attention masks\n",
    "        self.encoder_input_ids = encoder_inputs[\"input_ids\"]\n",
    "        self.encoder_attention_mask = encoder_inputs[\"attention_mask\"]\n",
    "        self.decoder_input_ids = decoder_inputs[\"input_ids\"][:, :-1]  # Shift target by 1 for decoder\n",
    "        self.decoder_attention_mask = decoder_inputs[\"attention_mask\"][:, :-1]  # Match decoder sequence length\n",
    "\n",
    "        self.max_seq_length_encoder = self.encoder_input_ids.size(1)  # Dynamic seq length for encoder\n",
    "        self.max_seq_length_decoder = self.decoder_input_ids.size(1)  # Dynamic seq length for decoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.d_model = d_model\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        #print(f\"The shape of Encoder input IDs: {self.encoder_input_ids.shape}\")\n",
    "        #print(f\"The shape of Decoder input IDs: {self.decoder_input_ids.shape}\")\n",
    "\n",
    "        # Calculate sequence lengths from attention masks\n",
    "        self.encoder_seq_len = self.encoder_attention_mask.sum(dim=1).tolist()\n",
    "        self.decoder_seq_len = self.decoder_attention_mask.sum(dim=1).tolist()\n",
    "\n",
    "        # Generate masks\n",
    "        self.encoder_mask, self.decoder_self_attention_mask, self.cross_attention_mask = self.create_masks()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encoder_input_ids.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return {\n",
    "            \"encoder_input_ids\": self.encoder_input_ids[idx],\n",
    "            \"encoder_attention_mask\": self.encoder_attention_mask[idx],\n",
    "            \"decoder_input_ids\": self.decoder_input_ids[idx],\n",
    "            \"decoder_attention_mask\": self.decoder_attention_mask[idx],\n",
    "            \"cross_attention_mask\": self.cross_attention_mask[idx]\n",
    "        }\n",
    "\n",
    "    def create_padding_mask(self, sequence_lengths, max_seq_len):\n",
    "        \"\"\"\n",
    "        Creates padding masks for the input sequences.\n",
    "        Padding positions are marked as False (0) for valid tokens and True (1) for padding.\n",
    "        \"\"\"\n",
    "        batch_size = len(sequence_lengths)\n",
    "        mask = torch.ones((batch_size, max_seq_len), dtype=torch.bool, device=self.device)\n",
    "\n",
    "        for i, length in enumerate(sequence_lengths):\n",
    "            mask[i, :length] = False  # Valid positions are False\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def create_look_ahead_mask(self, seq_len):\n",
    "        \"\"\"Creates a look-ahead mask to block future tokens for autoregressive decoding.\"\"\"\n",
    "        return torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool, device=self.device), diagonal=1)\n",
    "\n",
    "    def create_masks(self):\n",
    "        \"\"\"\n",
    "        Generates all required masks: encoder padding mask, decoder self-attention mask, and cross-attention mask.\n",
    "        \"\"\"\n",
    "        # Create padding masks\n",
    "        initial_encoder_padding_mask = self.create_padding_mask(self.encoder_seq_len, self.max_seq_length_encoder)\n",
    "        initial_decoder_padding_mask = self.create_padding_mask(self.decoder_seq_len, self.max_seq_length_decoder)\n",
    "        #print(\"The encoder padding mask generated is of shape - \", initial_encoder_padding_mask.shape)\n",
    "        #print(\"The decoder padding mask generated is of shape - \", initial_decoder_padding_mask.shape)\n",
    "\n",
    "        # Expand encoder padding mask for multi-head attention\n",
    "        encoder_mask = initial_encoder_padding_mask.unsqueeze(1).unsqueeze(2)  # Shape: (batch_size, 1, 1, encoder_seq_len)\n",
    "        #print(\"The generated encoder mask is of shape - \", encoder_mask.shape)\n",
    "\n",
    "        # Look-ahead mask for decoder\n",
    "        initial_decoder_self_attention_mask = self.create_look_ahead_mask(self.max_seq_length_decoder)\n",
    "        #print(\"The initial shape of decoder lookahead mask is - \", initial_decoder_self_attention_mask.shape)\n",
    "\n",
    "        # Unsqueeze and expand decoder padding mask\n",
    "        decoder_padding_mask = initial_decoder_padding_mask.unsqueeze(1).unsqueeze(2)  # Shape: (batch_size, 1, 1, decoder_seq_len)\n",
    "        #print(\"Unsqueezed decoder padding mask shape is - \", decoder_padding_mask.shape)\n",
    "\n",
    "        # Combine decoder padding mask with look-ahead mask for self-attention\n",
    "        #print(\"The decoder self attention mask is of shape - \", initial_decoder_self_attention_mask.shape)\n",
    "        decoder_self_attention_mask = initial_decoder_self_attention_mask.unsqueeze(0).unsqueeze(1)  # Shape: (1, 1, decoder_seq_len, decoder_seq_len)\n",
    "        #print(f\"After unsqueezing and adding dim at 0th and 1th position of decoder_self_attention_mask - {decoder_self_attention_mask.shape}\")\n",
    "        decoder_self_attention_mask = decoder_self_attention_mask | decoder_padding_mask  # Shape: (batch_size, 1, decoder_seq_len, decoder_seq_len)\n",
    "        #print(\"The decoder self attention mask generated is of shape - \", decoder_self_attention_mask.shape)\n",
    "\n",
    "        #print(\"Initial encoder padding mask used for cross attention mask\", initial_encoder_padding_mask.shape)\n",
    "        # Expand decoder padding mask for cross-attention\n",
    "        \n",
    "        # Correct Cross-Attention Mask: Use encoder padding mask for the cross-attention\n",
    "        cross_attention_mask = initial_encoder_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "        #print(\"Cross Attention mask generated unsqueezed is of shape - \",cross_attention_mask.shape)\n",
    "        cross_attention_mask =  cross_attention_mask.expand(-1, -1, self.max_seq_length_decoder, self.max_seq_length_encoder)\n",
    "        #print(\"The encoder decoder cross attention mask generated is of shape - \", cross_attention_mask.shape)\n",
    "\n",
    "        #cross_attention_mask = initial_decoder_self_attention_mask.expand(-1, -1, self.max_seq_length_decoder, self.max_seq_length_encoder)\n",
    "        #print(\"The encoder decoder cross attention mask generated is of shape - \", cross_attention_mask.shape)\n",
    "\n",
    "        print(\"---- APPLYING NEG INF TO MASKS ----\")\n",
    "\n",
    "        # Apply negative infinity values for masked positions\n",
    "        encoder_mask = encoder_mask.float().masked_fill(mask=encoder_mask, value=NEG_INFTY)\n",
    "        #print(\"The shape of encoder mask after adding -neg infinity values to 0 positions\", encoder_mask.shape)\n",
    "        decoder_self_attention_mask = decoder_self_attention_mask.float().masked_fill(mask=decoder_self_attention_mask, value=NEG_INFTY)\n",
    "        #print(\"The shape of decoder mask after adding -neg infinity values to 0 positions\", decoder_self_attention_mask.shape)\n",
    "        cross_attention_mask = cross_attention_mask.float().masked_fill(mask=cross_attention_mask, value=NEG_INFTY)\n",
    "        #print(\"The shape of encoder decoder cross attention mask after adding -neg infinity values to 0 positions\", cross_attention_mask.shape)\n",
    "\n",
    "\n",
    "        print(\"---- AFTER APPLYING NEG INFINITY TO THE MASKS ----\")\n",
    "\n",
    "        #print(f\"Encoder mask shape: {encoder_mask.shape}\")\n",
    "        #print(f\"Decoder self-attention mask shape: {decoder_self_attention_mask.shape}\")\n",
    "        #print(f\"Cross-attention mask shape: {cross_attention_mask.shape}\")\n",
    "\n",
    "        return encoder_mask, decoder_self_attention_mask, cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- APPLYING NEG INF TO MASKS ----\n",
      "---- AFTER APPLYING NEG INFINITY TO THE MASKS ----\n"
     ]
    }
   ],
   "source": [
    "# Initilization of dataset and dataloader\n",
    "\n",
    "# Initialize Dataset\n",
    "translation_dataset = TranslationDataset(\n",
    "    data=data,\n",
    "    tokenizer=tokenizer,\n",
    "    d_model=d_model,\n",
    "    max_seq_length=max_seq_length,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataLoader\n",
    "translation_dataloader = DataLoader(\n",
    "    dataset=translation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input IDs: torch.Size([16, 300])\n",
      "Decoder Input IDs: torch.Size([16, 299])\n",
      "Encoder Mask: torch.Size([16, 300])\n",
      "Decoder Self-Attention Mask: torch.Size([16, 299])\n",
      "Cross-Attention Mask: torch.Size([16, 1, 299, 300])\n"
     ]
    }
   ],
   "source": [
    "# Verifying prepared data\n",
    "\n",
    "for batch in translation_dataloader:\n",
    "    print(\"Encoder Input IDs:\", batch[\"encoder_input_ids\"].shape)\n",
    "    print(\"Decoder Input IDs:\", batch[\"decoder_input_ids\"].shape)\n",
    "    print(\"Encoder Mask:\", batch[\"encoder_attention_mask\"].shape)\n",
    "    print(\"Decoder Self-Attention Mask:\", batch[\"decoder_attention_mask\"].shape)\n",
    "    print(\"Cross-Attention Mask:\", batch[\"cross_attention_mask\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 vocab_size,\n",
    "                 num_heads,\n",
    "                 num_layers,\n",
    "                 drop_prob,\n",
    "                 max_seq_length,\n",
    "                 encoder_positional_encoder,\n",
    "                 decoder_positional_encoder,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device  # Store device\n",
    "        \n",
    "        # Embedding layers for encoder and decoder\n",
    "        self.encoder_embedding = nn.Embedding(tokenizer.vocab_size, d_model).to(device)\n",
    "        self.decoder_embedding = nn.Embedding(tokenizer.vocab_size, d_model).to(device)\n",
    "        \n",
    "        # Positional Encodings\n",
    "        self.encoder_positional_encoder = encoder_positional_encoder.to(device)\n",
    "        self.decoder_positional_encoder = decoder_positional_encoder.to(device)\n",
    "        \n",
    "        # Encoder and decoder layers\n",
    "        self.encoder = EncoderBlock(\n",
    "            d_model=d_model,\n",
    "            ffn_hidden=ffn_hidden,\n",
    "            num_heads=num_heads,\n",
    "            drop_prob=drop_prob,\n",
    "            num_layers=num_layers\n",
    "        ).to(device)\n",
    "        self.decoder = DecoderLayer(\n",
    "            d_model=d_model,\n",
    "            ffn_hidden=ffn_hidden,\n",
    "            drop_prob=drop_prob,\n",
    "            num_heads=num_heads\n",
    "        ).to(device)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(in_features=d_model, out_features=vocab_size).to(device)\n",
    "\n",
    "    def forward(self, encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_self_attention_mask, cross_attention_mask):\n",
    "        # Move inputs to device\n",
    "        encoder_input_ids = encoder_input_ids.to(self.device)\n",
    "        encoder_attention_mask = encoder_attention_mask.to(self.device)\n",
    "        decoder_input_ids = decoder_input_ids.to(self.device)\n",
    "        decoder_self_attention_mask = decoder_self_attention_mask.to(self.device)\n",
    "        cross_attention_mask = cross_attention_mask.to(self.device)\n",
    "        \n",
    "        # Apply embedding layers\n",
    "        encoder_embeds = self.encoder_embedding(encoder_input_ids)  # Shape: (batch_size, seq_len, d_model)\n",
    "        decoder_embeds = self.decoder_embedding(decoder_input_ids)  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Add positional encodings\n",
    "        encoder_embeds += self.encoder_positional_encoder(encoder_embeds)\n",
    "        decoder_embeds += self.decoder_positional_encoder(decoder_embeds)\n",
    "        \n",
    "        # Encoder and decoder\n",
    "        #print(\"The encoder attention mask shape is  -\", encoder_attention_mask.shape)\n",
    "        encoder_output = self.encoder(encoder_embeds, encoder_attention_mask)\n",
    "        #print(f\"The Encoder Encoder output shape before passing to decoder: {encoder_output.shape}\")\n",
    "        #print(f\"The shape of decoder embeds which enter the decoder layers is {decoder_embeds.shape}\")\n",
    "        #print(f\"The decoder self attention mask shape is -\", decoder_self_attention_mask.shape)\n",
    "        #print(f\"The encoder decoder cross attention mask shape is - \", cross_attention_mask.shape)\n",
    "\n",
    "        decoder_output = self.decoder(encoder_output, decoder_embeds, decoder_self_attention_mask, cross_attention_mask)\n",
    "        \n",
    "        # Output logits\n",
    "        logits = self.output_layer(decoder_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_positional_encoder = PositionEncoding(d_model, max_seq_length)\n",
    "decoder_positional_encoder = PositionEncoding(d_model, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = TransformerModel(d_model=d_model,\n",
    "                                     vocab_size= total_vocab_size,\n",
    "                                     num_heads= num_heads,\n",
    "                                     num_layers= num_layers,\n",
    "                                     drop_prob= drop_prob,\n",
    "                                     max_seq_length= max_seq_length,\n",
    "                                     encoder_positional_encoder= encoder_positional_encoder,\n",
    "                                     decoder_positional_encoder= decoder_positional_encoder,\n",
    "                                     device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.encoder_embedding = nn.Embedding(total_vocab_size, d_model).to(device)\n",
    "transformer_model.decoder_embedding = nn.Embedding(total_vocab_size, d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Encoder Input ID: 110239\n",
      "Max Decoder Input ID: 119548\n"
     ]
    }
   ],
   "source": [
    "for batch in translation_dataloader:\n",
    "    print(\"Max Encoder Input ID:\", batch[\"encoder_input_ids\"].max().item())\n",
    "    print(\"Max Decoder Input ID:\", batch[\"decoder_input_ids\"].max().item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_hi_>मंत्रालय ने कहा कि उड़े देश का आम नागरिक यानी उड़ान योजना के तहत अब तक कुल 766 मार्गों को मंजूरी दी गई है।\n",
      "tensor([[   101, 119547,  61881,    872,  28960,  18351,  10532,    885,  11208,\n",
      "          12151,  13088,    851,  16940,  33345,  10914, 110065,  10826,  15837,\n",
      "            119,    102]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_valid_hindi_sentences[99])\n",
    "tokenized_target = tokenizer(sample_valid_hindi_sentences[0], return_tensors=\"pt\", add_special_tokens=True)\n",
    "print(tokenized_target[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119549"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Special Tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '<_hi_>', '<_te_>']\n"
     ]
    }
   ],
   "source": [
    "print(\"All Special Tokens:\", tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary for '<_hi_>': 119547\n",
      "Tokenizer vocabulary for '<hi>': 100\n",
      "Tokenizer vocabulary for '<_te_>': 119548\n",
      "Tokenizer vocabulary for '<te>': 100\n",
      "Token ID for <_hi_>: 119547\n",
      "Token ID for <_te_>: 119548\n",
      "Total Vocabulary Size: 119549\n"
     ]
    }
   ],
   "source": [
    "hi_id = tokenizer.convert_tokens_to_ids('<_hi_>')\n",
    "te_id = tokenizer.convert_tokens_to_ids('<_te_>')\n",
    "\n",
    "print(f\"Tokenizer vocabulary for '<_hi_>': {tokenizer.convert_tokens_to_ids('<_hi_>')}\")\n",
    "print(f\"Tokenizer vocabulary for '<hi>': {tokenizer.convert_tokens_to_ids('<hi>')}\")\n",
    "print(f\"Tokenizer vocabulary for '<_te_>': {tokenizer.convert_tokens_to_ids('<_te_>')}\")\n",
    "print(f\"Tokenizer vocabulary for '<te>': {tokenizer.convert_tokens_to_ids('<te>')}\")\n",
    "\n",
    "print(f\"Token ID for <_hi_>: {hi_id}\")\n",
    "print(f\"Token ID for <_te_>: {te_id}\")\n",
    "print(f\"Total Vocabulary Size: {total_vocab_size}\")\n",
    "\n",
    "assert hi_id < total_vocab_size, \"Token ID for <_hi_> exceeds total vocab size!\"\n",
    "assert te_id < total_vocab_size, \"Token ID for <_te_> exceeds total vocab size!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample decoder input IDs:\n",
      "tensor([   101, 119548,   1234,  34479,  41772,  91124,  80466,   1224,  93137,\n",
      "         13179, 107828,   1220,  26056,  15280,  30805,    113,   1207,  15868,\n",
      "           119,   1234,  37637,  56585,    119,   1195,  13652,    119,  97558,\n",
      "           119,    114,   1234,  34479,  41772,  91124,  80466,    117,  97558,\n",
      "         17456, 108696,  52798,  15735,  80466,  55150,   1220,  52798,  87978,\n",
      "         13302,  46832,  14314,   1215,  18564,  15695, 111330,  52640,  15280,\n",
      "         21120,  61924,  13652,   1196,  15695,  48317,  83507,  30805,    119,\n",
      "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0])\n",
      "Decoded tokens:\n",
      "['[CLS]', '<_te_>', 'హ', '##ై', '##ద', '##రా', '##బాద్', 'మ', '##హా', '##న', '##గర', 'ప', '##ాల', '##క', 'సంస్థ', '(', 'జ', '##ి', '.', 'హ', '##ె', '##చ్', '.', 'ఎ', '##ం', '.', 'సి', '.', ')', 'హ', '##ై', '##ద', '##రా', '##బాద్', ',', 'సి', '##కి', '##ంద', '##్ర', '##ా', '##బాద్', 'లోని', 'ప', '##్ర', '##జ', '##ల', 'అవసరాల', '##ను', 'త', '##ీ', '##ర్', '##చ', '##డం', '##క', '##ో', '##స', '##ం', 'ఏ', '##ర్', '##ప', '##డిన', 'సంస్థ', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "for batch in translation_dataloader:\n",
    "    print(\"Sample decoder input IDs:\")\n",
    "    print(batch[\"decoder_input_ids\"][0])  # First sentence in the batch\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(batch[\"decoder_input_ids\"][0].tolist())\n",
    "    print(\"Decoded tokens:\")\n",
    "    print(decoded_tokens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits Shape is - torch.Size([16, 299, 119549])\n"
     ]
    }
   ],
   "source": [
    "for batch in translation_dataloader:\n",
    "    logits = transformer_model(\n",
    "        encoder_input_ids=batch[\"encoder_input_ids\"],\n",
    "        encoder_attention_mask=batch[\"encoder_attention_mask\"],\n",
    "        decoder_input_ids=batch[\"decoder_input_ids\"],\n",
    "        decoder_self_attention_mask=batch[\"decoder_attention_mask\"],\n",
    "        cross_attention_mask=batch[\"cross_attention_mask\"])\n",
    "    print(\"Logits Shape is -\", logits.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119547, 119548)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_id = tokenizer.convert_tokens_to_ids(\"<_hi_>\")\n",
    "te_id = tokenizer.convert_tokens_to_ids(\"<_te_>\")\n",
    "hi_id, te_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(params=transformer_model.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, dataloader, num_epochs, learning_rate, tokenizer, pad_token_id, device):\n",
    "#     \"\"\"\n",
    "#     Function to train the Transformer model for multilingual translation.\n",
    "#     Args:\n",
    "#         model (nn.Module): The Transformer model.\n",
    "#         dataloader (DataLoader): The DataLoader providing the training data.\n",
    "#         num_epochs (int): Number of training epochs.\n",
    "#         learning_rate (float): Learning rate for the optimizer.\n",
    "#         tokenizer (AutoTokenizer): Tokenizer for text processing.\n",
    "#         pad_token_id (int): Padding token ID for calculating loss.\n",
    "#         device (str): Device for training ('cuda' or 'cpu').\n",
    "#     Returns:\n",
    "#         model (nn.Module): The trained model.\n",
    "#         metrics (dict): Training metrics, including epoch and batch losses.\n",
    "#     \"\"\"\n",
    "#     # Move model to device\n",
    "#     model.to(device)\n",
    "#     metrics = {\"epoch_losses\": [], \n",
    "#                \"batch_loss\": [],\n",
    "#                \"language_specific_loss\": {\"hindi\": [], \"telugu\": []}}\n",
    "#     print(f\"Hindi Identifeir Token Id: {hi_id}, Telugu Identifier Token Id: {te_id}\")\n",
    "    \n",
    "#     # Training loop\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_loss = 0.0\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "#         for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "#             # Move batch data to device\n",
    "#             encoder_input_ids = batch[\"encoder_input_ids\"].to(device)\n",
    "#             encoder_attention_mask = batch[\"encoder_attention_mask\"].to(device)\n",
    "#             decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
    "#             decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
    "#             cross_attention_mask = batch[\"cross_attention_mask\"].to(device)\n",
    "\n",
    "#             # Prepare decoder target by shifting left\n",
    "#             target_ids = torch.cat([\n",
    "#                 decoder_input_ids[:, 1:], \n",
    "#                 torch.full((decoder_input_ids.size(0), 1), pad_token_id, dtype=torch.long, device=device)\n",
    "#             ], dim=1)  # Right-pad to match the shifted sequence\n",
    "\n",
    "#             # Debug: Count Hindi and Telugu sentences in the batch\n",
    "#             hi_count = (decoder_input_ids[:, 1] == hi_id).sum().item()\n",
    "#             te_count = (decoder_input_ids[:, 1] == te_id).sum().item()\n",
    "#             print(f\"Batch {batch_idx + 1}: Hindi: {hi_count}, Telugu: {te_count}\")\n",
    "\n",
    "#             # Forward pass\n",
    "#             logits = model(\n",
    "#                 encoder_input_ids=encoder_input_ids,\n",
    "#                 encoder_attention_mask=encoder_attention_mask,\n",
    "#                 decoder_input_ids=decoder_input_ids,\n",
    "#                 decoder_self_attention_mask=decoder_attention_mask,\n",
    "#                 cross_attention_mask=cross_attention_mask\n",
    "#             )\n",
    "\n",
    "#             # Reshape logits and target for loss calculation\n",
    "#             logits = logits.view(-1, logits.size(-1))  # Shape: (batch_size * seq_len, vocab_size)\n",
    "#             target_ids = target_ids.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "#             # Calculate loss\n",
    "#             loss = criterion(logits, target_ids)\n",
    "\n",
    "#             # Backpropagation and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Log batch loss\n",
    "#             metrics[\"batch_loss\"].append(loss.item())\n",
    "\n",
    "#             # Accumulate epoch loss\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#             # Language Specific Loss\n",
    "#             batch_hi_loss = []\n",
    "#             batch_te_loss = []\n",
    "\n",
    "#             # Language Specific Loss - \n",
    "#             for i in range(decoder_input_ids.size(0)):\n",
    "#                 if decoder_input_ids[i, 1].item() == hi_id:\n",
    "#                     #metrics[\"language_specific_loss\"][\"hindi\"].append(loss.item())\n",
    "#                     batch_hi_loss.append(loss.item())\n",
    "#                 elif decoder_input_ids[i, 1].item() == te_id:\n",
    "#                     #metrics[\"language_specific_loss\"][\"telugu\"].append(loss.item())\n",
    "#                     batch_te_loss.append(loss.item())\n",
    "\n",
    "\n",
    "#             avg_batch_hi_loss = sum(batch_hi_loss) / len(batch_hi_loss) if batch_hi_loss else 0\n",
    "#             avg_batch_te_loss = sum(batch_te_loss) / len(batch_te_loss) if batch_te_loss else 0\n",
    "\n",
    "#             # Print batch metrics\n",
    "#             print(f\"Batch {batch_idx + 1} Loss: {loss.item():.4f}\")\n",
    "#             print(f\"Batch {batch_idx + 1} Average Hindi Language Loss: {avg_batch_hi_loss:.4f}\")\n",
    "#             print(f\"Batch {batch_idx + 1} Average Telugu Language Loss: {avg_batch_te_loss:.4f}\")\n",
    "\n",
    "#         # Compute average epoch loss\n",
    "#         average_epoch_loss = epoch_loss / len(dataloader)\n",
    "#         metrics[\"epoch_losses\"].append(average_epoch_loss)\n",
    "\n",
    "#         # Average language specific loss - \n",
    "#         avg_hi_loss = sum(metrics[\"language_specific_loss\"][\"hindi\"]) / len(metrics[\"language_specific_loss\"][\"hindi\"]) if metrics[\"language_specific_loss\"][\"hindi\"] else 0\n",
    "#         avg_te_loss = sum(metrics[\"language_specific_loss\"][\"telugu\"]) / len(metrics[\"language_specific_loss\"][\"telugu\"]) if metrics[\"language_specific_loss\"][\"telugu\"] else 0\n",
    "        \n",
    "#         # Print epoch loss\n",
    "#         print(f\"Epoch {epoch + 1} Loss: {average_epoch_loss:.4f}\")\n",
    "#         print(f\"Epoch {epoch + 1} Average Hindi Language Loss: {avg_hi_loss:.4f}\")\n",
    "#         print(f\"Epoch {epoch + 1} Average Telugu Language Loss: {avg_te_loss:.4f}\")\n",
    "\n",
    "#     print(\"Training complete.\")\n",
    "#     return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, dataloader, num_epochs, learning_rate, tokenizer, pad_token_id, device):\n",
    "#     \"\"\"\n",
    "#     Function to train the Transformer model for multilingual translation.\n",
    "#     \"\"\"\n",
    "#     # Move model to device\n",
    "#     model.to(device)\n",
    "#     metrics = {\"epoch_losses\": [], \n",
    "#                \"batch_loss\": [],\n",
    "#                \"language_specific_loss\": {\"hindi\": [], \"telugu\": []}}\n",
    "    \n",
    "#     # Debug IDs\n",
    "#     hi_id = tokenizer.convert_tokens_to_ids(\"<_hi_>\")\n",
    "#     te_id = tokenizer.convert_tokens_to_ids(\"<_te_>\")\n",
    "#     print(f\"Hindi ID: {hi_id}, Telugu ID: {te_id}\")\n",
    "\n",
    "#     # Training loop\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_loss = 0.0\n",
    "#         print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "#         for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "#             # Move batch data to device\n",
    "#             encoder_input_ids = batch[\"encoder_input_ids\"].to(device)\n",
    "#             encoder_attention_mask = batch[\"encoder_attention_mask\"].to(device)\n",
    "#             decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
    "#             decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
    "#             cross_attention_mask = batch[\"cross_attention_mask\"].to(device)\n",
    "\n",
    "#             # Prepare decoder target by shifting left\n",
    "#             target_ids = torch.cat([\n",
    "#                 decoder_input_ids[:, 1:], \n",
    "#                 torch.full((decoder_input_ids.size(0), 1), pad_token_id, dtype=torch.long, device=device)\n",
    "#             ], dim=1)  # Right-pad to match the shifted sequence\n",
    "\n",
    "#             # Debug: Count Hindi and Telugu sentences in the batch\n",
    "#             hi_count = (decoder_input_ids[:, 0] == hi_id).sum().item()\n",
    "#             te_count = (decoder_input_ids[:, 0] == te_id).sum().item()\n",
    "#             print(f\"Batch {batch_idx + 1}: Hindi: {hi_count}, Telugu: {te_count}\")\n",
    "\n",
    "#             # Forward pass\n",
    "#             logits = model(\n",
    "#                 encoder_input_ids=encoder_input_ids,\n",
    "#                 encoder_attention_mask=encoder_attention_mask,\n",
    "#                 decoder_input_ids=decoder_input_ids,\n",
    "#                 decoder_self_attention_mask=decoder_attention_mask,\n",
    "#                 cross_attention_mask=cross_attention_mask\n",
    "#             )\n",
    "\n",
    "#             # Reshape logits and target for loss calculation\n",
    "#             logits = logits.view(-1, logits.size(-1))  # Shape: (batch_size * seq_len, vocab_size)\n",
    "#             target_ids = target_ids.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "#             # Calculate loss\n",
    "#             loss = criterion(logits, target_ids)\n",
    "\n",
    "#             # Backpropagation and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Log batch loss\n",
    "#             metrics[\"batch_loss\"].append(loss.item())\n",
    "\n",
    "#             # Accumulate epoch loss\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#             # Language-Specific Loss Tracking\n",
    "#             for i in range(decoder_input_ids.size(0)):\n",
    "#                 if decoder_input_ids[i, 0].item() == hi_id:\n",
    "#                     metrics[\"language_specific_loss\"][\"hindi\"].append(loss.item())\n",
    "#                 elif decoder_input_ids[i, 0].item() == te_id:\n",
    "#                     metrics[\"language_specific_loss\"][\"telugu\"].append(loss.item())\n",
    "\n",
    "#         # Compute average epoch loss\n",
    "#         average_epoch_loss = epoch_loss / len(dataloader)\n",
    "#         metrics[\"epoch_losses\"].append(average_epoch_loss)\n",
    "\n",
    "#         # Average language-specific loss\n",
    "#         avg_hi_loss = (\n",
    "#             sum(metrics[\"language_specific_loss\"][\"hindi\"]) /\n",
    "#             len(metrics[\"language_specific_loss\"][\"hindi\"]) \n",
    "#             if metrics[\"language_specific_loss\"][\"hindi\"] else 0\n",
    "#         )\n",
    "#         avg_te_loss = (\n",
    "#             sum(metrics[\"language_specific_loss\"][\"telugu\"]) /\n",
    "#             len(metrics[\"language_specific_loss\"][\"telugu\"]) \n",
    "#             if metrics[\"language_specific_loss\"][\"telugu\"] else 0\n",
    "#         )\n",
    "\n",
    "#         # Print epoch loss\n",
    "#         print(f\"Epoch {epoch + 1} Loss: {average_epoch_loss:.4f}\")\n",
    "#         print(f\"Epoch {epoch + 1} Average Hindi Language Loss: {avg_hi_loss:.4f}\")\n",
    "#         print(f\"Epoch {epoch + 1} Average Telugu Language Loss: {avg_te_loss:.4f}\")\n",
    "\n",
    "#     print(\"Training complete.\")\n",
    "#     return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs, learning_rate, tokenizer, pad_token_id, device):\n",
    "    \"\"\" Function to train the Transformer model for multilingual translation.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Transformer model.\n",
    "        dataloader (DataLoader): The DataLoader providing the training data.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        tokenizer (AutoTokenizer): Tokenizer for text processing.\n",
    "        pad_token_id (int): Padding token ID for calculating loss.\n",
    "        device (str): Device for training ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Module): The trained model.\n",
    "        metrics (dict): Training metrics, including epoch and batch losses.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    metrics = {\"epoch_losses\": [], \"batch_loss\": [], \"language_specific_loss\": {\"hindi\": [], \"telugu\": []}}\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        batch_losses = []  # Track batch losses for this epoch\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "            # Move batch data to device\n",
    "            encoder_input_ids = batch[\"encoder_input_ids\"].to(device)\n",
    "            encoder_attention_mask = batch[\"encoder_attention_mask\"].to(device)\n",
    "            decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
    "            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
    "            cross_attention_mask = batch[\"cross_attention_mask\"].to(device)\n",
    "\n",
    "            # Prepare decoder target by shifting left\n",
    "            target_ids = torch.cat([decoder_input_ids[:, 1:], torch.full((decoder_input_ids.size(0), 1), pad_token_id, dtype=torch.long, device=device)], dim=1)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(\n",
    "                encoder_input_ids=encoder_input_ids,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_self_attention_mask=decoder_attention_mask,\n",
    "                cross_attention_mask=cross_attention_mask,\n",
    "            )\n",
    "\n",
    "            # Reshape logits and target for loss calculation\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            target_ids = target_ids.view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, target_ids)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log batch loss\n",
    "            metrics[\"batch_loss\"].append(loss.item())\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "            # Print every 400 batches\n",
    "            if (batch_idx + 1) % 400 == 0:\n",
    "                hi_count = (decoder_input_ids[:, 1] == hi_id).sum().item()\n",
    "                te_count = (decoder_input_ids[:, 1] == te_id).sum().item()\n",
    "                print(f\"Batch {batch_idx + 1}: Hindi: {hi_count}, Telugu: {te_count}\")\n",
    "\n",
    "        # Accumulate epoch loss\n",
    "        epoch_loss = sum(batch_losses)\n",
    "\n",
    "        # Language Specific Loss\n",
    "        language_specific_batch_losses = {\"hindi\": [], \"telugu\": []}\n",
    "        for i in range(decoder_input_ids.size(0)):\n",
    "            if decoder_input_ids[i, 1].item() == hi_id:\n",
    "                language_specific_batch_losses[\"hindi\"].append(loss.item())\n",
    "            elif decoder_input_ids[i, 1].item() == te_id:\n",
    "                language_specific_batch_losses[\"telugu\"].append(loss.item())\n",
    "\n",
    "        # Calculate average epoch loss and language specific losses\n",
    "        average_epoch_loss = epoch_loss / len(dataloader)\n",
    "        avg_hi_loss = sum(language_specific_batch_losses[\"hindi\"]) / len(language_specific_batch_losses[\"hindi\"]) if language_specific_batch_losses[\"hindi\"] else 0\n",
    "        avg_te_loss = sum(language_specific_batch_losses[\"telugu\"]) / len(language_specific_batch_losses[\"telugu\"]) if language_specific_batch_losses[\"telugu\"] else 0\n",
    "\n",
    "        # Print epoch loss and language-specific losses\n",
    "        print(f\"Epoch {epoch + 1} Loss: {average_epoch_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch + 1} Average Hindi Language Loss: {avg_hi_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch + 1} Average Telugu Language Loss: {avg_te_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch + 1} Average Batch Loss: {sum(batch_losses) / len(batch_losses):.4f}\")\n",
    "\n",
    "        # Log epoch loss and language-specific losses to metrics\n",
    "        metrics[\"epoch_losses\"].append(average_epoch_loss)\n",
    "        metrics[\"language_specific_loss\"][\"hindi\"].append(avg_hi_loss)\n",
    "        metrics[\"language_specific_loss\"][\"telugu\"].append(avg_te_loss)\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 400/5166 [42:58<5:57:00,  4.49s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 800/5166 [1:11:03<5:08:17,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 1200/5166 [1:38:16<4:25:28,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Hindi: 5, Telugu: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 1600/5166 [2:04:58<4:07:27,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1600: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 2000/5166 [2:32:56<3:17:41,  3.75s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 2400/5166 [2:59:23<3:02:05,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2400: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 2800/5166 [3:24:56<2:24:26,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2800: Hindi: 3, Telugu: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 3200/5166 [4:05:50<2:17:37,  4.20s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3200: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 3600/5166 [7:59:12<5:13:54, 12.03s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3600: Hindi: 5, Telugu: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 4000/5166 [11:19:15<1:21:51,  4.21s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 4400/5166 [12:18:36<51:06,  4.00s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4400: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 4800/5166 [12:44:53<26:31,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4800: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5166/5166 [13:10:03<00:00,  9.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.3876\n",
      "Epoch 1 Average Hindi Language Loss: 1.2992\n",
      "Epoch 1 Average Telugu Language Loss: 1.2992\n",
      "Epoch 1 Average Batch Loss: 2.3876\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 400/5166 [26:25<5:21:14,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 800/5166 [1:45:39<4:37:48,  3.82s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 1200/5166 [2:18:43<5:54:36,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 1600/5166 [3:43:39<39:29:40, 39.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1600: Hindi: 5, Telugu: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 2000/5166 [7:58:15<3:11:23,  3.63s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 2400/5166 [8:27:07<3:20:41,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2400: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 2800/5166 [9:20:03<3:25:54,  5.22s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2800: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 3200/5166 [10:36:54<2:13:07,  4.06s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3200: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 3600/5166 [11:22:22<13:06:17, 30.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3600: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 4000/5166 [13:41:18<6:14:23, 19.27s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 4400/5166 [19:40:26<8:49:19, 41.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4400: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 4800/5166 [22:56:36<24:14,  3.97s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4800: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5166/5166 [24:41:14<00:00, 17.20s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.4358\n",
      "Epoch 2 Average Hindi Language Loss: 0.2723\n",
      "Epoch 2 Average Telugu Language Loss: 0.2723\n",
      "Epoch 2 Average Batch Loss: 0.4358\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 400/5166 [1:03:39<4:49:50,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 800/5166 [3:17:05<16:51:59, 13.91s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 1200/5166 [8:05:17<324:35:46, 294.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 1600/5166 [13:15:59<42:00:44, 42.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1600: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 2000/5166 [22:49:50<3:23:58,  3.87s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 2400/5166 [24:47:22<6:11:46,  8.06s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2400: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 2800/5166 [25:51:02<2:23:41,  3.64s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2800: Hindi: 11, Telugu: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 3200/5166 [29:07:15<10:51:41, 19.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3200: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 3600/5166 [32:47:10<4:31:20, 10.40s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3600: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 4000/5166 [35:24:38<1:30:27,  4.65s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 4400/5166 [36:51:45<45:39,  3.58s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4400: Hindi: 5, Telugu: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 4800/5166 [37:44:32<23:43,  3.89s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4800: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5166/5166 [38:11:00<00:00, 26.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.2116\n",
      "Epoch 3 Average Hindi Language Loss: 0.1375\n",
      "Epoch 3 Average Telugu Language Loss: 0.1375\n",
      "Epoch 3 Average Batch Loss: 0.2116\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 400/5166 [1:39:13<6:42:50,  5.07s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 800/5166 [5:32:11<9:38:13,  7.95s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 1200/5166 [6:06:48<5:01:41,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 1600/5166 [6:38:33<4:17:57,  4.34s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1600: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 2000/5166 [7:42:12<3:57:27,  4.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 2400/5166 [8:58:46<2:48:09,  3.65s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2400: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 2800/5166 [9:41:08<3:10:52,  4.84s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2800: Hindi: 11, Telugu: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 3200/5166 [10:19:03<2:20:32,  4.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3200: Hindi: 5, Telugu: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 3600/5166 [11:23:41<1:38:13,  3.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3600: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 4000/5166 [12:50:22<4:40:40, 14.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 4400/5166 [13:59:15<52:30,  4.11s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4400: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 4800/5166 [15:40:53<25:15,  4.14s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4800: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5166/5166 [16:28:16<00:00, 11.48s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.1389\n",
      "Epoch 4 Average Hindi Language Loss: 0.0759\n",
      "Epoch 4 Average Telugu Language Loss: 0.0759\n",
      "Epoch 4 Average Batch Loss: 0.1389\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 400/5166 [2:08:39<4:35:07,  3.46s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 800/5166 [4:25:39<4:28:26,  3.69s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 1200/5166 [8:28:42<9:12:13,  8.35s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Hindi: 5, Telugu: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 1600/5166 [12:52:18<5:22:39,  5.43s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1600: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 2000/5166 [14:17:35<7:38:51,  8.70s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 2400/5166 [15:10:57<7:11:10,  9.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2400: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 2800/5166 [16:40:15<3:59:48,  6.08s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2800: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 3200/5166 [19:36:10<3:32:23,  6.48s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3200: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 3600/5166 [20:30:44<2:10:51,  5.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3600: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 4000/5166 [21:43:03<2:59:18,  9.23s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000: Hindi: 13, Telugu: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 4400/5166 [23:20:19<41:57,  3.29s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4400: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 4800/5166 [23:46:35<24:30,  4.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4800: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5166/5166 [24:11:08<00:00, 16.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.1092\n",
      "Epoch 5 Average Hindi Language Loss: 0.2909\n",
      "Epoch 5 Average Telugu Language Loss: 0.2909\n",
      "Epoch 5 Average Batch Loss: 0.1092\n",
      "Epoch 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 400/5166 [2:51:42<4:45:39,  3.60s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 800/5166 [6:07:52<4:49:35,  3.98s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 1200/5166 [11:10:27<22:47:55, 20.69s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 1600/5166 [15:10:45<3:57:46,  4.00s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1600: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 2000/5166 [17:33:47<46:36:55, 53.01s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 2400/5166 [22:34:02<6:08:24,  7.99s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2400: Hindi: 5, Telugu: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 2800/5166 [23:53:36<3:14:37,  4.94s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2800: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 3200/5166 [25:42:08<1:59:38,  3.65s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3200: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 3600/5166 [26:17:15<2:32:00,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3600: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 4000/5166 [28:22:37<69:46:00, 215.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000: Hindi: 11, Telugu: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 4400/5166 [31:41:23<59:11,  4.64s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4400: Hindi: 5, Telugu: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 4800/5166 [32:52:22<20:22,  3.34s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4800: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5166/5166 [33:20:29<00:00, 23.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.0930\n",
      "Epoch 6 Average Hindi Language Loss: 0.0984\n",
      "Epoch 6 Average Telugu Language Loss: 0.0984\n",
      "Epoch 6 Average Batch Loss: 0.0930\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 400/5166 [1:13:27<15:37:07, 11.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 800/5166 [5:01:57<182:31:31, 150.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Hindi: 3, Telugu: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 1200/5166 [7:14:06<17:26:54, 15.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Hindi: 11, Telugu: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 1600/5166 [10:03:25<31:37:32, 31.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1600: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 2000/5166 [12:41:47<35:53:27, 40.81s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 2400/5166 [15:53:40<26:26:06, 34.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2400: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 2800/5166 [18:03:39<2:23:04,  3.63s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2800: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 3200/5166 [20:53:04<5:09:57,  9.46s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3200: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 3600/5166 [25:57:03<30:45:16, 70.70s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3600: Hindi: 12, Telugu: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 4000/5166 [28:03:09<2:47:37,  8.63s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 4400/5166 [30:50:39<52:39,  4.13s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4400: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 4800/5166 [31:53:40<1:09:48, 11.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4800: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5166/5166 [33:29:46<00:00, 23.34s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.0814\n",
      "Epoch 7 Average Hindi Language Loss: 0.0917\n",
      "Epoch 7 Average Telugu Language Loss: 0.0917\n",
      "Epoch 7 Average Batch Loss: 0.0814\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 400/5166 [33:37<6:26:23,  4.86s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 800/5166 [1:23:26<7:37:31,  6.29s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 1200/5166 [2:45:19<6:00:47,  5.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 1600/5166 [5:27:14<3:53:11,  3.92s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1600: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 2000/5166 [10:01:03<42:11:08, 47.97s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000: Hindi: 6, Telugu: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 2400/5166 [11:39:17<2:50:23,  3.70s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2400: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 2800/5166 [12:24:45<3:12:34,  4.88s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2800: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 3200/5166 [12:55:53<1:58:02,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3200: Hindi: 7, Telugu: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 3600/5166 [13:20:23<1:37:04,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3600: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 4000/5166 [14:15:00<2:11:16,  6.76s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000: Hindi: 8, Telugu: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 4400/5166 [14:48:55<1:06:57,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4400: Hindi: 10, Telugu: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 4800/5166 [20:49:40<24:19,  3.99s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4800: Hindi: 9, Telugu: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5166/5166 [21:27:40<00:00, 14.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.0750\n",
      "Epoch 8 Average Hindi Language Loss: 0.2074\n",
      "Epoch 8 Average Telugu Language Loss: 0.2074\n",
      "Epoch 8 Average Batch Loss: 0.0750\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_model, training_metrics = train_model(\n",
    "    model=transformer_model,\n",
    "    dataloader=translation_dataloader,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the trained model state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at /Users/venu/Documents/Productivity/Pytorch Tutorials/Attention is All You Need Paper Replication/transformer_model_eng_to_hin_tel_8epc_4hd_2l_256d_512ffn_42k.pt\n"
     ]
    }
   ],
   "source": [
    "# Directory to save the model\n",
    "save_directory = \"/Users/venu/Documents/Productivity/Pytorch Tutorials/Attention is All You Need Paper Replication\"\n",
    "model_name = \"transformer_model_eng_to_hin_tel_8epc_4hd_2l_256d_512ffn_42k\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Full path to save the model\n",
    "model_save_path = os.path.join(save_directory, model_name + \".pt\")\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(trained_model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Model saved successfully at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "drop_prob = 0.1\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "num_epochs = 8\n",
    "max_seq_length = 300\n",
    "ffn_hidden = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # if there is one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119549"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size + len(tokenizer.additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/r3k6h0y11431_5th45hwjmjr0000gn/T/ipykernel_1689/1838554484.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trained_model.load_state_dict(torch.load(model_load_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "trained_model = TransformerModel(d_model=d_model,\n",
    "                                     vocab_size= total_vocab_size,\n",
    "                                     num_heads= num_heads,\n",
    "                                     num_layers= num_layers,\n",
    "                                     drop_prob= drop_prob,\n",
    "                                     max_seq_length= max_seq_length,\n",
    "                                     encoder_positional_encoder= encoder_positional_encoder,\n",
    "                                     decoder_positional_encoder= decoder_positional_encoder,\n",
    "                                     device=device)\n",
    "\n",
    "# Force update the encoder and decoder embedding layers to match the expected size\n",
    "trained_model.encoder_embedding = nn.Embedding(119549, 256)  # Encoder embedding\n",
    "trained_model.decoder_embedding = nn.Embedding(119549, 256)  # Decoder embedding\n",
    "\n",
    "# Load state dictionary into the model\n",
    "model_load_path = \"/Users/venu/Documents/Productivity/Pytorch Tutorials/Attention is All You Need Paper Replication/transformer_model_eng_to_hin_tel_8epc_4hd_2l_256d_512ffn_42k.pt\"\n",
    "trained_model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
    "trained_model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder embedding shape: torch.Size([119549, 256])\n",
      "Decoder embedding shape: torch.Size([119549, 256])\n"
     ]
    }
   ],
   "source": [
    "# Verify the embedding shapes\n",
    "print(\"Encoder embedding shape:\", trained_model.encoder_embedding.weight.shape)\n",
    "print(\"Decoder embedding shape:\", trained_model.decoder_embedding.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to /Users/venu/Documents/Productivity/Pytorch Tutorials/Attention is All You Need Paper Replication/transformer_translation.pth\n"
     ]
    }
   ],
   "source": [
    "# # saving the model \n",
    "# model_save_dir = '/Users/venu/Documents/Productivity/Pytorch Tutorials/Attention is All You Need Paper Replication'\n",
    "# os.makedirs(model_save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "# model_save_path = os.path.join(model_save_dir, \"transformer_translation.pth\")\n",
    "\n",
    "# # saving the model's state dictionary\n",
    "# torch.save(trained_model.state_dict(), model_save_path)\n",
    "\n",
    "# print(f\"Model saved successfully to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### trial with top k inferencing technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_k_sampling(logits, k):\n",
    "    # Get the top k probabilities and their indices\n",
    "    top_k_probs, top_k_indices = torch.topk(logits, k, dim=-1)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    top_k_probs = torch.softmax(top_k_probs, dim=-1)\n",
    "    \n",
    "    # Sample from the top k tokens\n",
    "    next_token_id = torch.multinomial(top_k_probs, num_samples=1)\n",
    "    \n",
    "    # Map sampled token index back to original token indices\n",
    "    next_token_id = top_k_indices.gather(dim=-1, index=next_token_id)\n",
    "    \n",
    "    return next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentence = \"trying to convert it into telugu or hindi language.\"\n",
    "target_language = \"telugu\"\n",
    "\n",
    "source_tokens = tokenizer(source_sentence, return_tensors=\"pt\", padding= True, truncation= True, max_length= max_seq_length)\n",
    "language_tokens = {\n",
    "     \"hindi\":\"<_hi_>\",\n",
    "     \"telugu\":\"<_te_>\"\n",
    " }\n",
    "target_lang_token = language_tokens[target_language]\n",
    "\n",
    "#Get the token ids for language tokens\n",
    "lang_token_ids = {lang:tokenizer.convert_tokens_to_ids(token) for lang, token in language_tokens.items()}\n",
    "\n",
    "# Define the target language token\n",
    "language_token_id = lang_token_ids[target_language]\n",
    "\n",
    "\n",
    "#Initialize decoder input with the target language token and start token\n",
    "decoder_input_ids = torch.tensor([[tokenizer.cls_token_id, language_token_id]], device=device) #[<start>, <te>]\n",
    "input_tokens = tokenizer(\n",
    "    text=[source_sentence],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "encoder_input_ids = input_tokens[\"input_ids\"].to(device)\n",
    "encoder_attention_mask = input_tokens[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def infer_translation(\n",
    "#     source_sentence, \n",
    "#     target_language, \n",
    "#     tokenizer, \n",
    "#     model, \n",
    "#     max_seq_length, \n",
    "#     device, \n",
    "#     eos_token_id=None, \n",
    "#     pad_token_id=None, \n",
    "#     k=5\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Function to translate a given source sentence into the target language.\n",
    "#     Args:\n",
    "#         source_sentence (str): The source sentence to be translated.\n",
    "#         target_language (str): The target language (e.g., \"hindi\", \"telugu\").\n",
    "#         tokenizer (AutoTokenizer): The tokenizer used for encoding/decoding.\n",
    "#         model (nn.Module): The trained transformer model.\n",
    "#         max_seq_length (int): Maximum sequence length for decoding.\n",
    "#         device (str): Device for inference ('cuda' or 'cpu').\n",
    "#         eos_token_id (int, optional): Token ID for the end-of-sentence token.\n",
    "#         pad_token_id (int, optional): Padding token ID.\n",
    "#         k (int): Top-K value for sampling.\n",
    "#     Returns:\n",
    "#         str: The translated sentence.\n",
    "#     \"\"\"\n",
    "#     # Prepare language-specific token\n",
    "#     language_tokens = {\"hindi\": \"<_hi_>\", \"telugu\": \"<_te_>\"}\n",
    "#     target_lang_token = language_tokens[target_language]\n",
    "    \n",
    "#     # Encode the source sentence and target language token\n",
    "#     encoder_input_ids = tokenizer.encode(source_sentence, return_tensors=\"pt\").to(device)\n",
    "#     decoder_input_ids = tokenizer.encode(target_lang_token, return_tensors=\"pt\").to(device)  # Start with lang token\n",
    "    \n",
    "#     # Initialize attention masks\n",
    "#     encoder_attention_mask = torch.ones_like(encoder_input_ids).to(device)  # Assuming no padding in input\n",
    "#     translated_tokens = []\n",
    "\n",
    "#     # Precompute the full look-ahead mask for decoder (self-attention)\n",
    "#     full_mask = torch.triu(torch.ones((max_seq_length, max_seq_length), device=device), diagonal=1)\n",
    "\n",
    "#     # Switch to eval mode\n",
    "#     model.eval()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for step in range(max_seq_length):  # Generate tokens up to max_seq_length\n",
    "#             current_seq_length = decoder_input_ids.size(1)\n",
    "            \n",
    "#             # Ensure the decoder sequence length does not exceed max_seq_length\n",
    "#             if current_seq_length > max_seq_length:\n",
    "#                 break\n",
    "            \n",
    "#             # Slice the full mask for current sequence length (self-attention)\n",
    "#             decoder_attention_mask = full_mask[:current_seq_length, :current_seq_length].unsqueeze(0)\n",
    "            \n",
    "#             # Cross-attention mask (align with encoder input length)\n",
    "#             cross_attention_mask = encoder_attention_mask.unsqueeze(1).expand(-1, current_seq_length, -1)\n",
    "            \n",
    "#             # Generate the next token\n",
    "#             decoder_output = model(\n",
    "#                 encoder_input_ids=encoder_input_ids,\n",
    "#                 decoder_input_ids=decoder_input_ids,\n",
    "#                 encoder_attention_mask=encoder_attention_mask,\n",
    "#                 decoder_self_attention_mask=decoder_attention_mask,\n",
    "#                 cross_attention_mask=cross_attention_mask,\n",
    "#             )\n",
    "\n",
    "#             # Get logits for the last token in the sequence\n",
    "#             decoder_output.shape\n",
    "#             logits = decoder_output[:, -1, :]\n",
    "            \n",
    "#             # Apply top-k sampling\n",
    "#             next_token_id = top_k_sampling(logits, k)\n",
    "            \n",
    "#             # Append predicted token to decoder_input_ids\n",
    "#             decoder_input_ids = torch.cat([decoder_input_ids, next_token_id.unsqueeze(0)], dim=1)\n",
    "#             translated_tokens.append(next_token_id.item())\n",
    "            \n",
    "#             # Stop if EOS token is predicted\n",
    "#             if next_token_id.item() == eos_token_id:\n",
    "#                 print(\"End token detected. Stopping generation.\")\n",
    "#                 break\n",
    "\n",
    "#     # Decode the translated tokens\n",
    "#     translated_sentence = tokenizer.decode(translated_tokens, skip_special_tokens=True)\n",
    "#     return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# translated_tokens = []\n",
    "\n",
    "# # Precompute the full look-ahead mask for decoder (self-attention)\n",
    "# full_mask = torch.triu(torch.ones((max_seq_length, max_seq_length), device=device), diagonal=1)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for _ in range(max_seq_length):  # Loop for max_seq_length\n",
    "#         current_seq_length = decoder_input_ids.size(1)\n",
    "        \n",
    "#         # Ensure the decoder sequence length does not exceed max_seq_length\n",
    "#         if current_seq_length > max_seq_length:\n",
    "#             break  # Prevent infinite loop\n",
    "        \n",
    "#         # Slice the full mask for current sequence length (self-attention)\n",
    "#         decoder_attention_mask = full_mask[:current_seq_length, :current_seq_length].unsqueeze(0)\n",
    "        \n",
    "#         # Cross-attention mask (match encoder input length)\n",
    "#         cross_attention_mask = encoder_attention_mask.unsqueeze(1).expand(-1, current_seq_length, -1)\n",
    "        \n",
    "#         # Generate the next token\n",
    "#         decoder_output = trained_model(\n",
    "#             encoder_input_ids=encoder_input_ids,\n",
    "#             decoder_input_ids=decoder_input_ids,\n",
    "#             encoder_attention_mask=encoder_attention_mask,\n",
    "#             decoder_self_attention_mask=decoder_attention_mask,\n",
    "#             cross_attention_mask=cross_attention_mask,\n",
    "#         )\n",
    "\n",
    "#         # Get the logits for the last token in the sequence\n",
    "#         logits = decoder_output[:, -1, :]\n",
    "        \n",
    "#         # Apply top-k sampling during token generation\n",
    "#         #k = 5 \n",
    "#         next_token_id = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "#         # Append predicted token to decoder_input_ids\n",
    "#         decoder_input_ids = torch.cat([decoder_input_ids, next_token_id.unsqueeze(-1)], dim=1)\n",
    "#         translated_tokens.append(next_token_id.item())\n",
    "        \n",
    "#         # Stop if end token is predicted\n",
    "#         if next_token_id.item() == tokenizer.sep_token_id or len(translated_tokens) >= max_seq_length:\n",
    "#             print(\"End token detected. Stopping generation.\")\n",
    "#             break\n",
    "# translated_text = tokenizer.decode(translated_tokens, skip_special_tokens=True)\n",
    "# print(f\"Decoded Tokens: {translated_tokens}\")\n",
    "# print(f\"Generated Text: {translated_text}\")\n",
    "# print(\"Translated Tokens:\", translated_tokens)\n",
    "# print(\"Translated Text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Token ID: 1208, Token Probability: 0.030877524986863136\n",
      "Next Token ID: 15697, Token Probability: 0.17420968413352966\n",
      "Next Token ID: 15697, Token Probability: 0.7931942939758301\n",
      "Next Token ID: 15697, Token Probability: 0.9757930636405945\n",
      "Next Token ID: 13652, Token Probability: 0.027840252965688705\n",
      "Next Token ID: 13652, Token Probability: 0.9920156002044678\n",
      "Next Token ID: 13652, Token Probability: 0.9936662316322327\n",
      "Next Token ID: 13652, Token Probability: 0.9932971596717834\n",
      "Next Token ID: 13652, Token Probability: 0.9603087306022644\n",
      "Next Token ID: 13652, Token Probability: 0.9896225929260254\n",
      "Next Token ID: 13652, Token Probability: 0.9945001602172852\n",
      "Next Token ID: 13652, Token Probability: 0.9952658414840698\n",
      "Next Token ID: 13652, Token Probability: 0.9957367181777954\n",
      "Next Token ID: 13652, Token Probability: 0.9918225407600403\n",
      "Next Token ID: 13652, Token Probability: 0.9859959483146667\n",
      "Next Token ID: 13652, Token Probability: 0.9931080341339111\n",
      "Next Token ID: 13652, Token Probability: 0.9968007802963257\n",
      "Next Token ID: 13652, Token Probability: 0.9861883521080017\n",
      "Next Token ID: 13652, Token Probability: 0.988379716873169\n",
      "Next Token ID: 13652, Token Probability: 0.9979885816574097\n",
      "Next Token ID: 13652, Token Probability: 0.9953992962837219\n",
      "Next Token ID: 13652, Token Probability: 0.998241662979126\n",
      "Next Token ID: 13652, Token Probability: 0.9357299208641052\n",
      "Next Token ID: 95870, Token Probability: 0.04581192135810852\n",
      "Next Token ID: 95870, Token Probability: 0.9999250173568726\n",
      "Next Token ID: 95870, Token Probability: 0.9999593496322632\n",
      "Next Token ID: 95870, Token Probability: 0.9999284744262695\n",
      "Next Token ID: 95870, Token Probability: 0.9998817443847656\n",
      "Next Token ID: 95870, Token Probability: 0.9998987913131714\n",
      "Next Token ID: 95870, Token Probability: 0.9999415874481201\n",
      "Next Token ID: 95870, Token Probability: 0.9997966885566711\n",
      "Next Token ID: 95870, Token Probability: 0.9999189376831055\n",
      "Next Token ID: 95870, Token Probability: 0.9998249411582947\n",
      "Next Token ID: 95870, Token Probability: 0.9998013377189636\n",
      "Next Token ID: 95870, Token Probability: 0.999826967716217\n",
      "Next Token ID: 95870, Token Probability: 0.9998388290405273\n",
      "Next Token ID: 95870, Token Probability: 0.9998337030410767\n",
      "Next Token ID: 95870, Token Probability: 0.9998980760574341\n",
      "Next Token ID: 95870, Token Probability: 0.9999490976333618\n",
      "Next Token ID: 95870, Token Probability: 0.9997007846832275\n",
      "Next Token ID: 95870, Token Probability: 0.9998364448547363\n",
      "Next Token ID: 95870, Token Probability: 0.9999377727508545\n",
      "Next Token ID: 95870, Token Probability: 0.9998050332069397\n",
      "Next Token ID: 95870, Token Probability: 0.9996366500854492\n",
      "Next Token ID: 95870, Token Probability: 0.9997493624687195\n",
      "Next Token ID: 95870, Token Probability: 0.999687910079956\n",
      "Next Token ID: 95870, Token Probability: 0.9994860887527466\n",
      "Next Token ID: 95870, Token Probability: 0.9995928406715393\n",
      "Next Token ID: 95870, Token Probability: 0.9991567134857178\n",
      "Next Token ID: 95870, Token Probability: 0.9997974038124084\n",
      "Next Token ID: 95870, Token Probability: 0.9989858269691467\n",
      "Next Token ID: 95870, Token Probability: 0.9987903237342834\n",
      "Next Token ID: 95870, Token Probability: 0.9996575117111206\n",
      "Next Token ID: 95870, Token Probability: 0.9996954202651978\n",
      "Next Token ID: 95870, Token Probability: 0.9997405409812927\n",
      "Next Token ID: 95870, Token Probability: 0.9997439980506897\n",
      "Next Token ID: 95870, Token Probability: 0.9996370077133179\n",
      "Next Token ID: 95870, Token Probability: 0.9996576309204102\n",
      "Next Token ID: 95870, Token Probability: 0.9998155236244202\n",
      "Next Token ID: 95870, Token Probability: 0.9996814727783203\n",
      "Next Token ID: 95870, Token Probability: 0.9994937181472778\n",
      "Next Token ID: 95870, Token Probability: 0.9994505047798157\n",
      "Next Token ID: 95870, Token Probability: 0.9992377758026123\n",
      "Next Token ID: 95870, Token Probability: 0.9990856647491455\n",
      "Next Token ID: 95870, Token Probability: 0.9996476173400879\n",
      "Next Token ID: 95870, Token Probability: 0.9995478987693787\n",
      "Next Token ID: 95870, Token Probability: 0.9992087483406067\n",
      "Next Token ID: 95870, Token Probability: 0.9986538887023926\n",
      "Next Token ID: 95870, Token Probability: 0.9993851184844971\n",
      "Next Token ID: 95870, Token Probability: 0.9987879395484924\n",
      "Next Token ID: 95870, Token Probability: 0.9988585710525513\n",
      "Next Token ID: 95870, Token Probability: 0.9997648596763611\n",
      "Next Token ID: 95870, Token Probability: 0.9984732270240784\n",
      "Next Token ID: 95870, Token Probability: 0.9985829591751099\n",
      "Next Token ID: 95870, Token Probability: 0.9990344047546387\n",
      "Next Token ID: 95870, Token Probability: 0.997687816619873\n",
      "Next Token ID: 95870, Token Probability: 0.999548614025116\n",
      "Next Token ID: 95870, Token Probability: 0.9966896772384644\n",
      "Next Token ID: 95870, Token Probability: 0.9954578876495361\n",
      "Next Token ID: 95870, Token Probability: 0.9971164464950562\n",
      "Next Token ID: 95870, Token Probability: 0.9959915280342102\n",
      "Next Token ID: 95870, Token Probability: 0.9969276785850525\n",
      "Next Token ID: 95870, Token Probability: 0.9942671656608582\n",
      "Next Token ID: 95870, Token Probability: 0.9979609251022339\n",
      "Next Token ID: 95870, Token Probability: 0.9983230233192444\n",
      "Next Token ID: 95870, Token Probability: 0.9986348748207092\n",
      "Next Token ID: 95870, Token Probability: 0.9950042366981506\n",
      "Next Token ID: 95870, Token Probability: 0.9977891445159912\n",
      "Next Token ID: 95870, Token Probability: 0.9974791407585144\n",
      "Next Token ID: 95870, Token Probability: 0.9985008239746094\n",
      "Next Token ID: 95870, Token Probability: 0.9974737763404846\n",
      "Next Token ID: 95870, Token Probability: 0.9976968169212341\n",
      "Next Token ID: 95870, Token Probability: 0.9978110194206238\n",
      "Next Token ID: 95870, Token Probability: 0.9995025396347046\n",
      "Next Token ID: 95870, Token Probability: 0.9997496008872986\n",
      "Next Token ID: 95870, Token Probability: 0.998108983039856\n",
      "Next Token ID: 95870, Token Probability: 0.9988160133361816\n",
      "Next Token ID: 95870, Token Probability: 0.9976598024368286\n",
      "Next Token ID: 95870, Token Probability: 0.9987201690673828\n",
      "Next Token ID: 95870, Token Probability: 0.9978856444358826\n",
      "Next Token ID: 95870, Token Probability: 0.99875807762146\n",
      "Next Token ID: 95870, Token Probability: 0.9936087131500244\n",
      "Next Token ID: 95870, Token Probability: 0.9965182542800903\n",
      "Next Token ID: 95870, Token Probability: 0.993707001209259\n",
      "Next Token ID: 95870, Token Probability: 0.996293842792511\n",
      "Next Token ID: 95870, Token Probability: 0.9920175075531006\n",
      "Next Token ID: 95870, Token Probability: 0.9981829524040222\n",
      "Next Token ID: 95870, Token Probability: 0.9991950392723083\n",
      "Next Token ID: 95870, Token Probability: 0.9988781809806824\n",
      "Next Token ID: 95870, Token Probability: 0.9992856383323669\n",
      "Next Token ID: 95870, Token Probability: 0.9975326061248779\n",
      "Next Token ID: 95870, Token Probability: 0.9961141347885132\n",
      "Next Token ID: 95870, Token Probability: 0.9994533658027649\n",
      "Next Token ID: 95870, Token Probability: 0.9986199140548706\n",
      "Next Token ID: 95870, Token Probability: 0.9993202686309814\n",
      "Next Token ID: 95870, Token Probability: 0.9967171549797058\n",
      "Next Token ID: 95870, Token Probability: 0.9963735938072205\n",
      "Next Token ID: 95870, Token Probability: 0.9991357922554016\n",
      "Next Token ID: 95870, Token Probability: 0.9959819316864014\n",
      "Next Token ID: 95870, Token Probability: 0.9972416162490845\n",
      "Next Token ID: 95870, Token Probability: 0.9979546070098877\n",
      "Next Token ID: 95870, Token Probability: 0.999554455280304\n",
      "Next Token ID: 95870, Token Probability: 0.9993512034416199\n",
      "Next Token ID: 95870, Token Probability: 0.996532678604126\n",
      "Next Token ID: 95870, Token Probability: 0.9979401230812073\n",
      "Next Token ID: 95870, Token Probability: 0.9965600371360779\n",
      "Next Token ID: 95870, Token Probability: 0.9966714382171631\n",
      "Next Token ID: 95870, Token Probability: 0.9990264177322388\n",
      "Next Token ID: 95870, Token Probability: 0.9993906021118164\n",
      "Next Token ID: 95870, Token Probability: 0.9993653893470764\n",
      "Next Token ID: 95870, Token Probability: 0.9981604218482971\n",
      "Next Token ID: 95870, Token Probability: 0.9918872714042664\n",
      "Next Token ID: 95870, Token Probability: 0.995661199092865\n",
      "Next Token ID: 95870, Token Probability: 0.9984416365623474\n",
      "Next Token ID: 95870, Token Probability: 0.9980023503303528\n",
      "Next Token ID: 95870, Token Probability: 0.9984831213951111\n",
      "Next Token ID: 95870, Token Probability: 0.9976643323898315\n",
      "Next Token ID: 95870, Token Probability: 0.9977425336837769\n",
      "Next Token ID: 95870, Token Probability: 0.9979273080825806\n",
      "Next Token ID: 95870, Token Probability: 0.9968796968460083\n",
      "Next Token ID: 95870, Token Probability: 0.9961714148521423\n",
      "Next Token ID: 13652, Token Probability: 0.9952453970909119\n",
      "Next Token ID: 95870, Token Probability: 0.998694121837616\n",
      "Next Token ID: 95870, Token Probability: 0.7044276595115662\n",
      "Next Token ID: 95870, Token Probability: 0.9974066615104675\n",
      "Next Token ID: 15697, Token Probability: 0.9446075558662415\n",
      "Next Token ID: 95870, Token Probability: 0.9800546169281006\n",
      "Next Token ID: 95870, Token Probability: 0.9980295300483704\n",
      "Next Token ID: 95870, Token Probability: 0.9989346861839294\n",
      "Next Token ID: 13652, Token Probability: 0.8829668760299683\n",
      "Next Token ID: 95870, Token Probability: 0.9993733763694763\n",
      "Next Token ID: 15697, Token Probability: 0.8959333896636963\n",
      "Next Token ID: 95870, Token Probability: 0.952292799949646\n",
      "Next Token ID: 95870, Token Probability: 0.8978947997093201\n",
      "Next Token ID: 95870, Token Probability: 0.9920458793640137\n",
      "Next Token ID: 95870, Token Probability: 0.989032506942749\n",
      "Next Token ID: 95870, Token Probability: 0.9940914511680603\n",
      "Next Token ID: 15697, Token Probability: 0.8752864599227905\n",
      "Next Token ID: 95870, Token Probability: 0.9599465131759644\n",
      "Next Token ID: 95870, Token Probability: 0.9997829794883728\n",
      "Next Token ID: 95870, Token Probability: 0.9991605281829834\n",
      "Next Token ID: 95870, Token Probability: 0.9400098919868469\n",
      "Next Token ID: 13652, Token Probability: 0.9038398265838623\n",
      "Next Token ID: 95870, Token Probability: 0.9981606602668762\n",
      "Next Token ID: 95870, Token Probability: 0.9925478100776672\n",
      "Next Token ID: 15697, Token Probability: 0.9906574487686157\n",
      "Next Token ID: 95870, Token Probability: 0.9894967079162598\n",
      "Next Token ID: 15697, Token Probability: 0.9935030341148376\n",
      "Next Token ID: 13652, Token Probability: 0.9999001026153564\n",
      "Next Token ID: 95870, Token Probability: 0.9988024234771729\n",
      "Next Token ID: 13652, Token Probability: 0.3324304223060608\n",
      "Next Token ID: 95870, Token Probability: 0.9980161190032959\n",
      "Next Token ID: 95870, Token Probability: 0.9887579083442688\n",
      "Next Token ID: 31491, Token Probability: 0.0009017512784339488\n",
      "Next Token ID: 95870, Token Probability: 0.7805379629135132\n",
      "Next Token ID: 31491, Token Probability: 0.7744346261024475\n",
      "Next Token ID: 15697, Token Probability: 0.7252767086029053\n",
      "Next Token ID: 95870, Token Probability: 0.9547658562660217\n",
      "Next Token ID: 15697, Token Probability: 0.9554104804992676\n",
      "Next Token ID: 95870, Token Probability: 0.9822662472724915\n",
      "Next Token ID: 31491, Token Probability: 0.6089338064193726\n",
      "Next Token ID: 95870, Token Probability: 0.208903968334198\n",
      "Next Token ID: 31491, Token Probability: 0.9812065958976746\n",
      "Next Token ID: 95870, Token Probability: 0.6041995882987976\n",
      "Next Token ID: 15697, Token Probability: 0.9488905668258667\n",
      "Next Token ID: 95870, Token Probability: 0.9740942120552063\n",
      "Next Token ID: 31491, Token Probability: 0.9992650151252747\n",
      "Next Token ID: 95870, Token Probability: 0.7875885963439941\n",
      "Next Token ID: 31491, Token Probability: 0.9584643840789795\n",
      "Next Token ID: 15697, Token Probability: 0.9304819107055664\n",
      "Next Token ID: 95870, Token Probability: 0.9604007601737976\n",
      "Next Token ID: 15697, Token Probability: 0.9283939003944397\n",
      "Next Token ID: 95870, Token Probability: 0.9818861484527588\n",
      "Next Token ID: 31491, Token Probability: 0.9857380390167236\n",
      "Next Token ID: 75099, Token Probability: 0.055211391299963\n",
      "Next Token ID: 95870, Token Probability: 0.7910630106925964\n",
      "Next Token ID: 95870, Token Probability: 0.801101803779602\n",
      "Next Token ID: 15697, Token Probability: 0.9954955577850342\n",
      "Next Token ID: 95870, Token Probability: 0.9828135967254639\n",
      "Next Token ID: 31491, Token Probability: 0.9862073659896851\n",
      "Next Token ID: 95870, Token Probability: 0.9197512865066528\n",
      "Next Token ID: 15697, Token Probability: 0.783907413482666\n",
      "Next Token ID: 95870, Token Probability: 0.9444547295570374\n",
      "Next Token ID: 95870, Token Probability: 0.9791361689567566\n",
      "Next Token ID: 95870, Token Probability: 0.9869640469551086\n",
      "Next Token ID: 31491, Token Probability: 0.9947481751441956\n",
      "Next Token ID: 95870, Token Probability: 0.9072411060333252\n",
      "Next Token ID: 15697, Token Probability: 0.4819754958152771\n",
      "Next Token ID: 95870, Token Probability: 0.9748618006706238\n",
      "Next Token ID: 95870, Token Probability: 0.9725956320762634\n",
      "Next Token ID: 95870, Token Probability: 0.9826332926750183\n",
      "Next Token ID: 95870, Token Probability: 0.9454928040504456\n",
      "Next Token ID: 31491, Token Probability: 0.9834127426147461\n",
      "Next Token ID: 15697, Token Probability: 0.8046972751617432\n",
      "Next Token ID: 95870, Token Probability: 0.9337406158447266\n",
      "Next Token ID: 13652, Token Probability: 0.7161792516708374\n",
      "Next Token ID: 95870, Token Probability: 0.9976073503494263\n",
      "Next Token ID: 13652, Token Probability: 0.9823317527770996\n",
      "Next Token ID: 95870, Token Probability: 0.9990335702896118\n",
      "Next Token ID: 31491, Token Probability: 0.9844895601272583\n",
      "Next Token ID: 95870, Token Probability: 0.25495609641075134\n",
      "Next Token ID: 15697, Token Probability: 0.9758182764053345\n",
      "Next Token ID: 95870, Token Probability: 0.9213026165962219\n",
      "Next Token ID: 95870, Token Probability: 0.9430650472640991\n",
      "Next Token ID: 13652, Token Probability: 0.9073776006698608\n",
      "Next Token ID: 95870, Token Probability: 0.9883080124855042\n",
      "Next Token ID: 31491, Token Probability: 0.9899362325668335\n",
      "Next Token ID: 15697, Token Probability: 0.9770588278770447\n",
      "Next Token ID: 95870, Token Probability: 0.9274179935455322\n",
      "Next Token ID: 95870, Token Probability: 0.9602302312850952\n",
      "Next Token ID: 95870, Token Probability: 0.04948980733752251\n",
      "Next Token ID: 31491, Token Probability: 0.9505331516265869\n",
      "Next Token ID: 13652, Token Probability: 0.9383745789527893\n",
      "Next Token ID: 95870, Token Probability: 0.9956238865852356\n",
      "Next Token ID: 15697, Token Probability: 0.7910630106925964\n",
      "Next Token ID: 95870, Token Probability: 0.9590387940406799\n",
      "Next Token ID: 95870, Token Probability: 0.9954450130462646\n",
      "Next Token ID: 95870, Token Probability: 0.9996272325515747\n",
      "Next Token ID: 95870, Token Probability: 0.9986390471458435\n",
      "Next Token ID: 95870, Token Probability: 0.9972112774848938\n",
      "Next Token ID: 95870, Token Probability: 0.9977766871452332\n",
      "Next Token ID: 95870, Token Probability: 0.9883459210395813\n",
      "Next Token ID: 95870, Token Probability: 0.9812737703323364\n",
      "Next Token ID: 31491, Token Probability: 0.9824620485305786\n",
      "Next Token ID: 81787, Token Probability: 0.018312692642211914\n",
      "Next Token ID: 15697, Token Probability: 0.9939519762992859\n",
      "Next Token ID: 95870, Token Probability: 0.9805201888084412\n",
      "Next Token ID: 13652, Token Probability: 0.9373498558998108\n",
      "Next Token ID: 95870, Token Probability: 0.9787111878395081\n",
      "Next Token ID: 13652, Token Probability: 0.9012524485588074\n",
      "Next Token ID: 95870, Token Probability: 0.9939711689949036\n",
      "Next Token ID: 31491, Token Probability: 0.9915319085121155\n",
      "Next Token ID: 95870, Token Probability: 0.8237559199333191\n",
      "Next Token ID: 15697, Token Probability: 0.963365912437439\n",
      "Next Token ID: 13652, Token Probability: 0.9951713681221008\n",
      "Next Token ID: 95870, Token Probability: 0.9991362690925598\n",
      "Next Token ID: 13652, Token Probability: 0.9592759013175964\n",
      "Next Token ID: 95870, Token Probability: 0.9950987696647644\n",
      "Next Token ID: 15697, Token Probability: 0.7127493023872375\n",
      "Next Token ID: 95870, Token Probability: 0.9874338507652283\n",
      "Next Token ID: 15697, Token Probability: 0.25408920645713806\n",
      "Next Token ID: 95870, Token Probability: 0.5179075002670288\n",
      "Next Token ID: 13652, Token Probability: 0.9811375141143799\n",
      "Next Token ID: 95870, Token Probability: 0.9978193044662476\n",
      "Next Token ID: 31491, Token Probability: 0.9880016446113586\n",
      "Next Token ID: 95870, Token Probability: 0.34426432847976685\n",
      "Next Token ID: 13652, Token Probability: 0.7920317053794861\n",
      "Next Token ID: 95870, Token Probability: 0.988520085811615\n",
      "Next Token ID: 31491, Token Probability: 0.997711181640625\n",
      "Next Token ID: 13652, Token Probability: 0.6571388244628906\n",
      "Next Token ID: 95870, Token Probability: 0.9907037615776062\n",
      "Next Token ID: 95870, Token Probability: 0.9701456427574158\n",
      "Next Token ID: 15697, Token Probability: 0.9827114939689636\n",
      "Next Token ID: 95870, Token Probability: 0.970070481300354\n",
      "Next Token ID: 15697, Token Probability: 0.9884440302848816\n",
      "Next Token ID: 13652, Token Probability: 0.999762237071991\n",
      "Next Token ID: 95870, Token Probability: 0.9956911206245422\n",
      "Next Token ID: 13652, Token Probability: 0.9966524243354797\n",
      "Next Token ID: 95870, Token Probability: 0.9979186654090881\n",
      "Next Token ID: 95870, Token Probability: 0.991978645324707\n",
      "Next Token ID: 95870, Token Probability: 0.9958613514900208\n",
      "Next Token ID: 31491, Token Probability: 0.9984093308448792\n",
      "Next Token ID: 15697, Token Probability: 0.9896224737167358\n",
      "Next Token ID: 95870, Token Probability: 0.9896407723426819\n",
      "Next Token ID: 13652, Token Probability: 0.9369831681251526\n",
      "Next Token ID: 95870, Token Probability: 0.9919306635856628\n",
      "Next Token ID: 13652, Token Probability: 0.9941984415054321\n",
      "Next Token ID: 95870, Token Probability: 0.9845616221427917\n",
      "Next Token ID: 31491, Token Probability: 0.9746175408363342\n",
      "Next Token ID: 1220, Token Probability: 0.026536690071225166\n",
      "Next Token ID: 95870, Token Probability: 0.18657954037189484\n",
      "Next Token ID: 15697, Token Probability: 0.8992317914962769\n",
      "Next Token ID: 95870, Token Probability: 0.9491983652114868\n",
      "Next Token ID: 95870, Token Probability: 0.7276981472969055\n",
      "Next Token ID: 31491, Token Probability: 0.9992896318435669\n",
      "Next Token ID: 15697, Token Probability: 0.9806813597679138\n",
      "Next Token ID: 13652, Token Probability: 0.9913209080696106\n",
      "Next Token ID: 95870, Token Probability: 0.9932877421379089\n",
      "Next Token ID: 15697, Token Probability: 0.35819461941719055\n",
      "Sequence length exceeded max_seq_length. Stopping generation.\n",
      "Decoded Tokens: [1208, 15697, 15697, 15697, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 13652, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 13652, 95870, 95870, 95870, 15697, 95870, 95870, 95870, 13652, 95870, 15697, 95870, 95870, 95870, 95870, 95870, 15697, 95870, 95870, 95870, 95870, 13652, 95870, 95870, 15697, 95870, 15697, 13652, 95870, 13652, 95870, 95870, 31491, 95870, 31491, 15697, 95870, 15697, 95870, 31491, 95870, 31491, 95870, 15697, 95870, 31491, 95870, 31491, 15697, 95870, 15697, 95870, 31491, 75099, 95870, 95870, 15697, 95870, 31491, 95870, 15697, 95870, 95870, 95870, 31491, 95870, 15697, 95870, 95870, 95870, 95870, 31491, 15697, 95870, 13652, 95870, 13652, 95870, 31491, 95870, 15697, 95870, 95870, 13652, 95870, 31491, 15697, 95870, 95870, 95870, 31491, 13652, 95870, 15697, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 95870, 31491, 81787, 15697, 95870, 13652, 95870, 13652, 95870, 31491, 95870, 15697, 13652, 95870, 13652, 95870, 15697, 95870, 15697, 95870, 13652, 95870, 31491, 95870, 13652, 95870, 31491, 13652, 95870, 95870, 15697, 95870, 15697, 13652, 95870, 13652, 95870, 95870, 95870, 31491, 15697, 95870, 13652, 95870, 13652, 95870, 31491, 1220, 95870, 15697, 95870, 95870, 31491, 15697, 13652, 95870, 15697]\n",
      "Generated Text: ఝుుుంంంంంంంంంంంంంంంంంంంచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుచుంచుచుచుుచుచుచుంచుుచుచుచుచుచుుచుచుచుచుంచుచుుచుుంచుంచుచు విధంగాచు విధంగాుచుుచు విధంగాచు విధంగాచుుచు విధంగాచు విధంగాుచుుచు విధంగా పిచుచుుచు విధంగాచుుచుచుచు విధంగాచుుచుచుచుచు విధంగాుచుంచుంచు విధంగాచుుచుచుంచు విధంగాుచుచుచు విధంగాంచుుచుచుచుచుచుచుచుచు విధంగాయ్యుచుంచుంచు విధంగాచుుంచుంచుుచుుచుంచు విధంగాచుంచు విధంగాంచుచుుచుుంచుంచుచుచు విధంగాుచుంచుంచు విధంగా పచుుచుచు విధంగాుంచుు\n"
     ]
    }
   ],
   "source": [
    "translated_tokens = []\n",
    "\n",
    "# Precompute the full look-ahead mask for decoder (self-attention)\n",
    "full_mask = torch.triu(torch.ones((max_seq_length, max_seq_length), device=device), diagonal=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_seq_length):  # Loop for max_seq_length\n",
    "        current_seq_length = decoder_input_ids.size(1)\n",
    "        \n",
    "        # Ensure the decoder sequence length does not exceed max_seq_length\n",
    "        if current_seq_length > max_seq_length:\n",
    "            print(\"Sequence length exceeded max_seq_length. Stopping generation.\")\n",
    "            break\n",
    "        \n",
    "        # Slice the full mask for current sequence length (self-attention)\n",
    "        decoder_attention_mask = full_mask[:current_seq_length, :current_seq_length].unsqueeze(0)\n",
    "        \n",
    "        # Cross-attention mask (match encoder input length)\n",
    "        cross_attention_mask = encoder_attention_mask.unsqueeze(1).expand(-1, current_seq_length, -1)\n",
    "        \n",
    "        # Generate the next token\n",
    "        decoder_output = trained_model(\n",
    "            encoder_input_ids=encoder_input_ids,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            decoder_self_attention_mask=decoder_attention_mask,\n",
    "            cross_attention_mask=cross_attention_mask,\n",
    "        )\n",
    "\n",
    "        # Get the logits for the last token in the sequence\n",
    "        logits = decoder_output[:, -1, :]\n",
    "        \n",
    "        # Penalize logits for repeating tokens\n",
    "        for token_id in set(translated_tokens):\n",
    "            logits[:, token_id] -= 1.0  # Penalize repeating tokens\n",
    "\n",
    "        # Apply top-k sampling\n",
    "        k = 5\n",
    "        next_token_id = top_k_sampling(logits, k)\n",
    "\n",
    "        # Debugging\n",
    "        #print(f\"Logits: {logits}\")\n",
    "        print(f\"Next Token ID: {next_token_id.item()}, Token Probability: {torch.softmax(logits, dim=-1)[0, next_token_id].item()}\")\n",
    "        \n",
    "        # Append predicted token to decoder_input_ids\n",
    "        #print(f\"Decoder input id shape is - {decoder_input_ids.shape}\")\n",
    "        #print(f\"Next Token id shape is {next_token_id.shape}\")\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_token_id], dim=1)\n",
    "        translated_tokens.append(next_token_id.item())\n",
    "        \n",
    "        # Stop if end token is predicted or probability is low\n",
    "        if next_token_id.item() == tokenizer.sep_token_id:\n",
    "            print(\"End token detected. Stopping generation.\")\n",
    "            break\n",
    "\n",
    "# Decode the translated tokens\n",
    "translated_text = tokenizer.decode(translated_tokens, skip_special_tokens=True)\n",
    "print(f\"Decoded Tokens: {translated_tokens}\")\n",
    "print(f\"Generated Text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with beam search inferencing technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def beam_search_decoding(\n",
    "#     model, tokenizer, source_sentence, target_language, max_seq_length, beam_width, device, eos_token_id\n",
    "# ):\n",
    "#     language_tokens = {\"hindi\": \"<_hi_>\", \"telugu\": \"<_te_>\"}\n",
    "#     target_lang_token = language_tokens[target_language]\n",
    "\n",
    "#     input_tokens = tokenizer(\n",
    "#         text=[source_sentence],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=max_seq_length,\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).to(device)\n",
    "\n",
    "#     encoder_input_ids = input_tokens[\"input_ids\"]\n",
    "#     encoder_attention_mask = input_tokens[\"attention_mask\"]\n",
    "\n",
    "#     decoder_input_ids = torch.tensor(\n",
    "#         [[tokenizer.cls_token_id, tokenizer.convert_tokens_to_ids(target_lang_token)]],\n",
    "#         device=device,\n",
    "#     )\n",
    "\n",
    "#     beams = [(decoder_input_ids, 0.0)]\n",
    "#     completed_sequences = []\n",
    "#     full_mask = torch.triu(torch.ones((max_seq_length + 1, max_seq_length + 1), device=device), diagonal=1)\n",
    "\n",
    "#     for _ in range(max_seq_length):\n",
    "#         candidate_beams = []\n",
    "\n",
    "#         for seq, score in beams:\n",
    "#             if seq.size(1) > max_seq_length:\n",
    "#                 seq = seq[:, :max_seq_length]\n",
    "\n",
    "#             if seq[0, -1].item() == eos_token_id:\n",
    "#                 completed_sequences.append((seq, score))\n",
    "#                 continue\n",
    "\n",
    "#             current_seq_length = seq.size(1)\n",
    "#             decoder_self_attention_mask = full_mask[:current_seq_length, :current_seq_length].unsqueeze(0)\n",
    "#             cross_attention_mask = encoder_attention_mask.unsqueeze(1).expand(-1, current_seq_length, -1)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 decoder_output = model(\n",
    "#                     encoder_input_ids=encoder_input_ids,\n",
    "#                     decoder_input_ids=seq,\n",
    "#                     encoder_attention_mask=encoder_attention_mask,\n",
    "#                     decoder_self_attention_mask=decoder_self_attention_mask,\n",
    "#                     cross_attention_mask=cross_attention_mask,\n",
    "#                 )\n",
    "\n",
    "#             logits = decoder_output[:, -1, :]\n",
    "#             probs = torch.softmax(logits, dim=-1)\n",
    "#             top_k_probs, top_k_indices = torch.topk(probs, beam_width, dim=-1)\n",
    "\n",
    "#             for i in range(beam_width):\n",
    "#                 next_token_id = top_k_indices[0, i].unsqueeze(0).unsqueeze(0)\n",
    "#                 next_token_prob = top_k_probs[0, i].item()\n",
    "#                 candidate_seq = torch.cat([seq, next_token_id], dim=1)\n",
    "#                 candidate_score = score + torch.log(torch.tensor(next_token_prob, device=device))\n",
    "#                 candidate_beams.append((candidate_seq, candidate_score))\n",
    "\n",
    "#         beams = sorted(candidate_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "#         if len(beams) == 0:\n",
    "#             break\n",
    "\n",
    "#     if len(completed_sequences) == 0:\n",
    "#         completed_sequences = beams\n",
    "\n",
    "#     best_sequence = sorted(completed_sequences, key=lambda x: x[1], reverse=True)[0][0]\n",
    "#     return tokenizer.decode(best_sequence[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def beam_search_decoding(\n",
    "#     model, tokenizer, source_sentence, target_language, max_seq_length, beam_width, device, eos_token_id\n",
    "# ):\n",
    "#     language_tokens = {\"hindi\": \"<_hi_>\", \"telugu\": \"<_te_>\"}\n",
    "#     target_lang_token = language_tokens[target_language]\n",
    "\n",
    "#     input_tokens = tokenizer(\n",
    "#         text=[source_sentence],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=max_seq_length,\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).to(device)\n",
    "\n",
    "#     encoder_input_ids = input_tokens[\"input_ids\"]\n",
    "#     encoder_attention_mask = input_tokens[\"attention_mask\"]\n",
    "\n",
    "#     decoder_input_ids = torch.tensor(\n",
    "#         [[tokenizer.cls_token_id, tokenizer.convert_tokens_to_ids(target_lang_token)]],\n",
    "#         device=device,\n",
    "#     )\n",
    "\n",
    "#     beams = [(decoder_input_ids, 0.0)]\n",
    "#     completed_sequences = []\n",
    "#     full_mask = torch.triu(torch.ones((max_seq_length + 1, max_seq_length + 1), device=device), diagonal=1)\n",
    "\n",
    "#     for _ in range(max_seq_length):\n",
    "#         candidate_beams = []\n",
    "\n",
    "#         for seq, score in beams:\n",
    "#             if seq.size(1) > max_seq_length:\n",
    "#                 seq = seq[:, :max_seq_length]\n",
    "\n",
    "#             if seq[0, -1].item() == eos_token_id:\n",
    "#                 completed_sequences.append((seq, score))\n",
    "#                 continue\n",
    "\n",
    "#             current_seq_length = seq.size(1)\n",
    "#             decoder_self_attention_mask = full_mask[:current_seq_length, :current_seq_length].unsqueeze(0)\n",
    "#             cross_attention_mask = encoder_attention_mask.unsqueeze(1).expand(-1, current_seq_length, -1)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 decoder_output = model(\n",
    "#                     encoder_input_ids=encoder_input_ids,\n",
    "#                     decoder_input_ids=seq,\n",
    "#                     encoder_attention_mask=encoder_attention_mask,\n",
    "#                     decoder_self_attention_mask=decoder_self_attention_mask,\n",
    "#                     cross_attention_mask=cross_attention_mask,\n",
    "#                 )\n",
    "\n",
    "#             logits = decoder_output[:, -1, :]\n",
    "#             probs = torch.softmax(logits, dim=-1)\n",
    "#             top_k_probs, top_k_indices = torch.topk(probs, beam_width, dim=-1)\n",
    "\n",
    "#             for i in range(beam_width):\n",
    "#                 next_token_id = top_k_indices[0, i].unsqueeze(0).unsqueeze(0)\n",
    "#                 next_token_prob = top_k_probs[0, i].item()\n",
    "#                 candidate_seq = torch.cat([seq, next_token_id], dim=1)\n",
    "#                 candidate_score = score + torch.log(torch.tensor(next_token_prob, device=device))\n",
    "#                 candidate_beams.append((candidate_seq, candidate_score))\n",
    "\n",
    "#         beams = sorted(candidate_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "#         if len(beams) == 0:\n",
    "#             break\n",
    "\n",
    "#     if len(completed_sequences) == 0:\n",
    "#         completed_sequences = beams\n",
    "\n",
    "#     best_sequence = sorted(completed_sequences, key=lambda x: x[1], reverse=True)[0][0]\n",
    "#     return tokenizer.decode(best_sequence[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
