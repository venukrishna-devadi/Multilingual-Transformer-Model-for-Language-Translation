{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention in Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "seq_len, d_k, d_v = 4, 8, 8\n",
    "\n",
    "q = np.random.randn(seq_len, d_k)\n",
    "k = np.random.randn(seq_len, d_k)\n",
    "v = np.random.randn(seq_len, d_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Here lets say, I am taking the sentence as \"My name is Venu\" which has 4 seq length and the dimensionality of each word I am taking is 8.\n",
    "What I am doing is - >\n",
    "My - 8 vector\n",
    "name - 8 vector\n",
    "is - 8 vector\n",
    "venu - 8 vector\n",
    "\n",
    "seq_len - It indicates the number of tokens (or words) in the sequence you're processing.\n",
    "d_k and d_v are the dimensions of the query (Q), key (K), and value (V) vectors, respectively. Specifically, d_k is the dimension of the query and key vectors, and d_v is the dimension of the value vectors.\n",
    "In the context of the Transformer’s attention mechanism:\n",
    "\n",
    "For each position in the sequence, the model creates a query, key, and value vector.\n",
    "L sets how many such vectors are created in parallel, one for each position in the sequence.\n",
    "In practical terms, if you were working with a sentence with 4 tokens, you might set L = 4. Then each word or token would have its own query, key, and value vectors with dimensions d_k and d_v. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, q)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, k)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, v)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention\n",
    "\n",
    "Attention(Q, K, V) = softmax(d_q * d_k^T / sqrt(d_k) + M) * d_v\n",
    "\n",
    "1. The + M term in the attention formula is typically a mask matrix. This matrix is used to selectively ignore certain positions in the sequence, which is particularly important in the following contexts:\n",
    "\n",
    "2. Padding Mask - In sequences of varying lengths, shorter sequences are often padded to a uniform length for batch processing. The mask M can be used to prevent attention from being calculated on these padding tokens by assigning them a large negative value (often -inf), so that their softmax probability approaches zero, effectively ignoring them in the attention calculation.\n",
    "\n",
    "3. Causal Mask (Look-Ahead Mask): In autoregressive tasks like language generation or translation, a mask is used to ensure that a token only attends to tokens that come before it in the sequence. This mask prevents a token from \"seeing the future\" by setting a large negative value for tokens that are beyond the current token’s position. \n",
    "\n",
    "In both cases, the mask M typically has:\n",
    "\n",
    "0 values at positions where attention is allowed.\n",
    "-inf values at positions to be masked, so their contribution to the softmax becomes zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 8), (8, 4))"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.95971972,  7.87945147, -3.93452496, -0.05834295],\n",
       "       [-1.21077604, -0.04276918, -1.06859527,  0.98246685],\n",
       "       [-3.80418985,  4.01712605,  0.55547391,  3.13794416],\n",
       "       [ 7.55853039,  3.9686184 ,  6.31776681, -3.10309881]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4140272561320588, 1.0252924850927356, 13.728931728356509)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why we need sqrt(d_k) in denominator\n",
    "q.var(), k.var(), np.matmul(q, k.T).var()\n",
    "# we could see that the varance of q and k are close to 1 while matmul of q and k.t vector multiplication is high when compared to only q and v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33931216,  2.78580678, -1.39106464, -0.02062735],\n",
       "       [-0.42807398, -0.01512119, -0.37780548,  0.34735449],\n",
       "       [-1.34498422,  1.42026854,  0.19638968,  1.1094308 ],\n",
       "       [ 2.67234405,  1.40311849,  2.23366788, -1.09711111]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4140272561320588, 1.0252924850927356, 1.7161164660445631)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking\n",
    "\n",
    "M term in the attention formula is typically a mask matrix. This matrix is used to selectively ignore certain positions in the sequence, which is particularly important in the following contexts:\n",
    "\n",
    "1. Padding Mask - In sequences of varying lengths, shorter sequences are often padded to a uniform length for batch processing. The mask M can be used to prevent attention from being calculated on these padding tokens by assigning them a large negative value (often -inf), so that their softmax probability approaches zero, effectively ignoring them in the attention calculation.\n",
    "\n",
    "2. Causal Mask (Look-Ahead Mask): In autoregressive tasks like language generation or translation, a mask is used to ensure that a token only attends to tokens that come before it in the sequence. This mask prevents a token from \"seeing the future\" by setting a large negative value for tokens that are beyond the current token’s position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((seq_len,seq_len)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -np.inf\n",
    "mask[mask == 1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33931216,        -inf,        -inf,        -inf],\n",
       "       [-0.42807398, -0.01512119,        -inf,        -inf],\n",
       "       [-1.34498422,  1.42026854,  0.19638968,        -inf],\n",
       "       [ 2.67234405,  1.40311849,  2.23366788, -1.09711111]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis = -1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.39820431, 0.60179569, 0.        , 0.        ],\n",
       "       [0.04639498, 0.73689393, 0.21671109, 0.        ],\n",
       "       [0.51308274, 0.14420149, 0.33088171, 0.01183406]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.38196193, -1.23322893,  1.77097453,  0.67421384, -0.2475888 ,\n",
       "          1.26165014,  0.24105605, -1.24267933],\n",
       "        [-0.81034213,  0.50163253, -0.05860093,  0.31941135, -0.25340329,\n",
       "          1.14114286, -0.71447318,  0.27828631],\n",
       "        [-0.48896668,  0.91407827, -0.79182161,  0.14412254, -0.1280293 ,\n",
       "          0.950327  , -0.90291641,  0.97749284],\n",
       "        [-0.93439501, -0.75798635,  0.83101505,  0.4281438 , -0.04282717,\n",
       "          0.98464847,  0.06616976, -0.31738712]]),\n",
       " (4, 8))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_attention = np.matmul(attention, v)\n",
    "qkv_attention, qkv_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38196193, -1.23322893,  1.77097453,  0.67421384, -0.2475888 ,\n",
       "         1.26165014,  0.24105605, -1.24267933],\n",
       "       [-0.432105  ,  1.64957912, -1.26921916,  0.08464085, -0.2572507 ,\n",
       "         1.06140397, -1.34674069,  1.28469942],\n",
       "       [-0.49113792, -1.1271732 ,  0.28283719,  0.23289567,  0.336965  ,\n",
       "         0.50597608,  0.36133287,  0.40819193],\n",
       "       [-0.04355718,  0.83245748,  0.99680307, -0.59573238,  0.8286883 ,\n",
       "         1.42328971,  1.44765476, -0.00917915]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention(Q, K, V) = softmax(d_q * d_k^T / sqrt(d_k) + M) * d_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function for the scaled dot product attention\n",
    "\n",
    "# def scaled_dot_product_attention(q, k, v, mask = None):\n",
    "#     d_k = q.shape[-1]\n",
    "#     scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "#     if mask is not None:\n",
    "#         scaled = scaled + mask\n",
    "#     attention = softmax(scaled)\n",
    "#     qkv_attention = np.matmul(attention, v)\n",
    "#     return qkv_attention, attention\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.38196193, -1.23322893,  1.77097453,  0.67421384, -0.2475888 ,\n",
       "          1.26165014,  0.24105605, -1.24267933],\n",
       "        [-0.81034213,  0.50163253, -0.05860093,  0.31941135, -0.25340329,\n",
       "          1.14114286, -0.71447318,  0.27828631],\n",
       "        [-0.48896668,  0.91407827, -0.79182161,  0.14412254, -0.1280293 ,\n",
       "          0.950327  , -0.90291641,  0.97749284],\n",
       "        [-0.93439501, -0.75798635,  0.83101505,  0.4281438 , -0.04282717,\n",
       "          0.98464847,  0.06616976, -0.31738712]]),\n",
       " array([[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.39820431, 0.60179569, 0.        , 0.        ],\n",
       "        [0.04639498, 0.73689393, 0.21671109, 0.        ],\n",
       "        [0.51308274, 0.14420149, 0.33088171, 0.01183406]]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(q, k, v, mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "\n",
    "x = torch.randn((batch_size, seq_len, input_dim))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(input_dim, 3*d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv_layer(x)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'QKV Distribution')"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_val = torch.histc(qkv, bins = 400, min = -3, max = 3)\n",
    "x_val = np.linspace(-3, 3, 400)\n",
    "plt.bar(x_val, y_val, align = 'center', color = ['orange'])\n",
    "plt.title(\"QKV Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "head_dim\n",
    "# Now we have to break down the last dimension into\n",
    "# num_of_heads x q,k,v dimension (head_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.reshape(batch_size, seq_length, num_heads, 3* head_dim)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0,2,1,3)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,k,v = qkv.chunk(3, dim= -1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self Attention for Multiple Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention(Q, K, V) = softmax((Q . KT/√d_k) + M )V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = q.size()[-1]\n",
    "d_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4098,  0.1451,  0.4211,  0.0320],\n",
       "        [ 0.0304,  0.1343, -0.4155,  0.4881],\n",
       "        [ 0.1012,  0.1963, -0.2875, -0.4380],\n",
       "        [-0.2518, -0.3250,  0.3760,  0.0851]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "scaled.shape\n",
    "scaled[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full(scaled.size(), float(\"-inf\"))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask[0][1] # mask for input to a single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1903,    -inf,    -inf,    -inf],\n",
       "        [-0.4248,  0.1303,    -inf,    -inf],\n",
       "        [-0.2464, -0.2115, -0.6710,    -inf],\n",
       "        [-0.5323, -0.3889,  0.1952, -0.4244]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled + mask)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled += mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3647, 0.6353, 0.0000, 0.0000],\n",
       "        [0.3718, 0.3850, 0.2432, 0.0000],\n",
       "        [0.1873, 0.2162, 0.3878, 0.2087]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = F.softmax(scaled, dim = -1)\n",
    "attention.shape\n",
    "attention[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4, 4]) torch.Size([1, 8, 4, 64])\n",
      "torch.Size([1, 8, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "print(attention.shape, v.shape)\n",
    "print(torch.matmul(attention, v).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scaled_dot_product(q, k, v, mask = None):\n",
    "#     d_k = k.size()[-1]\n",
    "#     scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "#     if mask is not None:\n",
    "#         scaled += mask\n",
    "#     attention = F.softmax(scaled, dim = -1)\n",
    "#     new_values = torch.matmul(attention, v)\n",
    "#     return attention, new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 4]), torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Encoder\n",
    "attention, new_values = scaled_dot_product(q, k, v, mask= mask)\n",
    "attention.shape, new_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6135, 0.3865, 0.0000, 0.0000],\n",
       "        [0.2830, 0.4525, 0.2645, 0.0000],\n",
       "        [0.3335, 0.1443, 0.1772, 0.3450]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 4]), torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Decoder\n",
    "d_attention, d_new_values = scaled_dot_product(q, k, v, mask= None)\n",
    "d_attention.shape, d_new_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1357, 0.3607, 0.1863, 0.3172],\n",
       "        [0.2963, 0.1866, 0.2099, 0.3072],\n",
       "        [0.1978, 0.3162, 0.1848, 0.3012],\n",
       "        [0.3335, 0.1443, 0.1772, 0.3450]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will combine all of the heads together\n",
    "new_values = new_values.reshape(batch_size, seq_length, num_heads * head_dim)\n",
    "new_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will pass it through FF layer so that they can communicate with each other\n",
    "ffn_layer = nn.Linear(d_model, d_model)\n",
    "out_ffn_layer = ffn_layer(new_values)\n",
    "out_ffn_layer.shape # the output vector will be much more context aware now than the input as we have passed it through a ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        print(f\"x after first linear layer: {x.size()}\")\n",
    "        x = self.relu(x)\n",
    "        print(f\"x after activation: {x.size()}\")\n",
    "        x = self.dropout(x)\n",
    "        print(f\"x after dropout: {x.size()}\")\n",
    "        x = self.linear2(x)\n",
    "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_dim, d_model, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.d_model = d_model\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = d_model // num_heads\n",
    "#         self.qkv_layer = nn.Linear(input_dim, 3 * d_model)\n",
    "#         self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "#     def forward(self, x, mask = None):\n",
    "#         print(f\"x.shape in attention layer = {x.shape}\")\n",
    "#         batch_size, seq_length, input_dim = x.size()\n",
    "#         qkv = self.qkv_layer(x)\n",
    "#         print(f\"qkv torch matrix of x is - {qkv.size()}\")\n",
    "#         qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
    "#         print(f\"qkv torch matrix reshaped shape is : {qkv.size()}\")\n",
    "#         qkv = qkv.permute(0,2,1,3)\n",
    "#         print(f\"Permutted qkv shape is - {qkv.size()}\")\n",
    "#         q,k,v = qkv.chunk(3, dim = -1)\n",
    "#         print(\"The shape of q,k,v matrix is below - \")\n",
    "#         print(f\"q - {q.size()}\")\n",
    "#         print(f\"k - {k.size()}\")\n",
    "#         print(f\"v - {v.size()}\")\n",
    "#         attention, new_values = scaled_dot_product(q, k, v, mask)\n",
    "#         print(f\"Attention Size - {attention.size()}, New Values Size is - {new_values.size()}\")\n",
    "#         new_values = new_values.reshape(batch_size, seq_length, self.num_heads * self.head_dim)\n",
    "#         print(f\"Reshaped New Values shape - {new_values.size()}\")\n",
    "#         out = self.linear_layer(new_values)\n",
    "#         print(f\"Output from the FF Layer is  - {out.shape}\")\n",
    "#         return out\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 1024])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 1024\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 32\n",
    "seq_length = 20\n",
    "x = torch.randn(batch_size, seq_length, input_dim)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape in attention layer = torch.Size([32, 20, 1024])\n",
      "qkv torch matrix of x is - torch.Size([32, 20, 3072])\n",
      "qkv torch matrix reshaped shape is : torch.Size([32, 20, 8, 384])\n",
      "Permutted qkv shape is - torch.Size([32, 8, 20, 384])\n",
      "The shape of q,k,v matrix is below - \n",
      "q - torch.Size([32, 8, 20, 128])\n",
      "k - torch.Size([32, 8, 20, 128])\n",
      "v - torch.Size([32, 8, 20, 128])\n",
      "Attention Size - torch.Size([32, 8, 20, 20]), New Values Size is - torch.Size([32, 8, 20, 128])\n",
      "Reshaped New Values shape - torch.Size([32, 20, 1024])\n",
      "Output from the FF Layer is  - torch.Size([32, 20, 1024])\n"
     ]
    }
   ],
   "source": [
    "model = MultiHeadAttention(input_dim= input_dim, num_heads= num_heads, d_model= d_model)\n",
    "ouput_multi_head_attention = model.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 10\n",
    "d_model = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Encoding Formula\n",
    "\n",
    "#### Even Positions (sin)\n",
    "\n",
    "### PE(pos, 2i) = sin(pos / 10000^(2i/d))\n",
    "\n",
    "#### Odd Positions (cos)\n",
    "\n",
    "### PE(pos, 2i+1) = cos(pos / 10000^(2i/d))\n",
    "\n",
    "#### Variables\n",
    "\n",
    "* PE: Position encoding\n",
    "* pos: Position in sequence\n",
    "* i: Dimension index\n",
    "* d: Embedding dimension\n",
    "\n",
    "\n",
    "#### We can also write the position encoding formula as\n",
    "\n",
    "#### Even Positions (sin)\n",
    "\n",
    "### PE(pos, i) = sin(pos / 10000^(i/d_model))\n",
    "\n",
    "#### Odd Positions (cos)\n",
    "\n",
    "### PE(pos, i) = cos(pos / 10000^(i - 1/d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6., 8.])"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_i = torch.arange(0, d_model, 2).float()\n",
    "even_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 5., 7., 9.])"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_i = even_i + 1\n",
    "odd_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 6.3096e+00, 3.9811e+01, 2.5119e+02, 1.5849e+03])"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_denominator = torch.pow(10000, even_i / d_model)\n",
    "even_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 6.3096e+00, 3.9811e+01, 2.5119e+02, 1.5849e+03])"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_denominator = torch.pow(10000, (odd_i - 1) / d_model)\n",
    "odd_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "## even_denominator and odd_denominator are the same! So we can just do one of these actions and call the resulting variable denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = even_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(max_seq_length, dtype = torch.float).reshape(max_seq_length, 1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_position_emb = torch.sin(position / denominator)\n",
    "odd_position_emb = torch.cos(position / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 8.4147e-01,  1.5783e-01,  2.5116e-02,  3.9811e-03,  6.3096e-04],\n",
       "         [ 9.0930e-01,  3.1170e-01,  5.0217e-02,  7.9621e-03,  1.2619e-03],\n",
       "         [ 1.4112e-01,  4.5775e-01,  7.5285e-02,  1.1943e-02,  1.8929e-03],\n",
       "         [-7.5680e-01,  5.9234e-01,  1.0031e-01,  1.5924e-02,  2.5238e-03],\n",
       "         [-9.5892e-01,  7.1207e-01,  1.2526e-01,  1.9904e-02,  3.1548e-03],\n",
       "         [-2.7942e-01,  8.1396e-01,  1.5014e-01,  2.3884e-02,  3.7857e-03],\n",
       "         [ 6.5699e-01,  8.9544e-01,  1.7493e-01,  2.7864e-02,  4.4167e-03],\n",
       "         [ 9.8936e-01,  9.5448e-01,  1.9960e-01,  3.1843e-02,  5.0476e-03],\n",
       "         [ 4.1212e-01,  9.8959e-01,  2.2415e-01,  3.5822e-02,  5.6786e-03]]),\n",
       " torch.Size([10, 5]))"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_position_emb, even_position_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [ 0.5403,  0.9875,  0.9997,  1.0000,  1.0000],\n",
       "         [-0.4161,  0.9502,  0.9987,  1.0000,  1.0000],\n",
       "         [-0.9900,  0.8891,  0.9972,  0.9999,  1.0000],\n",
       "         [-0.6536,  0.8057,  0.9950,  0.9999,  1.0000],\n",
       "         [ 0.2837,  0.7021,  0.9921,  0.9998,  1.0000],\n",
       "         [ 0.9602,  0.5809,  0.9887,  0.9997,  1.0000],\n",
       "         [ 0.7539,  0.4452,  0.9846,  0.9996,  1.0000],\n",
       "         [-0.1455,  0.2983,  0.9799,  0.9995,  1.0000],\n",
       "         [-0.9111,  0.1439,  0.9746,  0.9994,  1.0000]]),\n",
       " torch.Size([10, 5]))"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_position_emb, odd_position_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  1.5783e-01,  9.8747e-01,  2.5116e-02,\n",
       "           9.9968e-01,  3.9811e-03,  9.9999e-01,  6.3096e-04,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  3.1170e-01,  9.5018e-01,  5.0217e-02,\n",
       "           9.9874e-01,  7.9621e-03,  9.9997e-01,  1.2619e-03,  1.0000e+00],\n",
       "         [ 1.4112e-01, -9.8999e-01,  4.5775e-01,  8.8908e-01,  7.5285e-02,\n",
       "           9.9716e-01,  1.1943e-02,  9.9993e-01,  1.8929e-03,  1.0000e+00],\n",
       "         [-7.5680e-01, -6.5364e-01,  5.9234e-01,  8.0569e-01,  1.0031e-01,\n",
       "           9.9496e-01,  1.5924e-02,  9.9987e-01,  2.5238e-03,  1.0000e+00],\n",
       "         [-9.5892e-01,  2.8366e-01,  7.1207e-01,  7.0211e-01,  1.2526e-01,\n",
       "           9.9212e-01,  1.9904e-02,  9.9980e-01,  3.1548e-03,  1.0000e+00],\n",
       "         [-2.7942e-01,  9.6017e-01,  8.1396e-01,  5.8092e-01,  1.5014e-01,\n",
       "           9.8866e-01,  2.3884e-02,  9.9971e-01,  3.7857e-03,  9.9999e-01],\n",
       "         [ 6.5699e-01,  7.5390e-01,  8.9544e-01,  4.4518e-01,  1.7493e-01,\n",
       "           9.8458e-01,  2.7864e-02,  9.9961e-01,  4.4167e-03,  9.9999e-01],\n",
       "         [ 9.8936e-01, -1.4550e-01,  9.5448e-01,  2.9827e-01,  1.9960e-01,\n",
       "           9.7988e-01,  3.1843e-02,  9.9949e-01,  5.0476e-03,  9.9999e-01],\n",
       "         [ 4.1212e-01, -9.1113e-01,  9.8959e-01,  1.4389e-01,  2.2415e-01,\n",
       "           9.7455e-01,  3.5822e-02,  9.9936e-01,  5.6786e-03,  9.9998e-01]]),\n",
       " torch.Size([10, 10]))"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked = torch.stack([even_position_emb, odd_position_emb], dim= 2)\n",
    "\n",
    "PE = torch.flatten(stacked, start_dim= 1, end_dim= 2)\n",
    "PE, PE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Compute the position encodings\n",
    "        even_i = torch.arange(0, d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i / d_model)\n",
    "        position = torch.arange(max_seq_length).reshape(max_seq_length, 1)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        self.position_encoding = torch.flatten(stacked, start_dim=1, end_dim=2)  # Shape: [max_seq_length, d_model]\n",
    "        #print(f\"[DEBUG] Initial position encoding shape: {self.position_encoding.shape}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the batch size and sequence length\n",
    "        batch_size, seq_len, d_model = x.size(0), x.size(1), x.size(2)\n",
    "        #print(f\"[DEBUG] Input tensor shape: batch_size={batch_size}, seq_len={seq_len}, d_model={d_model}\")\n",
    "\n",
    "        # Check the dimensions of position_encoding\n",
    "        position_encoding = self.position_encoding[:seq_len, :]  # Shape: [seq_len, d_model]\n",
    "        #print(f\"[DEBUG] Sliced position encoding shape: {position_encoding.shape}\")\n",
    "\n",
    "        # Ensure that the position encoding is broadcastable across the batch dimension\n",
    "        position_encoding = position_encoding.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: [batch_size, seq_len, d_model]\n",
    "        #print(f\"[DEBUG] Broadcasted position encoding shape: {position_encoding.shape}\")\n",
    "        \n",
    "        # Perform the addition and verify the output shape\n",
    "        output = x + position_encoding\n",
    "        #print(f\"[DEBUG] Output tensor shape after addition: {output.shape}\")\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 34, 512]),\n",
       " torch.Size([32, 34, 512]),\n",
       " torch.Size([32, 34, 512]))"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seq_len = 34\n",
    "d_model = 512\n",
    "max_seq_length = 200\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "position_encoding = torch.randn(max_seq_length, d_model)\n",
    "\n",
    "position_encoding_sliced = position_encoding[:seq_len, :]\n",
    "\n",
    "position_encoding_sliced_expanded = position_encoding_sliced.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "output = x + position_encoding_sliced_expanded\n",
    "\n",
    "x.shape, output.shape, position_encoding_sliced_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 34, 512]), torch.Size([32, 34, 512]))"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoder = PositionEncoding(d_model=d_model, max_seq_length=max_seq_length)\n",
    "output = position_encoder(x)\n",
    "x.shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "         [[0.5000, 0.6000, 0.7000]]]),\n",
       " torch.Size([2, 1, 3]))"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[[0.1,0.2,0.3], [0.5,0.6,0.7]]])\n",
    "print(inputs.size())\n",
    "B, S, E = inputs.size()\n",
    "inputs = inputs.reshape(S, B, E)\n",
    "inputs, inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[1., 1., 1.]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0., 0., 0.]], requires_grad=True))"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_shape = inputs.size()[-2:]\n",
    "print(parameter_shape)\n",
    "gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "beta = nn.Parameter(torch.zeros(parameter_shape))\n",
    "print(gamma.shape)\n",
    "print(beta.shape)\n",
    "gamma, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -2]"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = [-(i+1) for i in range(len(parameter_shape))]\n",
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2000]],\n",
      "\n",
      "        [[0.6000]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1])"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = inputs.mean(dim = dims, keepdim= True)\n",
    "print(mean)\n",
    "mean.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0817]],\n",
       "\n",
       "        [[0.0817]]])"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = ((inputs - mean) **2).mean(dim = dims, keepdim=True)\n",
    "epsilon = 1e-5\n",
    "std = (var+epsilon).sqrt()\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2238e+00,  0.0000e+00,  1.2238e+00]],\n",
       "\n",
       "        [[-1.2238e+00,  7.2946e-07,  1.2238e+00]]])"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (inputs - mean) / std\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2238e+00,  0.0000e+00,  1.2238e+00]],\n",
       "\n",
       "        [[-1.2238e+00,  7.2946e-07,  1.2238e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gamma * y + beta\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameter_shape, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameter_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameter_shape))\n",
    "\n",
    "    def forward(self, input):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim = dims, keepdim= True)\n",
    "        print(f\"Mean of inputs is - {mean.size()}\\n Mean value is - {mean}\")\n",
    "        var = ((inputs - mean) ** 2).mean(dim = dims, keepdim= True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        print(f\"Standard Deviation is \\n {std.size()}\\n std value is - {std}\")\n",
    "        y = (input - mean) / std\n",
    "        print(f\"y is {y}\\n The shape of y is {y.size()}\")\n",
    "        out = self.gamma * y + self.beta\n",
    "        print(f\"The output is {out.size()}\\n The shape of output is {out.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 15])"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "seq_length = 5\n",
    "embedding_dim = 15\n",
    "inputs = torch.randn(seq_length, batch_size, embedding_dim)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNormalization()"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm = LayerNormalization(inputs.size()[-1:])\n",
    "layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of inputs is - torch.Size([5, 10, 1])\n",
      " Mean value is - tensor([[[ 0.3235],\n",
      "         [ 0.1398],\n",
      "         [-0.3515],\n",
      "         [ 0.1191],\n",
      "         [ 0.1709],\n",
      "         [-0.0468],\n",
      "         [ 0.0245],\n",
      "         [ 0.4237],\n",
      "         [-0.1499],\n",
      "         [-0.1170]],\n",
      "\n",
      "        [[ 0.1430],\n",
      "         [ 0.2185],\n",
      "         [ 0.0465],\n",
      "         [ 0.2267],\n",
      "         [-0.0661],\n",
      "         [-0.1528],\n",
      "         [-0.0760],\n",
      "         [ 0.3744],\n",
      "         [-0.3958],\n",
      "         [ 0.0744]],\n",
      "\n",
      "        [[-0.1501],\n",
      "         [-0.2867],\n",
      "         [ 0.4361],\n",
      "         [-0.0888],\n",
      "         [ 0.0412],\n",
      "         [-0.1105],\n",
      "         [-0.4728],\n",
      "         [ 0.0483],\n",
      "         [ 0.4540],\n",
      "         [ 0.1632]],\n",
      "\n",
      "        [[ 0.3820],\n",
      "         [ 0.0926],\n",
      "         [ 0.1274],\n",
      "         [ 0.3634],\n",
      "         [ 0.0671],\n",
      "         [-0.1880],\n",
      "         [ 0.0761],\n",
      "         [-0.2325],\n",
      "         [-0.0337],\n",
      "         [-0.2985]],\n",
      "\n",
      "        [[-0.0218],\n",
      "         [-0.0569],\n",
      "         [ 0.5496],\n",
      "         [ 0.2421],\n",
      "         [ 0.1891],\n",
      "         [-0.1433],\n",
      "         [ 0.0137],\n",
      "         [ 0.0949],\n",
      "         [ 0.3504],\n",
      "         [-0.0419]]])\n",
      "Standard Deviation is \n",
      " torch.Size([5, 10, 1])\n",
      " std value is - tensor([[[0.9869],\n",
      "         [1.1172],\n",
      "         [0.7316],\n",
      "         [1.1242],\n",
      "         [0.9685],\n",
      "         [0.9898],\n",
      "         [1.1514],\n",
      "         [0.7256],\n",
      "         [0.8775],\n",
      "         [0.8908]],\n",
      "\n",
      "        [[0.8063],\n",
      "         [0.9794],\n",
      "         [0.9899],\n",
      "         [0.8723],\n",
      "         [1.3039],\n",
      "         [1.0527],\n",
      "         [1.1376],\n",
      "         [1.2532],\n",
      "         [0.6232],\n",
      "         [0.8317]],\n",
      "\n",
      "        [[1.0455],\n",
      "         [1.1765],\n",
      "         [1.0008],\n",
      "         [0.9846],\n",
      "         [0.9372],\n",
      "         [0.7333],\n",
      "         [1.1309],\n",
      "         [0.8693],\n",
      "         [1.0109],\n",
      "         [1.1779]],\n",
      "\n",
      "        [[0.9252],\n",
      "         [0.9096],\n",
      "         [1.0035],\n",
      "         [0.8530],\n",
      "         [1.0373],\n",
      "         [1.0732],\n",
      "         [0.8091],\n",
      "         [1.0563],\n",
      "         [1.0705],\n",
      "         [0.5368]],\n",
      "\n",
      "        [[0.6945],\n",
      "         [0.9004],\n",
      "         [0.9266],\n",
      "         [1.1748],\n",
      "         [1.1654],\n",
      "         [0.9168],\n",
      "         [0.6649],\n",
      "         [1.0233],\n",
      "         [0.8821],\n",
      "         [0.6110]]])\n",
      "y is tensor([[[ 6.8989e-01,  8.9608e-01, -2.2659e-01, -1.5098e+00,  2.5801e-01,\n",
      "           7.5447e-01, -9.6391e-01, -1.9809e-01, -1.0312e-02, -8.6728e-01,\n",
      "           7.9558e-01,  1.6045e+00,  2.4101e-01,  8.0070e-01, -2.2643e+00],\n",
      "         [ 1.6737e+00,  8.5605e-01, -6.1714e-01, -6.8399e-01,  2.3503e-01,\n",
      "          -8.2906e-02,  1.9343e-01,  2.0724e+00, -1.5253e-01,  2.4418e-01,\n",
      "           3.5781e-02, -2.0001e+00, -9.5826e-01, -1.0739e+00,  2.5824e-01],\n",
      "         [ 8.8639e-01,  2.6462e-01,  6.7941e-01, -2.4322e-01, -7.6158e-01,\n",
      "          -1.5636e-01, -1.1550e+00,  4.0393e-01, -8.3113e-02,  1.3849e+00,\n",
      "          -2.6512e+00,  5.6665e-01,  1.0782e+00, -8.3535e-01,  6.2166e-01],\n",
      "         [-2.0124e+00,  1.3013e+00,  1.4718e-01,  2.9099e-02,  1.4327e+00,\n",
      "          -1.7187e-01,  1.3818e+00, -1.8022e-01, -1.7829e-01,  3.7003e-01,\n",
      "           3.4407e-02, -1.2146e+00, -1.3569e+00,  1.1134e+00, -6.9573e-01],\n",
      "         [ 4.8682e-01,  1.6615e-01, -2.3035e-01,  6.4152e-01,  3.2158e-01,\n",
      "          -1.7520e+00, -1.1184e+00, -1.1194e+00,  1.7447e+00,  1.1047e+00,\n",
      "          -1.5386e+00,  3.6633e-01,  4.8684e-01,  1.0844e+00, -6.4435e-01],\n",
      "         [-1.1632e+00, -1.2645e+00,  2.9345e-01, -1.4538e+00,  1.8552e+00,\n",
      "          -1.6389e+00,  3.7923e-01,  3.4333e-01,  7.5135e-01, -2.0252e-01,\n",
      "          -1.6857e-01, -8.2030e-02, -8.6055e-04,  1.3388e+00,  1.0129e+00],\n",
      "         [-1.8308e+00,  4.4931e-01, -4.6669e-01, -1.2506e+00,  1.7504e+00,\n",
      "          -1.6342e-02, -3.0228e-01, -2.8724e-01,  9.8451e-01, -1.2867e+00,\n",
      "           4.2881e-01,  1.5256e+00, -2.5515e-01,  1.0100e+00, -4.5287e-01],\n",
      "         [ 1.5825e+00, -1.0932e+00,  5.3041e-02,  8.0235e-01,  2.2530e-01,\n",
      "           1.3964e+00,  2.8139e-01,  4.4567e-01, -2.4958e+00, -6.4593e-01,\n",
      "           4.8179e-01,  6.3210e-01, -5.3738e-01, -2.7097e-01, -8.5730e-01],\n",
      "         [-2.0885e+00,  6.4510e-01, -1.3304e+00, -5.1069e-01,  4.0531e-01,\n",
      "           8.7259e-01,  4.0747e-01,  8.0267e-02, -3.4803e-01,  1.0126e+00,\n",
      "           6.4103e-01, -8.1829e-01,  1.7288e+00,  5.5520e-01, -1.2525e+00],\n",
      "         [ 7.5964e-01, -1.2402e+00,  6.2928e-01, -6.6669e-01,  1.1729e+00,\n",
      "          -1.4777e+00,  1.2362e-01, -2.5958e-01, -2.5092e-01, -1.0310e+00,\n",
      "           1.2998e-01,  2.3861e+00,  7.3998e-01, -1.0092e+00, -6.2809e-03]],\n",
      "\n",
      "        [[ 1.6099e+00, -1.3196e+00,  9.0183e-01, -5.4906e-01, -1.1873e+00,\n",
      "           1.7700e+00, -1.3536e+00, -3.5818e-01,  1.3752e+00,  4.9217e-01,\n",
      "           1.1820e-01,  8.7723e-02, -6.7010e-01, -5.6797e-01, -3.4933e-01],\n",
      "         [ 1.5724e+00,  1.2774e-01, -5.8883e-01,  1.7964e+00, -1.3160e+00,\n",
      "           8.4946e-02, -1.3903e+00, -6.1454e-01, -8.0479e-01, -4.6502e-01,\n",
      "           3.0840e-01,  1.3147e+00,  1.0817e+00, -9.6472e-02, -1.0103e+00],\n",
      "         [-7.7030e-01, -1.0678e+00, -1.5108e+00, -6.6044e-01,  6.7247e-01,\n",
      "           4.7688e-01, -2.6115e-01,  3.7709e-01,  1.0937e+00,  6.9286e-01,\n",
      "          -1.0924e+00,  2.9430e-01,  1.3450e+00, -1.3452e+00,  1.7558e+00],\n",
      "         [ 1.0318e+00, -1.4929e-01, -7.4704e-01, -6.3851e-01, -8.6637e-01,\n",
      "          -5.9447e-01, -1.2953e+00,  2.2148e+00, -4.4692e-02,  7.8711e-01,\n",
      "           1.3710e-01,  9.8819e-01, -1.7806e+00,  3.9376e-01,  5.6350e-01],\n",
      "         [ 1.1723e+00, -2.0048e+00,  2.9206e-01,  1.4986e-01, -1.0899e+00,\n",
      "           1.4161e-01, -6.3365e-01, -8.6440e-01,  7.1722e-01, -3.2657e-01,\n",
      "           3.8652e-01,  6.0074e-03, -5.1716e-01,  1.0236e-01,  2.4685e+00],\n",
      "         [-6.3352e-01, -1.0723e+00, -7.3685e-01, -7.4444e-01,  1.1770e+00,\n",
      "           1.3382e+00, -8.7473e-01,  1.1598e-01,  1.3288e+00, -5.0767e-01,\n",
      "          -1.1708e-03, -9.3587e-01, -5.5614e-01, -1.6875e-01,  2.2715e+00],\n",
      "         [ 7.7167e-01, -3.0090e-01,  7.7233e-01,  7.9589e-01, -1.5252e+00,\n",
      "          -1.0366e+00, -1.6129e+00, -2.6633e-02,  1.2717e+00,  1.7220e-01,\n",
      "          -1.5296e+00, -3.4707e-01,  3.6116e-01,  6.9973e-01,  1.5342e+00],\n",
      "         [ 2.3751e-01,  1.7189e+00,  6.7002e-01, -2.0101e-01,  5.6321e-01,\n",
      "           8.5130e-01,  1.0093e+00, -6.5589e-01, -6.1965e-01, -1.1357e+00,\n",
      "          -9.8591e-02,  7.2490e-01, -2.1095e+00,  4.9572e-01, -1.4506e+00],\n",
      "         [ 1.1879e+00,  2.9974e-01, -2.0801e+00, -6.4275e-01,  1.3806e+00,\n",
      "          -1.0118e+00,  2.6837e-01, -8.3751e-01,  4.2300e-01, -9.4100e-01,\n",
      "          -9.9261e-01,  8.6919e-01,  1.1000e+00, -4.2165e-02,  1.0191e+00],\n",
      "         [ 7.2230e-01, -3.5891e-01, -4.3055e-01, -6.7718e-01,  2.3815e-01,\n",
      "          -5.1551e-01, -1.1295e+00,  4.1009e-01, -5.8659e-01,  1.3740e+00,\n",
      "           2.3365e+00, -1.1355e+00,  7.0424e-01,  5.2670e-01, -1.4782e+00]],\n",
      "\n",
      "        [[-8.2879e-01, -4.1978e-01, -1.7502e+00, -9.8862e-01,  5.9206e-01,\n",
      "          -8.5350e-01, -4.4996e-01, -3.5928e-01,  4.1537e-01,  8.0961e-01,\n",
      "           3.7221e-02,  4.8592e-03,  2.5015e+00,  1.2641e+00,  2.5415e-02],\n",
      "         [-1.7214e+00, -1.5964e+00, -1.3847e+00,  1.0164e+00,  1.1102e+00,\n",
      "          -2.6718e-01,  1.0096e+00,  1.0590e+00,  2.6024e-01,  1.9581e-01,\n",
      "           1.0398e+00, -3.9035e-01, -9.5887e-01, -2.5029e-01,  8.7810e-01],\n",
      "         [ 9.1166e-02, -1.0236e+00,  1.5939e+00,  8.8498e-03, -1.1737e-01,\n",
      "          -6.2594e-01,  1.1290e-01,  6.5311e-01,  2.5257e+00, -1.0905e+00,\n",
      "          -8.1593e-01,  6.8539e-01, -1.2088e+00, -5.9668e-01, -1.9218e-01],\n",
      "         [-6.1989e-01, -5.8948e-01, -5.1278e-01, -1.2556e-01,  1.8492e+00,\n",
      "          -1.8470e-02,  1.2484e+00, -8.0773e-01, -2.4748e-01,  7.9088e-01,\n",
      "          -1.0729e+00, -1.3336e+00,  1.9564e-01,  2.0237e+00, -7.8003e-01],\n",
      "         [ 4.7271e-01, -1.6836e+00, -1.5620e+00,  1.0032e+00, -1.4287e-01,\n",
      "          -8.6442e-01,  5.9934e-02, -2.8872e-01,  8.1622e-01, -5.7585e-01,\n",
      "           1.4279e+00, -8.3688e-01, -3.8872e-01,  1.7655e+00,  7.9759e-01],\n",
      "         [ 6.3010e-01,  1.1062e+00, -1.6546e+00,  4.4845e-01,  1.5213e+00,\n",
      "          -2.0019e+00, -1.0156e-01, -3.3516e-01, -1.1326e-01,  8.8016e-01,\n",
      "           6.5102e-01, -1.2825e+00,  9.4728e-01, -3.4336e-01, -3.5221e-01],\n",
      "         [ 6.7725e-02,  7.7836e-01, -3.1211e-01,  2.6827e-01, -6.0995e-01,\n",
      "           2.0170e+00,  1.0398e+00, -1.1920e+00, -6.1150e-01,  5.3479e-01,\n",
      "           1.8092e-01, -1.5454e+00, -1.6602e+00,  1.1911e+00, -1.4679e-01],\n",
      "         [ 1.5135e-01,  9.8916e-01, -7.2525e-01,  2.8293e-02, -5.7050e-01,\n",
      "          -1.4655e+00,  1.0860e+00, -3.0140e-01,  1.1885e-01,  6.3695e-01,\n",
      "          -1.1178e-02, -1.0022e+00,  8.1995e-01, -1.8268e+00,  2.0723e+00],\n",
      "         [-9.6274e-01, -5.0171e-01,  2.6288e-02, -6.2022e-01, -3.9053e-02,\n",
      "          -4.7176e-01, -1.1277e-01, -7.9767e-01, -9.1401e-01, -1.0509e+00,\n",
      "           7.0775e-02,  1.1278e+00,  1.9041e+00,  2.3912e+00, -4.9343e-02],\n",
      "         [ 3.1245e-01, -8.9621e-01,  1.1906e+00, -2.1631e-01, -7.9198e-01,\n",
      "           8.5212e-01, -7.4681e-01,  5.0993e-01,  1.1751e+00, -1.3622e+00,\n",
      "          -1.0516e+00,  6.2660e-01, -5.9476e-01, -1.0658e+00,  2.0589e+00]],\n",
      "\n",
      "        [[-4.5056e-01,  1.3426e+00,  1.9418e+00,  2.5096e-01, -9.6067e-01,\n",
      "          -3.9033e-02, -6.7936e-01, -8.0035e-01, -1.3678e+00,  8.3715e-01,\n",
      "           1.5972e+00,  2.5844e-01, -2.3824e-01, -1.3237e+00, -3.6836e-01],\n",
      "         [-2.5315e-01,  1.0351e-01,  8.7742e-02, -6.4315e-01,  4.3683e-01,\n",
      "          -1.4458e+00, -1.4925e+00,  1.3602e+00,  5.8425e-01,  1.7096e+00,\n",
      "          -1.1083e-01,  1.6231e+00,  1.0522e-01, -1.2454e+00, -8.1966e-01],\n",
      "         [ 1.1899e+00, -1.1259e+00, -2.4220e-02, -1.2830e+00, -5.0394e-01,\n",
      "          -1.8853e+00, -3.1460e-01,  2.1582e+00,  3.0370e-01,  2.8356e-01,\n",
      "          -6.5418e-01,  7.6668e-02,  9.5896e-01,  7.5819e-01,  6.2055e-02],\n",
      "         [-1.8103e+00,  1.5456e+00,  8.8432e-01, -7.3463e-01,  7.0134e-02,\n",
      "          -7.7857e-01,  3.0924e-01,  1.8910e-01,  8.7716e-01, -1.0722e+00,\n",
      "           1.8415e+00, -1.1958e+00,  2.5439e-01, -6.3315e-01,  2.5323e-01],\n",
      "         [ 2.6552e-02, -3.3652e-01, -4.8099e-02, -1.9969e+00,  5.3798e-01,\n",
      "           3.3792e-01, -6.1100e-01,  7.1687e-01,  2.3124e+00, -2.3645e-01,\n",
      "          -1.6207e-01,  1.1879e+00,  2.0389e-01, -3.1211e-01, -1.6203e+00],\n",
      "         [-1.3388e+00,  1.9320e-01, -2.3323e-01,  1.3659e+00, -5.5287e-01,\n",
      "           1.7275e-01, -1.8302e+00,  1.1927e+00, -1.3455e+00,  4.2671e-01,\n",
      "           1.4319e+00,  1.0546e+00,  3.3015e-01,  6.7396e-02, -9.3469e-01],\n",
      "         [-1.5000e-01,  5.6330e-01, -7.0736e-01, -1.8709e+00,  8.4930e-01,\n",
      "           2.7041e-01, -6.0841e-01,  6.1471e-01,  1.2675e+00,  8.0814e-02,\n",
      "          -2.3361e+00,  1.2528e+00,  6.7774e-01, -3.2576e-02,  1.2866e-01],\n",
      "         [-5.3155e-01,  8.3739e-01, -4.5941e-01, -1.6149e+00, -3.5886e-01,\n",
      "          -8.9265e-01, -3.5608e-01, -1.2615e-01,  2.9181e+00,  8.3424e-02,\n",
      "           2.7429e-01, -9.4278e-01,  6.8312e-01,  7.3058e-02,  4.1302e-01],\n",
      "         [-2.3943e+00, -1.7314e-02,  2.1023e-01, -3.8311e-01, -4.0535e-01,\n",
      "           1.8475e+00,  1.3109e+00, -1.1716e+00,  7.4744e-01, -4.4429e-01,\n",
      "           5.6225e-01, -7.2451e-01, -7.3082e-02,  8.9635e-01,  3.8909e-02],\n",
      "         [ 1.4664e+00, -2.2863e-01,  8.9765e-02, -3.4158e-01, -3.6215e-01,\n",
      "           1.0689e-01, -1.8556e+00,  1.6450e-01, -1.9576e+00,  1.6165e-01,\n",
      "          -5.8235e-02,  5.7666e-01,  1.8502e+00, -6.3274e-01,  1.0205e+00]],\n",
      "\n",
      "        [[-2.0228e-01,  3.9049e-01,  1.4972e-01,  4.5300e-01,  9.2967e-01,\n",
      "           1.0895e+00, -1.6179e+00,  1.5881e+00, -8.9302e-02,  7.0882e-01,\n",
      "          -5.2394e-01, -1.1950e+00,  6.7078e-01, -1.8592e-01, -2.1657e+00],\n",
      "         [ 2.6141e-01, -1.1225e-01, -1.1073e+00, -2.2527e-01, -1.5916e+00,\n",
      "           8.3759e-01,  5.6290e-01, -1.3881e+00, -1.7244e+00,  1.7340e+00,\n",
      "           9.5704e-01,  5.3277e-01,  5.6086e-01, -1.0818e-01,  8.1053e-01],\n",
      "         [-1.1195e+00, -7.9752e-01, -4.5300e-01, -2.1755e-01,  1.7034e+00,\n",
      "           8.4380e-03, -1.2483e+00,  9.3658e-01,  2.0626e+00,  5.9406e-01,\n",
      "           1.4740e-01, -3.6141e-02, -1.0579e-01, -1.6839e+00,  2.0926e-01],\n",
      "         [-1.4128e-01,  5.3148e-01, -8.7178e-01, -3.1751e-01, -5.7644e-01,\n",
      "           1.9451e+00, -1.1208e+00, -8.6645e-01,  1.0942e+00, -1.0330e+00,\n",
      "          -2.6818e-01, -1.3173e+00,  3.9137e-01,  9.3109e-01,  1.6195e+00],\n",
      "         [ 6.7763e-01, -1.8743e+00, -1.0729e+00, -2.0162e-01, -7.4362e-01,\n",
      "          -3.9461e-01,  1.3024e+00, -9.1611e-01, -8.7710e-01,  9.9489e-01,\n",
      "          -2.3414e-01,  6.2051e-01,  2.0219e-01,  2.0281e+00,  4.8870e-01],\n",
      "         [-3.9358e-01, -2.0009e-01,  5.3912e-01,  3.7163e-01, -1.4580e+00,\n",
      "          -1.0770e+00, -6.4308e-01, -1.4925e+00,  1.9760e+00,  1.6169e+00,\n",
      "          -5.0485e-01, -2.5295e-01,  1.2557e+00,  1.2585e-01,  1.3693e-01],\n",
      "         [-5.4825e-01, -5.7517e-01, -6.7162e-01, -5.4122e-01,  9.8340e-01,\n",
      "           1.1582e+00,  1.5778e-01, -1.9492e+00,  2.1230e+00, -7.6120e-01,\n",
      "          -2.2660e-01, -9.2726e-01,  9.1575e-01,  8.0759e-01,  5.4769e-02],\n",
      "         [-8.1678e-01, -9.8632e-01,  2.2369e-01, -1.1403e+00,  1.1473e+00,\n",
      "          -1.2197e+00,  5.4691e-01, -6.1646e-01,  6.6726e-01,  5.4284e-01,\n",
      "           1.3556e+00, -4.1011e-01, -6.0171e-01, -8.5669e-01,  2.1644e+00],\n",
      "         [ 9.3747e-01,  5.6735e-01,  1.1684e+00,  1.3750e+00,  9.7748e-01,\n",
      "          -1.5567e+00, -1.5593e+00,  2.6032e-02, -1.4013e+00, -3.5376e-01,\n",
      "          -1.0190e+00, -6.7492e-01,  3.0369e-02,  8.9802e-01,  5.8504e-01],\n",
      "         [ 9.0430e-01,  2.5341e-01,  3.9167e-02, -4.7763e-01,  2.9356e-01,\n",
      "          -2.0527e-01,  1.5188e+00, -3.8680e-01, -7.5635e-02, -4.9527e-01,\n",
      "           1.2786e+00, -3.2872e-01,  1.3825e+00, -2.3748e+00, -1.3261e+00]]])\n",
      " The shape of y is torch.Size([5, 10, 15])\n",
      "The output is torch.Size([5, 10, 15])\n",
      " The shape of output is torch.Size([5, 10, 15])\n"
     ]
    }
   ],
   "source": [
    "out = layer_norm.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    print(f\"Initial Query shape: {q.shape}\")\n",
    "    print(f\"Initial Key shape: {k.shape}\")\n",
    "    print(f\"Initial Value shape: {v.shape}\")\n",
    "\n",
    "    # Compute the scaled dot-product\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    print(f\"Scaled dot product (q * k^T) shape: {scaled.shape}\")\n",
    "\n",
    "    if mask is not None:\n",
    "        print(f\"Mask shape before broadcasting: {mask.shape}\")\n",
    "        # If mask has 3 dimensions (batch_size, seq_len, seq_len), it might need an additional dimension for broadcasting\n",
    "        if mask.ndimension() == 3:\n",
    "            print(f\"Mask has 3 dimensions, adding additional singleton dimensions for multi-head attention.\")\n",
    "            mask = mask.unsqueeze(1)  # Add singleton dimension for multi-head (batch_size, 1, seq_len, seq_len)\n",
    "            print(f\"Mask shape after unsqueeze(1): {mask.shape}\")\n",
    "        \n",
    "        # Check the compatibility of the mask and scaled tensor\n",
    "        print(f\"Scaled tensor shape before adding mask: {scaled.shape}\")\n",
    "        print(f\"Mask shape before adding: {mask.shape}\")\n",
    "\n",
    "        scaled += mask  # Broadcast the mask to the scaled tensor\n",
    "        print(f\"Scaled tensor shape after adding mask: {scaled.shape}\")\n",
    "\n",
    "    # Apply softmax to the scaled tensor\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    print(f\"Attention (after softmax) shape: {attention.shape}\")\n",
    "\n",
    "    # Multiply the attention with values\n",
    "    print(f\"Shape of scaled tensor before matmul with values: {scaled.shape}\")\n",
    "    print(f\"Shape of value tensor (v): {v.shape}\")\n",
    "    values = torch.matmul(scaled, v)\n",
    "    print(f\"Output values shape: {values.shape}\")\n",
    "\n",
    "    return attention, values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_heads, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # 512\n",
    "        self.num_heads = num_heads # 8\n",
    "        self.head_dim = d_model//num_heads # 64\n",
    "        self.qkv_layer = nn.Linear(d_model, 3*d_model) # 512 x 1536\n",
    "        self.linear_layer = nn.Linear(d_model, d_model) # 512 x 512\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        print(f\"x.shape = {x.shape}\")\n",
    "        batch_size, max_seq_length, d_model = x.size()\n",
    "        batch_size, max_seq_length, d_model = x.size()\n",
    "        print(f\"x.size() = {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"x after going through qkv layer - {qkv.size()}\")\n",
    "        qkv_reshape  = qkv.reshape(batch_size, max_seq_length, self.num_heads, 3*self.head_dim)\n",
    "        print(\"--- Entering MultiHeadedAttention ---\")\n",
    "        print(f\"qkv reshaped shape is {qkv_reshape.size()}\")\n",
    "        qkv_permute = qkv_reshape.permute(0,2,1,3)\n",
    "        print(f\"QKV Permuted Shape is - {qkv_permute.size()}\")\n",
    "        print(\"--- Dividing QKV into individual tensors ---\")\n",
    "        q, k, v = qkv_permute.chunk(3, dim = -1)\n",
    "        print(\"Now q, k, v tensors are passed into the Attention Calculation\")\n",
    "        attention, values = scaled_dot_product(q, k, v, mask = mask)\n",
    "        print(\"Now we get the Attention Scores and New Values\")\n",
    "        print(f\"Values shape we get from scaled dot product attention is - {values.size()}\")\n",
    "        print(\"Now we concatinate the values received from scaled dot product.\")\n",
    "        values_reshaped = values.permute(0,2,1,3).reshape(batch_size, max_seq_length, self.num_heads * self.head_dim)\n",
    "        print(f\"New Values from Multiheaded attention output shape is - {values_reshaped.size()}\")\n",
    "        output = self.linear_layer(values_reshaped)\n",
    "        print(f\"Final Output shape from multiheaded attention heads - {output.size()}\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.paramters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.paramters_shape))]\n",
    "        mean = inputs.mean(dim = dims, keepdim = True)\n",
    "        print(f\"Shape of the Mean after preserving the dimensions - {mean.size()}\")\n",
    "        var = ((inputs - mean)**2).mean(dim = dims, keepdim = True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        print(f\"Standard Deviation is {std.size()}\")\n",
    "        normalized_out = (inputs - mean) / std\n",
    "        normalized_out = (normalized_out * self.gamma) + self.beta\n",
    "        print(\"Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\") \n",
    "        print(f\"Shape of gamma and beta are - {self.gamma.size()} and {self.beta.size()}\")\n",
    "        print(f\"Layer Normalized Output shape is {normalized_out.size()}\")\n",
    "        return normalized_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Putting it all together \n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention_layer = MultiHeadAttention(num_heads=num_heads, d_model=d_model)\n",
    "        self.layer_normalizer1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.layer_normalizer2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p = drop_prob)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        residual_x = x\n",
    "        print(\"----- Attention Layer 1 ----\")\n",
    "        x = self.attention_layer(x, mask = mask)\n",
    "        print(f\"Output shape from Multi Head Attention - {x.size()}\")\n",
    "        print(\"---- Dropout 1 ----\")\n",
    "        x = self.dropout1(x)\n",
    "        print(f\"x shape after passing through dropout Layer 1 - {x.size()}\")\n",
    "        print(\"----- Add and Normalizer 1 ---\")\n",
    "        x = self.layer_normalizer1(residual_x + x)\n",
    "        print(f\"x shape after passing through layer normalizer 1 - {x.size()}\")\n",
    "        residual_x = x\n",
    "        print(\"---- Attention 2 through ffn ----\")\n",
    "        x = self.ffn(x)\n",
    "        print(\"----- Dropout 2 -----\")\n",
    "        x = self.dropout2(x)\n",
    "        print(\"----- Add and Normalizer 2 ---\")\n",
    "        x = self.layer_normalizer2(residual_x + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder Block\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(* [EncoderLayer(d_model=d_model, ffn_hidden=ffn_hidden, num_heads=num_heads, drop_prob= drop_prob) \n",
    "                                       for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 32\n",
    "max_seq_length = 200\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderBlock(d_model, ffn_hidden, num_heads,drop_prob, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "# create a dummy x randn input\n",
    "x = torch.randn((batch_size, max_seq_length, d_model))\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Attention Layer 1 ----\n",
      "x.shape = torch.Size([32, 200, 512])\n",
      "x.size() = torch.Size([32, 200, 512])\n",
      "x after going through qkv layer - torch.Size([32, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([32, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([32, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Now we get the Attention Scores and New Values\n",
      "Values shape we get from scaled dot product attention is - torch.Size([32, 8, 200, 64])\n",
      "Now we concatinate the values received from scaled dot product.\n",
      "New Values from Multiheaded attention output shape is - torch.Size([32, 200, 512])\n",
      "Final Output shape from multiheaded attention heads - torch.Size([32, 200, 512])\n",
      "Output shape from Multi Head Attention - torch.Size([32, 200, 512])\n",
      "---- Dropout 1 ----\n",
      "x shape after passing through dropout Layer 1 - torch.Size([32, 200, 512])\n",
      "----- Add and Normalizer 1 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "x shape after passing through layer normalizer 1 - torch.Size([32, 200, 512])\n",
      "---- Attention 2 through ffn ----\n",
      "x after first linear layer: torch.Size([32, 200, 2048])\n",
      "x after activation: torch.Size([32, 200, 2048])\n",
      "x after dropout: torch.Size([32, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
      "----- Dropout 2 -----\n",
      "----- Add and Normalizer 2 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "----- Attention Layer 1 ----\n",
      "x.shape = torch.Size([32, 200, 512])\n",
      "x.size() = torch.Size([32, 200, 512])\n",
      "x after going through qkv layer - torch.Size([32, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([32, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([32, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Now we get the Attention Scores and New Values\n",
      "Values shape we get from scaled dot product attention is - torch.Size([32, 8, 200, 64])\n",
      "Now we concatinate the values received from scaled dot product.\n",
      "New Values from Multiheaded attention output shape is - torch.Size([32, 200, 512])\n",
      "Final Output shape from multiheaded attention heads - torch.Size([32, 200, 512])\n",
      "Output shape from Multi Head Attention - torch.Size([32, 200, 512])\n",
      "---- Dropout 1 ----\n",
      "x shape after passing through dropout Layer 1 - torch.Size([32, 200, 512])\n",
      "----- Add and Normalizer 1 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "x shape after passing through layer normalizer 1 - torch.Size([32, 200, 512])\n",
      "---- Attention 2 through ffn ----\n",
      "x after first linear layer: torch.Size([32, 200, 2048])\n",
      "x after activation: torch.Size([32, 200, 2048])\n",
      "x after dropout: torch.Size([32, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
      "----- Dropout 2 -----\n",
      "----- Add and Normalizer 2 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "----- Attention Layer 1 ----\n",
      "x.shape = torch.Size([32, 200, 512])\n",
      "x.size() = torch.Size([32, 200, 512])\n",
      "x after going through qkv layer - torch.Size([32, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([32, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([32, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Now we get the Attention Scores and New Values\n",
      "Values shape we get from scaled dot product attention is - torch.Size([32, 8, 200, 64])\n",
      "Now we concatinate the values received from scaled dot product.\n",
      "New Values from Multiheaded attention output shape is - torch.Size([32, 200, 512])\n",
      "Final Output shape from multiheaded attention heads - torch.Size([32, 200, 512])\n",
      "Output shape from Multi Head Attention - torch.Size([32, 200, 512])\n",
      "---- Dropout 1 ----\n",
      "x shape after passing through dropout Layer 1 - torch.Size([32, 200, 512])\n",
      "----- Add and Normalizer 1 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "x shape after passing through layer normalizer 1 - torch.Size([32, 200, 512])\n",
      "---- Attention 2 through ffn ----\n",
      "x after first linear layer: torch.Size([32, 200, 2048])\n",
      "x after activation: torch.Size([32, 200, 2048])\n",
      "x after dropout: torch.Size([32, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
      "----- Dropout 2 -----\n",
      "----- Add and Normalizer 2 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "----- Attention Layer 1 ----\n",
      "x.shape = torch.Size([32, 200, 512])\n",
      "x.size() = torch.Size([32, 200, 512])\n",
      "x after going through qkv layer - torch.Size([32, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([32, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([32, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Now we get the Attention Scores and New Values\n",
      "Values shape we get from scaled dot product attention is - torch.Size([32, 8, 200, 64])\n",
      "Now we concatinate the values received from scaled dot product.\n",
      "New Values from Multiheaded attention output shape is - torch.Size([32, 200, 512])\n",
      "Final Output shape from multiheaded attention heads - torch.Size([32, 200, 512])\n",
      "Output shape from Multi Head Attention - torch.Size([32, 200, 512])\n",
      "---- Dropout 1 ----\n",
      "x shape after passing through dropout Layer 1 - torch.Size([32, 200, 512])\n",
      "----- Add and Normalizer 1 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "x shape after passing through layer normalizer 1 - torch.Size([32, 200, 512])\n",
      "---- Attention 2 through ffn ----\n",
      "x after first linear layer: torch.Size([32, 200, 2048])\n",
      "x after activation: torch.Size([32, 200, 2048])\n",
      "x after dropout: torch.Size([32, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
      "----- Dropout 2 -----\n",
      "----- Add and Normalizer 2 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "----- Attention Layer 1 ----\n",
      "x.shape = torch.Size([32, 200, 512])\n",
      "x.size() = torch.Size([32, 200, 512])\n",
      "x after going through qkv layer - torch.Size([32, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([32, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([32, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Now we get the Attention Scores and New Values\n",
      "Values shape we get from scaled dot product attention is - torch.Size([32, 8, 200, 64])\n",
      "Now we concatinate the values received from scaled dot product.\n",
      "New Values from Multiheaded attention output shape is - torch.Size([32, 200, 512])\n",
      "Final Output shape from multiheaded attention heads - torch.Size([32, 200, 512])\n",
      "Output shape from Multi Head Attention - torch.Size([32, 200, 512])\n",
      "---- Dropout 1 ----\n",
      "x shape after passing through dropout Layer 1 - torch.Size([32, 200, 512])\n",
      "----- Add and Normalizer 1 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "x shape after passing through layer normalizer 1 - torch.Size([32, 200, 512])\n",
      "---- Attention 2 through ffn ----\n",
      "x after first linear layer: torch.Size([32, 200, 2048])\n",
      "x after activation: torch.Size([32, 200, 2048])\n",
      "x after dropout: torch.Size([32, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
      "----- Dropout 2 -----\n",
      "----- Add and Normalizer 2 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder_output = encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Headed Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedCrossAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = nn.Linear(d_model, 2 * d_model)\n",
    "        self.q_layer = nn.Linear(d_model, d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, y, mask = None):\n",
    "        batch_size, max_seq_length, d_model = x.size()\n",
    "        print(\"This is the input to the Decoder Layer\")\n",
    "        print(f\"The shape of the input x which comes from Encoder is {x.size()}\")\n",
    "        kv = self.kv_layer(x)\n",
    "        print(\"The Keys and Values for Cross Attention will be coming from Encoder Layer\")\n",
    "        print(f\"The shape of kv is {kv.size()}\")\n",
    "        q = self.q_layer(y)\n",
    "        print(\"The y which is the query matrix will be coming from the masked self attention head from Decoder.\")\n",
    "        print(f\"The shape of q is {q.size()}\")\n",
    "        kv = kv.reshape(batch_size, max_seq_length, num_heads, 2* self.head_dim)\n",
    "        q = q.reshape(batch_size, max_seq_length, num_heads, self.head_dim)\n",
    "        # q shape is 30 x 200 x 8 x 64\n",
    "        # k, v shape is 30 x 200 x 8 x 128\n",
    "        # The shape we want is q = 30 x 8 x 200 x 64\n",
    "        # The shape we want for k, v = 30 x 8 x 200 x 128\n",
    "        kv = kv.permute(0, 2, 1, 3)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k, v = kv.chunk(2, dim = -1)\n",
    "        # the shape of k, v = 30 x 8 x 200 x 64\n",
    "        attention, values = scaled_dot_product(q, k, v, mask = mask)\n",
    "        print(f\"Values and Attention shapes are {values.size()} and {attention.size()}\")\n",
    "        values = values.permute(0,2,1,3).reshape(batch_size, max_seq_length, self.num_heads * self.head_dim)\n",
    "        print(f\"The contextualized values will be reshaped by 'values.reshape(batch_size, max_seq_length, self.num_heads * self.head_dim)'\")\n",
    "        print(f\"The contextualized values will be of shape {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"Values passing through a linear leayer and returns output with shape - {out.size()}\")\n",
    "        return out # 30 x 200 x 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model, ffn_hidden, drop_prob, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
    "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
    "        self.encoder_decoder_cross_attention = MultiHeadedCrossAttention(d_model = d_model, num_heads=num_heads)\n",
    "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p = drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model = d_model,hidden=ffn_hidden,drop_prob=drop_prob)\n",
    "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout3 = nn.Dropout(p = drop_prob)\n",
    "\n",
    "    def forward(self, x, y, decoder_self_attention_mask = None, cross_attention_mask = None):\n",
    "        residual_y = y # 30 x 200 x 512\n",
    "        print(\"----ENTERING MASKED SELF ATTENTION----\")\n",
    "        y = self.self_attention(y, mask = decoder_self_attention_mask)\n",
    "        print(\"---ENTERING DROPOUT 1---\")\n",
    "        y = self.dropout1(y) # 30 x 200 x 512\n",
    "        print(\"---ENTERING LAYERNORM 1---\")\n",
    "        y = self.layer_norm1(residual_y + y) # 30 x 200 x 512\n",
    "\n",
    "        residual_y = y\n",
    "        print(\"---ENTERING CROSS ATTENTION---\")\n",
    "        y = self.encoder_decoder_cross_attention(x, y, mask = cross_attention_mask) # 30 x 200 x 512\n",
    "        print(\"---ENTERING DROPOT 2---\")\n",
    "        y = self.dropout2(y)\n",
    "        print(\"---ENTERING LAYERNORM 2---\")\n",
    "        y = self.layer_norm2(residual_y + y) # 30 x 200 x 512\n",
    "\n",
    "        residual_y = y\n",
    "        print(\"---ENTERING FEED FORWARD LAYER---\")\n",
    "        y = self.ffn(y)\n",
    "        print(\"---ENTERING DROPOUT 3---\")\n",
    "        y = self.dropout3(y)\n",
    "        print(\"---ENTERING LAYERNORM 3---\")\n",
    "        y = self.layer_norm3(residual_y + y)\n",
    "        return y # 30 x 200 x 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(nn.Sequential):\n",
    "#         self._modules is a dictionary of all the layers stored in the SequentialDecoder (inherited from nn.Sequential).\n",
    "#         self._modules.values() gives a list of the layers in the order they were added.\n",
    "\n",
    "    def forward(self, x, y, decoder_self_attention_mask=None, cross_attention_mask=None):\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, y, decoder_self_attention_mask, cross_attention_mask)\n",
    "        return y\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Sequential):\n",
    "    def __init__(self, d_model, num_heads, ffn_hidden, drop_prob, num_layers=5):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[\n",
    "            DecoderLayer(d_model=d_model,\n",
    "                         ffn_hidden=ffn_hidden,\n",
    "                         drop_prob=drop_prob,\n",
    "                         num_heads=num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, y, decoder_self_attention_mask=None, cross_attention_mask=None):\n",
    "        # x shape is 30 x 200 x 512\n",
    "        # y shape is 30 x 200 x 512\n",
    "        return self.layers(x, y, decoder_self_attention_mask, cross_attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 32\n",
    "max_seq_length = 200\n",
    "ffn_hidden = 2048\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ENTERING MASKED SELF ATTENTION----\n",
      "x.shape = torch.Size([32, 200, 512])\n",
      "x.size() = torch.Size([32, 200, 512])\n",
      "x after going through qkv layer - torch.Size([32, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([32, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([32, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before broadcasting: torch.Size([200, 200])\n",
      "Scaled tensor shape before adding mask: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before adding: torch.Size([200, 200])\n",
      "Scaled tensor shape after adding mask: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Now we get the Attention Scores and New Values\n",
      "Values shape we get from scaled dot product attention is - torch.Size([32, 8, 200, 64])\n",
      "Now we concatinate the values received from scaled dot product.\n",
      "New Values from Multiheaded attention output shape is - torch.Size([32, 200, 512])\n",
      "Final Output shape from multiheaded attention heads - torch.Size([32, 200, 512])\n",
      "---ENTERING DROPOUT 1---\n",
      "---ENTERING LAYERNORM 1---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "---ENTERING CROSS ATTENTION---\n",
      "This is the input to the Decoder Layer\n",
      "The shape of the input x which comes from Encoder is torch.Size([32, 200, 512])\n",
      "The Keys and Values for Cross Attention will be coming from Encoder Layer\n",
      "The shape of kv is torch.Size([32, 200, 1024])\n",
      "The y which is the query matrix will be coming from the masked self attention head from Decoder.\n",
      "The shape of q is torch.Size([32, 200, 512])\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before broadcasting: torch.Size([200, 200])\n",
      "Scaled tensor shape before adding mask: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before adding: torch.Size([200, 200])\n",
      "Scaled tensor shape after adding mask: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Values and Attention shapes are torch.Size([32, 8, 200, 64]) and torch.Size([32, 8, 200, 200])\n",
      "The contextualized values will be reshaped by 'values.reshape(batch_size, max_seq_length, self.num_heads * self.head_dim)'\n",
      "The contextualized values will be of shape torch.Size([32, 200, 512])\n",
      "Values passing through a linear leayer and returns output with shape - torch.Size([32, 200, 512])\n",
      "---ENTERING DROPOT 2---\n",
      "---ENTERING LAYERNORM 2---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "---ENTERING FEED FORWARD LAYER---\n",
      "x after first linear layer: torch.Size([32, 200, 2048])\n",
      "x after activation: torch.Size([32, 200, 2048])\n",
      "x after dropout: torch.Size([32, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
      "---ENTERING DROPOUT 3---\n",
      "---ENTERING LAYERNORM 3---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((batch_size, max_seq_length, d_model))\n",
    "# x is the english words sent as input to the Encoder\n",
    "y = torch.randn(batch_size, max_seq_length, d_model)\n",
    "# y is the target language words sent as input to the Decoder\n",
    "mask = torch.full([max_seq_length, max_seq_length], float(\"-inf\"))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "decoder = Decoder(d_model=d_model,\n",
    "                  num_heads=num_heads,\n",
    "                  ffn_hidden=ffn_hidden,\n",
    "                  drop_prob=drop_prob,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "# Use the updated code\n",
    "out = decoder(x, y, mask, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/venu/Documents/Productivity/Pytorch Tutorials/en-te/English_Text.txt',\"r\") as f:\n",
    "    english_text = f.readlines()\n",
    "\n",
    "with open('/Users/venu/Documents/Productivity/Pytorch Tutorials/en-te/Telugu_Text.txt',\"r\") as f:\n",
    "    telugu_text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Heres a look at some of them. \\n',\n",
       "  'For this Rs. \\n',\n",
       "  'Everyone lives happily ever after. \\n',\n",
       "  '\\n'],\n",
       " ['వాటిలో కొన్నిటిని మీకు అందిస్తున్నాము పరిశీలించండి. \\n',\n",
       "  'ఇందులో ఆహారం వండేందు కు రూ. \\n',\n",
       "  'అన్ని ప్రజలు తర్వాత ఎప్పుడైనా సంతోషంగా నివసిస్తున్నట్లు. \\n',\n",
       "  '\\n'])"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_text[-4:],telugu_text[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"[START]\"\n",
    "PADDING_TOKEN = \"[PAD]\"\n",
    "END_TOKEN = \"[END]\"\n",
    "UNK_TOKEN = \"[UNK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "telugu_vocab = [\n",
    "    # Vowels\n",
    "    START_TOKEN,\"అ\", \"ఆ\", \"ఇ\", \"ఈ\", \"ఉ\", \"ఊ\", \"ఋ\", \"ౠ\", \"ఎ\", \"ఏ\", \"ఐ\", \"ఒ\", \"ఓ\", \"ఔ\", \"అం\", \"అః\",\n",
    "    \n",
    "    # Consonants\n",
    "    \"క\", \"ఖ\", \"గ\", \"ఘ\", \"ఙ\", \n",
    "    \"చ\", \"ఛ\", \"జ\", \"ఝ\", \"ఞ\", \n",
    "    \"ట\", \"ఠ\", \"డ\", \"ఢ\", \"ణ\", \n",
    "    \"త\", \"థ\", \"ద\", \"ధ\", \"న\", \n",
    "    \"ప\", \"ఫ\", \"బ\", \"భ\", \"మ\", \n",
    "    \"య\", \"ర\", \"ల\", \"వ\", \n",
    "    \"శ\", \"ష\", \"స\", \"హ\", \"ళ\", \n",
    "    \"క్ష\", \"ఱ\",\n",
    "\n",
    "    # Diacritics (Gunintamulu)\n",
    "    \"ా\", \"ి\", \"ీ\", \"ు\", \"ూ\", \"ె\", \"ే\", \"ై\", \"ొ\", \"ో\", \"ౌ\", \"ం\", \"ః\",\n",
    "    \n",
    "    # Special Characters\n",
    "    \"౦\", \"౧\", \"౨\", \"౩\", \"౪\", \"౫\", \"౬\", \"౭\", \"౮\", \"౯\",  # Telugu digits\n",
    "    \"।\", \"॥\", \":\", \",\", \".\", \"?\", \"!\", \"-\", \"—\", \";\", \"\\\"\", \"\\'\",\" \",\n",
    "    \n",
    "    # Common Words\n",
    "    \"నేను\", \"నువ్వు\", \"మీరు\", \"అతను\", \"ఆమె\", \"వారు\", \"ఇది\", \"అది\", \"మనం\", \"మనము\",\n",
    "    \"ఇంటి\", \"పిల్లలు\", \"కుటుంబం\", \"పుస్తకం\", \"కలం\", \"ప్రభుత్వం\", \"రాష్ట్రం\", \n",
    "    \"పాఠశాల\", \"కళాశాల\", \"విద్యార్థి\", \"గురువు\", \"స్నేహితుడు\", \"పెద్ద\", \"చిన్న\", \n",
    "    \"అమ్మ\", \"నాన్న\", \"వెళ్ళు\", \"రా\", \"తెలుసు\", \"చూడు\", \"మాట్లాడు\", \"ఉండు\", \"ఇష్టం\", \n",
    "    \"వద్దు\", \"స్వాగతం\", \"శుభోదయం\", \"శుభ సాయంత్రం\", \"ధన్యవాదాలు\", \"క్షమించండి\", \"దయచేసి\",\"పిల్లలు\", \"ప్రేమ\", \n",
    "    \"సందేహం\", \"సమయం\", \"వార్త\", \"పెద్దమనిషి\", \"ఆహారం\", \"నీరు\", \"మాట\", \"బయట\", \"లోపల\", \"దారి\", \"రహదారి\", \n",
    "    \"నగరం\", \"గ్రామం\", \"గది\", \"చేప\", \"పండు\", \"కూరగాయ\", \"ఆకులు\", \"పుష్పం\", \"జీవితం\", \"సందర్శన\", \"సంఘం\", \n",
    "    \"సంస్కృతి\", \"గమ్యం\", \"ప్రయాణం\", \"పని\", \"వాక్యం\", \"సమాధానం\", \"పరీక్ష\", \"తప్పు\", \"సరైనది\", \"చరిత్ర\", \"తనిఖీ\", \n",
    "    \"పాఠం\", \"కథ\", \"కోరిక\", \"మనం\", \"అభిప్రాయం\", \"ఉపాయం\", \"నేస్తం\", \"ఆనందం\", \"మహిళ\", \"పురుషుడు\", \"విద్య\", \n",
    "    \"సముద్రం\", \"వెలుగు\", \"నిజం\", \"అబద్దం\", \"నివేదిక\", \"శ్రద్ధ\", \"మార్పు\", \"విజయం\", \"అవకాశం\", \"సాహసం\", \"పోరాటం\", \n",
    "    \"అడుగు\", \"పట్టణం\", \"పట్టుదల\", \"సంకల్పం\", \"నిర్ణయం\", \"జ్ఞానం\", \"ఆశ\", \"ప్రయత్నం\",\"ఆశ్చర్యం\", \"క్లేశం\", \"ఆహ్వానం\", \"భయంకరం\", \n",
    "    \"అందం\", \"సాహసము\", \"గుర్తు\", \"విజ్ఞానం\", \"సంబంధం\", \"ఆరోగ్యం\", \"కవిత\", \"భాష\", \"రంగం\", \"మంచి\", \"చెడు\", \"స్నేహం\", \n",
    "    \"గమనించు\", \"శక్తి\", \"బలహీనత\", \"తల్లి\", \"తండ్రి\", \"అన్న\", \"చెల్లి\", \"తమ్ముడు\", \"మాట్లాడు\", \"నవ్వు\", \"అరచు\", \"బావుందా\", \n",
    "    \"ఎక్కడ\", \"ఎప్పుడు\", \"ఎందుకు\", \"ఎలా\", \"ఎవరూ\", \"దారిని\", \"పోలీసు\", \"దేవుడు\", \"మతం\", \"నమ్మకం\", \"పుణ్యం\", \"పాపం\", \n",
    "    \"ఆసక్తి\", \"కృషి\", \"పరిశ్రమ\", \"ఆహారపదార్థం\", \"బంధం\", \"చెట్టు\", \"కొమ్మ\", \"కొబ్బరి\", \"విత్తనాలు\", \"ఆస్పత్రి\", \"వ్యాధి\", \"చికిత్స\", \n",
    "    \"డాక్టర్\", \"సందేహాలు\", \"పరిరక్షణ\", \"పౌరులు\", \"సమూహం\", \"తీర్పు\", \"ఆజ్ఞ\", \"అడుగు\", \"ఉచితం\", \"లాభం\", \"నష్టము\", \"చరిత్ర\", \n",
    "    \"సంబంధిత\", \"వీధి\", \"నోట్లు\", \"సంగీతం\", \"కళలు\", \"క్రీడలు\", \"పనిముట్లు\", \"సమయం\", \"కార్యక్రమం\", \"పరిశీలన\", \"పలుకరింపు\", \n",
    "    \"పశువులు\", \"జంతువులు\", \"పక్షులు\", \"అంతరిక్షం\", \"భూమి\", \"ప్లానెట్\", \"వాతావరణం\", \"నిబంధన\", \"న్యాయం\", \"కంప్యూటర్\", \"మిత్రుడు\", \n",
    "    \"ఉపాధి\", \"ప్రజలు\",\"ఆకలి\", \"తలుపు\", \"ముళ్లు\", \"చెరువు\", \"మండలం\", \"విషయం\", \"గాలి\", \"దరువు\", \"రైలు\", \"పిట్ట\", \n",
    "\"అడవి\", \"నది\", \"పులి\", \"సింహం\", \"ఏనుగు\", \"జింక\", \"చందమామ\", \"నక్షత్రం\", \"గంట\", \"మంచు\", \n",
    "\"మేఘం\", \"మబ్బులు\", \"కార్తికం\", \"దీపాలు\", \"దీపావళి\", \"సంక్రాంతి\", \"దసరా\", \"హోలీ\", \"దేవాలయం\", \n",
    "\"పూజ\", \"అగ్ని\", \"రుద్రుడు\", \"విష్ణువు\", \"లక్ష్మి\", \"సరస్వతి\", \"శివుడు\", \"హనుమంతుడు\", \"రాముడు\", \n",
    "\"కృష్ణుడు\", \"అనంతం\", \"చిన్నపిల్ల\", \"గౌరవం\", \"నామకరణం\", \"కన్నీళ్లు\", \"స్వభావం\", \"కారణం\", \"క్లిష్టత\", \n",
    "\"ఉత్సాహం\", \"విజయం\", \"తీవ్రత\", \"ఆరంభం\", \"నిరుత్సాహం\", \"గంటల\", \"కాసులు\", \"స్నేహితురాలు\", \"తాత్కాలికం\", \"మరణం\", \n",
    "\"సుఖం\", \"నిట్టూర్పు\", \"ఆశ్చర్యం\", \"భయభీతులు\", \"ఉత్సాహవంతంగా\", \"ఆకర్షణ\", \"బహుమతి\", \"గెలుపు\", \"పోటీ\", \"జీవితం\", \n",
    "\"కలహం\", \"శాంతి\", \"నిర్బంధం\", \"ఆశయం\", \"ప్రగతి\", \"సంఘటనలు\", \"క్రమబద్ధత\", \"మితిమీరిన\", \"సౌకర్యం\", \"సమతూకం\", \n",
    "\"భావన\", \"మౌనం\", \"పూర్తి\", \"పగ\", \"వెండితెర\", \"సినిమాలు\", \"తార\", \"సరదా\", \"స్నేహితులు\", \"ప్రేమ\", \n",
    "\"సంభాషణ\", \"ఆశయం\", \"నిర్ణయం\", \"గోప్యత\", \"చింతన\", \"సమకాలీన\", \"నెమ్మది\", \"సందేహం\", \"శ్రద్ధ\", \"విశ్వాసం\", \n",
    "\"నిబద్ధత\", \"సమర్థత\", \"సమృద్ధి\", \"సాధన\", \"సంపద\", \"గౌరవం\", \"తల్లిదండ్రులు\", \"ప్రేరణ\", \"శక్తివంతం\", \"నిరాశ\", \n",
    "\"భక్తి\", \"ఆచరణ\", \"ప్రజాస్వామ్యం\", \"స్వతంత్రం\", \"సంకల్పం\", \"పరిశ్రమ\", \"సమరసత్వం\", \"రచయితలు\", \"పాఠకులు\", \"కృతజ్ఞత\", \n",
    "\"స్వేచ్ఛ\", \"స్వేచ్ఛ\", \"మాటలు\", \"పరిశీలన\", \"సవాళ్ళు\", \"తేలిక\", \"గమనిక\", \"ఆత్మవిశ్వాసం\", \"ప్రతిపాదనలు\", \"క్రమం\", \n",
    "\"సంకీర్ణత\", \"భావాలు\", \"అనుభవం\", \"ఉపాయం\", \"సాధ్యాలు\", \"శాస్త్రవేత్తలు\", \"పరిశోధనలు\", \"నిపుణులు\", \"సాంకేతికత\", \"పరిశ్రమలు\", \n",
    "\"ఆశయం\", \"రూపకల్పన\", \"నూతనత\", \"ఆవిష్కరణలు\", \"పరిష్కారాలు\", \"సమర్ధత\", \"అంతరిక్ష\", \"రావణుడు\", \"దేవదాసి\", \"రంగస్థలం\", \n",
    "\"పాఠశాల\", \"కళాశాల\", \"అధికారి\", \"పరీక్షలు\", \"విద్యార్థులు\", \"తాత్కాలికం\", \"న్యాయవాది\", \"రాజకీయ నాయకులు\", \"ప్రజల సమస్యలు\", \"పరిష్కారం\", \n",
    "\"నిత్యజీవితం\", \"శ్రామికులు\", \"రైతులు\", \"వ్యాపారులు\", \"సంబంధిత\", \"సేవలు\", \"వ్యాపారాలు\", \"ఆధునిక\", \"పథకాలు\", \"పరిసరాలు\", \n",
    "\"పర్యావరణం\", \"సుస్థిరత\", \"పురుషులు\", \"మహిళలు\", \"పిల్లలు\", \"పెన్షన్\", \"విధానాలు\", \"వ్యవస్థ\", \"సంప్రదాయాలు\", \"ఆచారాలు\", \n",
    "\"పునరుద్ధరణ\", \"సంపద\", \"సాంకేతికత\", \"ఆర్థిక స్థితి\", \"పర్యావరణ పరిరక్షణ\", \"సాంఘిక సమీక్ష\", \"సామాజిక సేవలు\", \"ఆహార భద్రత\", \"వృద్ధి\", \"సమస్యలు\", \n",
    "\"నిరూపణ\", \"సముద్రం\", \"ప్రత్యేకత\", \"పర్యాటకులు\", \"సాంస్కృతిక పర్యటన\", \"సమకాలీన\", \"సాంకేతికత\", \"సౌకర్యాలు\", \"ప్రాంతాలు\", \"నగరాలు\", \n",
    "\"గ్రామాలు\", \"వ్యవస్థలు\", \"సమావేశాలు\", \"పరిశోధన\", \"కళలు\", \"సాహిత్యం\", \"నాటకాలు\", \"సంగీతం\", \"నృత్యం\", \"కవిత్వం\", \n",
    "\"రంగులు\", \"రచనలు\", \"సమగ్రత\", \"ప్రాముఖ్యత\", \"తాత్వికత\", \"సాంస్కృతిక భావాలు\", \"ప్రయోజనాలు\", \"సమాజం\", \"పరిశోధన\", \"కార్యక్రమాలు\", \n",
    "\"సహకారం\", \"సహకార\", \"విప్లవం\", \"ఆలోచనలు\", \"ప్రేరణ\", \"ఆహ్వానం\", \"ప్రసంగం\", \"గమనిక\", \"ప్రారంభం\", \"సమాప్తి\", \n",
    "\"పునాదులు\", \"నిర్వహణ\", \"పరిపాలన\", \"ఆర్థిక స్థిరత్వం\", \"పనితీరులు\", \"నిరూపణ\", \"ప్రత్యేకత\", \"సమగ్రత\", \"సంక్షిప్తత\", \"నిష్కర్ష\", \n",
    "\"ఆశయాలు\", \"విషయాలు\", \"తేదీలు\", \"సంబంధిత\", \"సంకల్పన\", \"సమాధానం\", \"విలువలు\", \"కార్యాచరణ\", \"సందేహం\", \"సమర్థత\", \n",
    "\"ఆశయం\", \"వ్యవస్థ\", \"వాటికలు\", \"విధానాలు\", \"సమర్పణ\", \"సంక్షిప్తత\", \"సాంకేతిక పరిజ్ఞానం\", \"నిరూపణ\", \"సంప్రదాయాలు\", \"క్రమబద్ధత\", \n",
    "\"సమగ్రత\", \"సాంస్కృతిక భావాలు\", \"సాంకేతిక పరిజ్ఞానం\", \"ప్రతిపాదనలు\", \"సమాజం\", \"ప్రగతి\", \"సంపద\", \"ఆర్థిక వ్యవస్థ\", \"పరిశ్రమలు\", \"సంకల్పం\", \n",
    "\"సంప్రదాయాలు\", \"పరిశోధన\", \"విద్య\", \"సాంఘిక\", \"పరిరక్షణ\", \"వృద్ధి\", \"సంపద\", \"సంకల్పం\", \"ప్రసంగం\", \"సమగ్రత\", \n",
    "\"సంకల్పం\", \"ఆలోచనలు\", \"ప్రేరణ\", \"సమాజం\", \"ఆధ్యాత్మికత\", \"సాహిత్యం\", \"నాటకాలు\", \"సంగీతం\", \"కవిత్వం\", \"పరిశోధన\", \n",
    "\"సంకల్పం\", \"సమగ్రత\", \"నిర్వహణ\", \"సంపద\", \"సాంకేతిక పరిజ్ఞానం\", \"సమాజం\", \"సంకల్పం\", \"సమగ్రత\", \"ఆశయాలు\", \"క్రమబద్ధత\", \n",
    "\"సంఘటనలు\", \"ప్రేమ\", \"విశ్వాసం\", \"సంస్కృతి\", \"సమాఖ్యాలు\", \"సంక్షిప్తత\", \"సమాధానం\", \"నిర్వహణ\", \"పరిశీలన\", \"సంపద\",\n",
    "    \n",
    "    # Numbers (written out)\n",
    "    \"సున్న\", \"ఒకటి\", \"రెండు\", \"మూడు\", \"నాలుగు\", \"ఐదు\", \"ఆరు\", \"ఏడు\", \"ఎనిమిది\", \"తొమ్మిది\", \"పది\",\n",
    "    \"పదకొండు\", \"పన్నెండు\", \"ముప్పై\", \"నలభై\", \"యాభై\", \"వంద\", \"వెయ్యి\", \"లక్ష\", \"కోటి\",\n",
    "    \n",
    "    # Common Phrases\n",
    "    \"నమస్కారం\", \"శుభోదయం\", \"శుభరాత్రి\", \"ధన్యవాదాలు\", \"క్షమించండి\", \"అలాగే\", \"అభినందనలు\", \n",
    "    \"నేను బాగున్నాను\", \"మీరు ఎలా ఉన్నారు\", \"నేను మీకు సహాయం చేస్తాను\", \"నాకు తెలీదు\", \n",
    "    \"నాకు ఇష్టం\", \"నాకు సహాయం చేయండి\", \"అది ఏమిటి?\", \"ఇది మీది\", \"నువ్వు ఏమి చేస్తున్నావు?\", \n",
    "    \"అందరూ ఎలా ఉన్నారు?\", \"నేను తెలుగులో మాట్లాడుతున్నాను\", \"తెలుగులో చదువుతున్నాను\",\n",
    "    \n",
    "    # Days of the Week\n",
    "    \"ఆదివారం\", \"సోమవారం\", \"మంగళవారం\", \"బుధవారం\", \"గురువారం\", \"శుక్రవారం\", \"శనివారం\",\n",
    "    \n",
    "    # Directions and Common Terms\n",
    "    \"ఉత్తర\", \"దక్షిణ\", \"తూర్పు\", \"పడమర\", \"కుడి\", \"ఎడమ\", \"వెళ్ళు\", \"రా\", \"ఆగు\", \"చూడు\",\n",
    "    \n",
    "    # Relationship Terms\n",
    "    \"అక్క\", \"చెల్లి\", \"తమ్ముడు\", \"అన్న\", \"నాన్న\", \"అమ్మ\", \"బాబు\", \"పాప\", \"మామ\", \"పిన్ని\", \"బావ\", \"అత్త\",\n",
    "    \n",
    "    # Other Useful Words\n",
    "    \"ఆహారం\", \"నీరు\", \"అవును\", \"కాదు\", \"మనసు\", \"ఆరోగ్యం\", \"ప్రయత్నం\", \"సమయం\", \"తల్లి\", \"తండ్రి\", \n",
    "    \"చెయ్యి\", \"ముఖం\", \"పని\", \"విలువ\", \"బలమైన\", \"ధైర్యం\", \"ఆనందం\", \"సంకల్పం\", \"సంకల్పిత\", \"సత్యం\",\n",
    "    \"సమాధానం\", \"ప్రశ్న\", \"సమాధానం\", \"ప్రయత్నం\", \"విషయం\", \"సహాయం\", \"విజయం\", \"విషయం\", \"సమయం\", \n",
    "    \"కలలు\", \"కలయిక\", \"బాల్యం\", \"వయస్సు\", \"మిత్రం\", \"శ్రద్ధ\", \"కళ\", \"పాట\", \"నాటకం\", \"సినిమా\", \n",
    "    \"సాహిత్యం\", \"కథ\", \"వినోదం\", \"అమ్మ\", \"నాన్న\", \"చెట్టు\", \"పదము\", \"స్నేహితుడు\", \"ఇంట్లో\", \"ఉంటారు\", \"కూరగాయలు\", \"పంట\", \"పాట\",\n",
    "    \"విద్య\", \"అతను\", \"అవును\", \"కాదు\", \"అలాగే\", \"అందరూ\", \"ఇక్కడ\", \"వద్ద\", \"వస్తుంది\", \"వెళ్తుంది\",\n",
    "    \"సరే\", \"తెలుగు\", \"వీరు\", \"ఇలా\", \"ఆకాష్\", \"అతను\", \"వాళ్ళు\", \"దాని\", \"అక్కడ\", \"నిద్ర\", \"ప్రతీ\",\n",
    "    \"సంవత్సరం\", \"తల్లి\", \"తండ్రి\", \"కష్టం\", \"మనిషి\", \"పుస్తకం\", \"చెప్పు\", \"కానీ\", \"మరియు\", \"మొదటి\",\n",
    "    \"గురించి\", \"తర్వాత\", \"ప్రపంచం\", \"వైద్యం\", \"వార్తలు\", \"పాఠశాల\", \"క్లాస్\", \"ఇంగ్లీష్\", \"తనతో\",\n",
    "    \"అమ్మా\", \"తాత\", \"గురువారం\", \"చిన్న\", \"పెద్ద\", \"సాధారణ\", \"రాష్ట్రం\", \"మంచి\", \"రాత్రి\", \"గంట\",\n",
    "    \"సమయం\", \"వాటిని\", \"అప్పటికి\", \"కొంత\", \"మనసు\", \"శరీరం\", \"సేవ\", \"ఉపయోగం\", \"సందేశం\", \"భాష\",\n",
    "    \"సంగీతం\", \"ప్రభుత్వం\", \"సినిమా\", \"చిత్రం\", \"ఇటువంటి\", \"బాలుడు\", \"చేసాడు\", \"రాజు\", \"భక్తి\",\n",
    "    \"ఆనందం\", \"విజయం\", \"విప్లవం\", \"గెలిచాడు\", \"ప్రేమ\", \"అతిగా\", \"మనకు\", \"మా\", \"పిల్లలు\", \"గురువులు\",\n",
    "    \"ముందు\", \"వినండి\", \"చూసి\", \"పోలీ\", \"దారి\", \"అన్నం\", \"మటన్\", \"పుస్తకాలు\", \"ఆరోగ్యం\", \"ప్రయోగం\",\n",
    "    \"భారతదేశం\", \"విధానం\", \"భాష\", \"స్వతంత్రం\", \"భయంకర\", \"ప్రశ్న\", \"చదువు\", \"ఆశ\", \"కవి\", \"కల్పన\",\n",
    "    \"సాహసం\", \"విరసం\", \"వివాహం\", \"తమిళం\", \"కామెడీ\", \"వినోదం\", \"అవకాశం\", \"అలంకారం\", \"మాట్లాడి\",\n",
    "    \"విశ్వాసం\", \"ప్రత్యేకం\", \"వైభవం\", \"నిర్వచనం\", \"సమాజం\", \"నిపుణుడు\", \"సమస్య\", \"నాయుడు\", \"మాటలు\",\n",
    "    \"ఆట\", \"అప్పటి\", \"సాహిత్యం\", \"జీవితం\", \"నాటకం\", \"కథ\", \"గానం\", \"సమయం\", \"కళ\", \"కార్యక్రమం\",\n",
    "    \"పెళ్లి\", \"మరువకండి\", \"సంస్కృతి\", \"తిరిగి\", \"తక్కువ\", \"పని\", \"అమ్మకాలు\", \"విశ్రాంతి\", \"యాత్ర\",\n",
    "    \"నడపడం\", \"గడచిన\", \"ప్రసంగం\", \"చివరి\", \"అమ్మాయి\", \"వారిని\", \"రూపం\", \"వలె\", \"అర్థం\", \"దారిలో\",\n",
    "    \"ఉన్నా\", \"చెబుతున్నారు\", \"జీవితం\", \"విశ్రాంతి\", \"అభిమానం\", \"కనుగొన్నారు\", \"మాట\", \"ఉండటానికి\",\n",
    "    \"సభ్యుడు\", \"అంతే\", \"క్లిష్టం\", \"చేసిన\", \"నిర్ణయం\", \"మనం\", \"మరల\", \"ఆలోచనలు\", \"ఆచారాలు\",\n",
    "    \"భూమి\", \"వృత్తి\", \"చిత్రకారుడు\", \"మానసిక\", \"దేవుడు\", \"ప్రజలు\", \"మార్గం\", \"ఇలా\", \"గర్వం\",\n",
    "    \"వినయము\", \"ఆకర్షణ\", \"అంతరించిపోవడం\", \"క్రమం\", \"మరింత\", \"ముందు\", \"వివరణ\", \"నాకు\", \"కోసం\", \"రంగు\",\n",
    "    \"విధానము\", \"ప్రార్థన\", \"కారణం\", \"పరిశోధన\", \"నిర్ణయాలు\", \"మంచినీళ్ళు\", \"స్వరాలు\", \"ప్రదర్శన\", \n",
    "    \"అభ్యాసం\", \"పరిచయం\", \"గురువు\", \"తల\", \"వాయు\", \"విద్యుత్\", \"ఆశయం\", \"కార్యం\", \"విజ్ఞానం\", \n",
    "    \"పురుషుడు\", \"స్త్రీ\", \"పిల్ల\", \"ప్రసిద్ధి\", \"మంచి\", \"తప్పు\", \"తీరు\", \"పరిహారం\", \"ఆశయం\", \n",
    "    \"విద్యా\", \"ఉపకరణం\", \"సమయము\", \"పరిష్కారం\", \"క్లుప్తంగా\", \"సంకల్పం\", \"సేవ\", \"కృతజ్ఞత\", \n",
    "    \"ప్రకృతి\", \"నాటకం\", \"ఆట\", \"ఆరోపణలు\", \"సమాధానం\", \"వ్యాసం\", \"పరిశీలన\", \"సూచన\", \"పాఠం\", \n",
    "    \"పర్యావరణం\", \"స్పష్టం\", \"స్వేచ్ఛ\", \"సాధన\", \"సంకల్పం\", \"విజ్ఞానం\", \"ప్రయత్నం\", \"పౌరాణికం\", \n",
    "    \"ప్రపంచం\", \"సంగీతం\", \"ప్రకటన\", \"సంఘర్షణ\", \"సాహసము\", \"పరీక్ష\", \"శాంతి\", \"పట్టణం\", \"ఆశయం\", \n",
    "    \"విశ్లేషణ\", \"వ్యాసం\", \"కవిత్వం\", \"అభిమానం\", \"రంగస్థలం\", \"గమనిక\", \"పోరాటం\", \"సహనం\", \n",
    "    \"అవకాశం\", \"పునాది\", \"పరిశీలన\", \"అభివృద్ధి\", \"చర్చ\", \"సమర్థత\", \"రచయిత\", \"ప్రశ్న\", \"క్లుప్తం\", \n",
    "    \"పౌరుడు\", \"విశ్వాసం\", \"సేవలు\", \"విశేషణం\", \"ఆశయం\", \"చారిత్రక\", \"మాట\", \"రహస్యం\", \"వీణ\", \n",
    "    \"శాంతి\", \"చిత్రకారుడు\", \"వివరణ\", \"మానవత్వం\", \"సమాధానం\", \"పలుకులు\", \"సేవ\", \"శిక్షణ\", \n",
    "    \"చింతన\", \"సిద్ధాంతం\", \"సంఘటన\", \"సమితి\", \"రాజకీయాలు\", \"ఆకర్షణ\", \"కవిత్వం\", \"ఆలోచనలు\", \n",
    "    \"పరిశోధన\", \"అభ్యసనము\", \"రుచులు\", \"అతిధి\", \"సంకల్పం\",\n",
    "\n",
    "    # Numbers\n",
    "    \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", PADDING_TOKEN,END_TOKEN, UNK_TOKEN\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = [\n",
    "    # Alphabet\n",
    "    START_TOKEN, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \n",
    "    \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\",\n",
    "    \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \n",
    "    \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\",\n",
    "\n",
    "    # Numbers\n",
    "    \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n",
    "\n",
    "    # Common Punctuation\n",
    "    \".\", \",\", \"!\", \"?\", \":\", \";\", \"-\", \"—\", \"\\\"\", \"\\'\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \n",
    "    \"&\", \"@\", \"#\", \"$\", \"%\", \"*\", \"+\", \"=\", \"<\", \">\", \"/\",\" \",\n",
    "\n",
    "    # Common Words\n",
    "    \"the\", \"of\", \"and\", \"to\", \"a\", \"in\", \"that\", \"is\", \"was\", \"he\", \"for\", \"it\", \"with\", \"as\", \n",
    "    \"his\", \"on\", \"be\", \"at\", \"by\", \"i\", \"this\", \"had\", \"not\", \"are\", \"but\", \"from\", \"or\", \n",
    "    \"have\", \"an\", \"they\", \"which\", \"one\", \"you\", \"were\", \"her\", \"all\", \"she\", \"there\", \n",
    "    \"would\", \"their\", \"we\", \"him\", \"been\", \"has\", \"when\", \"who\", \"will\", \"more\", \"if\", \n",
    "    \"no\", \"out\", \"do\", \"so\", \"can\", \"what\", \"up\", \"said\", \"about\", \"other\", \"into\", \n",
    "    \"than\", \"its\", \"time\", \"only\", \"could\", \"new\", \"them\", \"man\", \"some\", \"these\", \"then\", \n",
    "    \"two\", \"first\", \"may\", \"any\", \"like\", \"now\", \"my\", \"such\", \"make\", \"over\", \"our\", \"even\", \n",
    "    \"most\", \"me\", \"state\", \"after\", \"also\", \"made\", \"many\", \"did\", \"must\", \"before\", \n",
    "    \"back\", \"see\", \"through\", \"way\", \"where\", \"get\", \"much\", \"go\", \"good\", \"how\", \"know\", \n",
    "    \"should\", \"well\", \"people\", \"down\", \"own\", \"just\", \"because\", \"each\", \"those\", \n",
    "    \"both\", \"under\", \"last\", \"never\", \"us\", \"leave\", \"put\", \"every\", \"why\", \"ask\", \"another\", \n",
    "    \"going\", \"take\", \"thought\", \"still\", \"work\", \"found\", \"use\", \"day\", \"too\", \"part\", \n",
    "    \"little\", \"come\", \"around\", \"against\", \"three\", \"place\", \"again\", \"same\", \"look\", \"lot\", \n",
    "    \"right\", \"set\", \"next\", \"while\", \"house\", \"give\", \"find\", \"school\", \"run\", \"help\", \n",
    "    \"number\", \"end\", \"long\", \"off\", \"open\", \"best\", \"keep\", \"saw\", \"might\", \"let\", \n",
    "    \"better\", \"ever\", \"once\", \"old\", \"tell\", \"few\", \"keep\", \"want\", \"does\", \"really\", \n",
    "    \"family\", \"friend\", \"try\", \"important\", \"need\", \"feel\", \"world\", \"try\", \"start\", \"life\", \n",
    "    \"great\", \"mean\", \"high\", \"thing\", \"small\", \"big\", \"change\", \"own\", \"idea\", \"show\", \n",
    "    \"live\", \"ask\", \"call\", \"different\", \"try\", \"let\", \"turn\", \"children\", \"begin\", \n",
    "    \"important\", \"move\", \"close\", \"happen\", \"write\", \"talk\", \"country\", \"city\", \"week\", \n",
    "    \"morning\", \"night\", \"today\", \"early\", \"later\", \"year\", \"old\", \"young\", \"feel\", \"ask\", \n",
    "    \"yes\", \"no\", \"maybe\", \"sure\", \"not\", \"okay\", \"please\", \"thank\", \"sorry\", \"excuse\", \n",
    "    \"hello\", \"hi\", \"goodbye\", \"bye\", \"morning\", \"evening\", \"afternoon\", \"name\", \"happy\", \n",
    "    \"sad\", \"angry\", \"glad\", \"tired\", \"sleep\", \"awake\", \"beautiful\", \"ugly\", \"right\", \n",
    "    \"wrong\", \"fast\", \"slow\", \"hot\", \"cold\", \"light\", \"dark\", \"rain\", \"sun\", \"cloud\", \n",
    "    \"snow\", \"wind\", \"earth\", \"water\", \"fire\", \"air\", \"land\", \"sea\", \"tree\", \"forest\", \n",
    "    \"mountain\", \"river\", \"lake\", \"food\", \"drink\", \"eat\", \"apple\", \"bread\", \"rice\", \"meat\", \n",
    "    \"milk\", \"water\", \"tea\", \"coffee\", \"juice\", \"sweet\", \"sour\", \"salt\", \"pepper\", \"spice\", \n",
    "    \"soup\", \"fruit\", \"vegetable\", \"dog\", \"cat\", \"bird\", \"fish\", \"cow\", \"horse\", \"pig\", \n",
    "    \"lion\", \"tiger\", \"elephant\", \"mouse\", \"rat\", \"animal\", \"nature\", \"human\", \"body\", \n",
    "    \"hand\", \"head\", \"arm\", \"leg\", \"eye\", \"ear\", \"nose\", \"mouth\", \"tooth\", \"foot\", \"toe\", \n",
    "    \"finger\", \"knee\", \"back\", \"chest\", \"heart\", \"brain\", \"blood\", \"skin\", \"bone\", \"health\", \n",
    "    \"doctor\", \"nurse\", \"medicine\", \"hospital\", \"sick\", \"hurt\", \"pain\", \"cure\", \"care\", \n",
    "    \"clean\", \"dirty\", \"wash\", \"water\", \"rest\", \"sleep\", \"dream\", \"hope\", \"wish\", \"pray\", \n",
    "    \"love\", \"hate\", \"fear\", \"happy\", \"sad\", \"angry\", \"glad\", \"bored\", \"excited\", \"afraid\", \n",
    "    \"shy\", \"proud\", \"lonely\", \"kind\", \"brave\", \"wise\", \"funny\", \"serious\", \"gentle\", \n",
    "    \"strong\", \"weak\", \"hard\", \"soft\", \"easy\", \"difficult\", \"safe\", \"dangerous\", \"old\", \n",
    "    \"young\", \"new\", \"smart\", \"foolish\", \"rich\", \"poor\", \"wealthy\", \"money\", \"bank\", \"price\",\"about\", \n",
    "    \"above\", \"accept\", \"accident\", \"across\", \"advice\", \"afraid\", \"after\", \"again\", \"against\",\n",
    "\"agree\", \"airplane\", \"almost\", \"along\", \"always\", \"angry\", \"animal\", \"another\", \"answer\", \"anybody\",\n",
    "\"anything\", \"anywhere\", \"appear\", \"apple\", \"around\", \"arrive\", \"artist\", \"asleep\", \"attack\", \"attention\",\n",
    "\"August\", \"autumn\", \"beautiful\", \"because\", \"before\", \"beginning\", \"believe\", \"belong\", \"below\", \"beside\",\n",
    "\"between\", \"beyond\", \"bicycle\", \"billion\", \"birthday\", \"blanket\", \"bottle\", \"bottom\", \"bought\", \"breakfast\",\n",
    "\"brother\", \"brought\", \"building\", \"butterfly\", \"careful\", \"carpet\", \"carrot\", \"castle\", \"center\", \"change\",\n",
    "\"chicken\", \"children\", \"circle\", \"clothes\", \"cloudy\", \"coffee\", \"collect\", \"color\", \"common\", \"complete\",\n",
    "\"concert\", \"correct\", \"cotton\", \"country\", \"cousin\", \"create\", \"culture\", \"curious\", \"current\", \"curtain\",\n",
    "\"dancing\", \"daughter\", \"decide\", \"different\", \"dinner\", \"discover\", \"distance\", \"doctor\", \"drawing\", \"dream\",\n",
    "\"easier\", \"easily\", \"education\", \"effect\", \"effort\", \"elephant\", \"energy\", \"enough\", \"entire\", \"escape\",\n",
    "\"evening\", \"exactly\", \"example\", \"excited\", \"exercise\", \"expensive\", \"explain\", \"famous\", \"farmer\", \"feeling\",\n",
    "\"finally\", \"flower\", \"forever\", \"forgot\", \"friendly\", \"frozen\", \"future\", \"garden\", \"general\", \"gentle\",\n",
    "\"giant\", \"girlfriend\", \"glasses\", \"golden\", \"grandfather\", \"grandmother\", \"grocery\", \"happiest\", \"happily\", \"health\",\n",
    "\"history\", \"holiday\", \"horizon\", \"hospital\", \"imagine\", \"improve\", \"include\", \"insect\", \"instead\", \"interesting\",\n",
    "\"jungle\", \"kitchen\", \"knowledge\", \"language\", \"learning\", \"library\", \"listening\", \"manager\", \"marriage\", \"message\",\"morning\", \n",
    "\"mountain\", \"mystery\", \"natural\", \"neighbor\", \"notebook\", \"outside\", \"painting\", \"perfect\", \"picture\", \"practice\", \"prepare\",\n",
    "\"ability\", \"absence\", \"academy\", \"account\", \"active\", \"activity\", \"addition\", \"address\", \"admit\", \"advance\",\n",
    "    \"adventure\", \"advice\", \"affect\", \"afternoon\", \"agreement\", \"airport\", \"alarm\", \"alike\", \"alive\", \"allow\",\n",
    "    \"alphabet\", \"amazing\", \"amount\", \"ancient\", \"angel\", \"anger\", \"annoy\", \"answering\", \"apology\", \"appearance\",\n",
    "    \"appetite\", \"approve\", \"area\", \"argument\", \"arrival\", \"artist\", \"attempt\", \"attention\", \"attraction\", \"average\",\n",
    "    \"avoid\", \"baby\", \"balance\", \"balloon\", \"banana\", \"barrier\", \"baseball\", \"basket\", \"beach\", \"behavior\",\n",
    "    \"benefit\", \"beyond\", \"bicycle\", \"bigger\", \"blanket\", \"blessing\", \"blind\", \"blow\", \"board\", \"body\",\n",
    "    \"bold\", \"border\", \"borrow\", \"bottle\", \"brave\", \"bridge\", \"brief\", \"brilliant\", \"brotherhood\", \"brush\",\n",
    "    \"bubble\", \"bucket\", \"budget\", \"builder\", \"burn\", \"cable\", \"calendar\", \"camera\", \"campus\", \"cancel\",\n",
    "    \"capture\", \"careful\", \"carrier\", \"carrot\", \"cash\", \"castle\", \"caution\", \"celebrate\", \"certain\", \"chance\",\n",
    "    \"chapter\", \"charge\", \"charm\", \"cheese\", \"chemical\", \"choice\", \"chocolate\", \"circle\", \"classic\", \"climate\",\n",
    "    \"closer\", \"cloud\", \"coach\", \"coin\", \"college\", \"combine\", \"comfort\", \"committee\", \"compare\", \"compass\",\n",
    "    \"complaint\", \"complete\", \"complex\", \"conclusion\", \"confident\", \"confusion\", \"connection\", \"conquer\", \"constant\", \"container\",\n",
    "    \"content\", \"contest\", \"context\", \"continue\", \"contract\", \"contrast\", \"control\", \"convert\", \"correct\", \"costume\",\n",
    "    \"cottage\", \"courage\", \"course\", \"courtesy\", \"create\", \"creature\", \"credit\", \"crime\", \"critical\", \"crush\",\n",
    "    \"crystal\", \"culture\", \"curious\", \"current\", \"custom\", \"danger\", \"darkness\", \"database\", \"debate\", \"decade\",\n",
    "    \"decision\", \"decline\", \"decorate\", \"define\", \"degree\", \"deliver\", \"density\", \"depend\", \"depth\", \"desire\",\n",
    "    \"destroy\", \"detect\", \"develop\", \"device\", \"diamond\", \"differ\", \"digital\", \"dining\", \"direction\", \"director\",\n",
    "    \"discover\", \"disease\", \"dislike\", \"distance\", \"district\", \"diverse\", \"document\", \"donation\", \"double\", \"dragon\",\n",
    "    \"drawer\", \"dreamer\", \"driver\", \"duet\", \"duty\", \"eager\", \"early\", \"earthquake\", \"easily\", \"economy\",\n",
    "    \"effort\", \"elder\", \"election\", \"elevator\", \"emotion\", \"empire\", \"energy\", \"engage\", \"engine\", \"enjoy\",\n",
    "    \"enough\", \"ensure\", \"enter\", \"enthusiasm\", \"entire\", \"equal\", \"equation\", \"escape\", \"essential\", \"event\",\n",
    "    \"evidence\", \"example\", \"exchange\", \"excuse\", \"exercise\", \"exist\", \"expand\", \"expect\", \"explore\", \"express\",\n",
    "    \"extend\", \"extra\", \"failure\", \"famous\", \"farmer\", \"fashion\", \"feather\", \"feature\", \"festival\", \"fiction\",\n",
    "    \"field\", \"figure\", \"finance\", \"fishing\", \"fitness\", \"flower\", \"focus\", \"follow\", \"forest\", \"forever\",\n",
    "    \"formal\", \"fortune\", \"found\", \"freedom\", \"freeze\", \"friendship\", \"frustration\", \"function\", \"future\", \"gadget\",\n",
    "    \"gallery\", \"garage\", \"garden\", \"garment\", \"gateway\", \"gather\", \"gentleman\", \"gesture\", \"giant\", \"global\",\n",
    "    \"golden\", \"govern\", \"grammar\", \"grape\", \"graph\", \"gravity\", \"greet\", \"guarantee\", \"guest\", \"guide\",\n",
    "    \"habit\", \"harmony\", \"harvest\", \"health\", \"heritage\", \"history\", \"horizon\", \"host\", \"humanity\", \"humble\",\n",
    "    \"hunger\", \"hurry\", \"identity\", \"imagine\", \"impact\", \"import\", \"impress\", \"improve\", \"include\", \"income\",\n",
    "    \"increase\", \"influence\", \"inform\", \"initial\", \"insect\", \"inspire\", \"install\", \"instant\", \"institute\", \"interest\",\n",
    "    \"interview\", \"invent\", \"invest\", \"invite\", \"island\", \"issue\", \"item\", \"jacket\", \"jewel\", \"journey\",\n",
    "    \"justice\", \"kindness\", \"kitchen\", \"kingdom\", \"knowledge\", \"labor\", \"landscape\", \"language\", \"laughter\", \"leader\",\n",
    "    \"league\", \"legend\", \"library\", \"lightning\", \"limit\", \"literature\", \"location\", \"logic\", \"loyalty\", \"machine\",\n",
    "    \"magazine\", \"magic\", \"manner\", \"market\", \"master\", \"matter\", \"medicine\", \"memory\", \"mention\", \"message\",\n",
    "    \"method\", \"mineral\", \"mirror\", \"mission\", \"mixture\", \"model\", \"moment\", \"monitor\", \"motion\", \"mountain\",\n",
    "    \"museum\", \"mystery\", \"nation\", \"nature\", \"nearby\", \"network\", \"newspaper\", \"nightmare\", \"noble\", \"notion\",\n",
    "    \"nuclear\", \"number\", \"nurture\", \"object\", \"occasion\", \"ocean\", \"officer\", \"operation\", \"opinion\", \"option\",\n",
    "    \"orbit\", \"origin\", \"outcome\", \"outside\", \"overall\", \"overcome\", \"package\", \"palace\", \"panic\", \"parade\",\n",
    "    \"parcel\", \"parent\", \"passage\", \"passion\", \"pattern\", \"peaceful\", \"perform\", \"period\", \"permit\", \"persuade\",\n",
    "    \"photograph\", \"phrase\", \"physics\", \"piano\", \"picture\", \"planet\", \"plastic\", \"pleasure\", \"poetry\", \"policy\",\n",
    "    \"polite\", \"portion\", \"position\", \"positive\", \"potential\", \"practice\", \"praise\", \"precious\", \"prepare\", \"present\",\n",
    "    \"pressure\", \"prevent\", \"primary\", \"private\", \"problem\", \"process\", \"produce\", \"product\", \"progress\", \"promise\",\n",
    "    \"property\", \"protect\", \"provide\", \"purpose\", \"puzzle\", \"quality\", \"quantity\", \"quarter\", \"question\", \"quickly\",\n",
    "    \"quiet\", \"radiant\", \"rainbow\", \"random\", \"rapid\", \"reaction\", \"reading\", \"reason\", \"recall\", \"recipe\",\n",
    "    \"recover\", \"recycle\", \"reflect\", \"refuse\", \"regret\", \"related\", \"release\", \"reliable\", \"remind\", \"remote\",\n",
    "    \"remove\", \"repair\", \"replace\", \"request\", \"require\", \"rescue\", \"reserve\", \"respect\", \"respond\", \"result\",\n",
    "    \"return\", \"reveal\", \"reverse\", \"reward\", \"rhythm\", \"richness\", \"riddle\", \"romantic\", \"routine\", \"safety\",\n",
    "    \"satisfy\", \"scholar\", \"science\", \"season\", \"secure\", \"select\", \"senior\", \"sensitive\", \"serious\", \"service\",\n",
    "    \"shadow\", \"shelter\", \"shining\", \"shorten\", \"sibling\", \"signal\", \"silence\", \"similar\", \"simple\", \"singer\",\n",
    "    \"situation\", \"skillful\", \"skyline\", \"society\", \"soldier\", \"solution\", \"special\", \"species\", \"spiritual\", \"stadium\",\n",
    "    \"station\", \"statue\", \"stomach\", \"storage\", \"strategy\", \"stretch\", \"student\", \"success\", \"suitable\", \"summary\",\n",
    "    \"sunrise\", \"sunset\", \"support\", \"surface\", \"surprise\", \"survive\", \"sustain\", \"symbol\", \"system\", \"teacher\",\n",
    "    \"technical\", \"technology\", \"temple\", \"temporary\", \"tension\", \"texture\", \"theater\", \"therapy\", \"thought\", \"thunder\",\n",
    "    \"together\", \"tourist\", \"tradition\", \"traffic\", \"training\", \"transform\", \"travel\", \"treasure\", \"treatment\", \"triangle\",\n",
    "    \"trouble\", \"unique\", \"universe\", \"utility\", \"vacation\", \"valley\", \"value\", \"vehicle\", \"venture\", \"victory\",\n",
    "    \"village\", \"violence\", \"virtue\", \"visitor\", \"volcano\", \"volume\", \"warning\", \"warrior\", \"wealth\", \"welcome\",\n",
    "    \"welfare\", \"whisper\", \"whistle\", \"wildlife\", \"wisdom\", \"witness\", \"wonderful\", \"workplace\", \"worship\", \"worthy\",\n",
    "    \"writing\", \"yesterday\", \"yourself\", \"youth\", \"zebra\", \"zero\", \"zone\",\"this\",\n",
    "    \n",
    "    # Days of the Week\n",
    "    \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\",\n",
    "\n",
    "    # Months\n",
    "    \"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \n",
    "    \"October\", \"November\", \"December\",\n",
    "\n",
    "    # Numbers (written out)\n",
    "    \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \n",
    "    \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \n",
    "    \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \n",
    "    \"eighty\", \"ninety\", \"hundred\", \"thousand\", \"million\", \"billion\",\n",
    "\n",
    "    # Numbers\n",
    "    \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", PADDING_TOKEN, END_TOKEN, UNK_TOKEN\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "telugu_vocab_size = len(telugu_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab_size = len(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['మ', 'ు', 'ఖ', 'ం']"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample checking\n",
    "\n",
    "word = \"ముఖం\"\n",
    "list(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ముఖం'"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'మ'+ 'ు'+ 'ఖ'+ 'ం'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing English and Telugu Vocab\n",
    "\n",
    "index_to_telugu = {k:v for k,v in enumerate(telugu_vocab)}\n",
    "telugu_to_index = {v:k for v,k in enumerate(telugu_vocab)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocab)}\n",
    "english_to_index = {v:k for v,k in enumerate(english_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1012, 1283)"
      ]
     },
     "execution_count": 767,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_telugu), len(index_to_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2473018"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limiting number of sentences for training\n",
    "len(english_text), len(telugu_text)\n",
    "TOTAL_SENTENCES = len(english_text) // 2\n",
    "TOTAL_SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2473018, 2473018)"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences = english_text[:TOTAL_SENTENCES]\n",
    "telugu_sentences = telugu_text[:TOTAL_SENTENCES]\n",
    "len(english_sentences), len(telugu_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripping new line\n",
    "english_sentences = [sentence.rstrip('\\n') for sentence in english_sentences]\n",
    "telugu_sentences = [sentence.rstrip('\\n') for sentence in telugu_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have you heard about Foie gras? ',\n",
       " 'I never thought of acting in films. ',\n",
       " 'Installed Software ',\n",
       " 'A case has been registered under Sections 302 and 376, IPC. ']"
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Have you heard about Foie gras? ',\n",
       "  'I never thought of acting in films. ',\n",
       "  'Installed Software ',\n",
       "  'A case has been registered under Sections 302 and 376, IPC. ',\n",
       "  'Of this, 10 people succumbed to the injuries. '],\n",
       " ['ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా? ',\n",
       "  'సూర్య సినిమాల్లో నటించాలని ఎప్పుడూ అనుకోలేదు. ',\n",
       "  'స్థాపించబడిన సాఫ్ట్\\u200dవేర్ ',\n",
       "  'నిందితులపై సెక్షన్ 376 మరియు 302ల కింద కేసు నమోదు చేశాం. ',\n",
       "  'అందులో 10 మంది తీవ్రంగా గాయపడ్డారు. '])"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:5], telugu_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9268, 20042)"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking max lenght of english and telugu sentences\n",
    "max_english_sentence_lenght = max(len(sentence) for sentence in english_sentences)\n",
    "max_telugu_sentence_length = max(len(sentence) for sentence in telugu_sentences)\n",
    "max_english_sentence_lenght, max_telugu_sentence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/r3k6h0y11431_5th45hwjmjr0000gn/T/ipykernel_42168/2461350469.py:8: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([english_lengths, telugu_lengths], labels=['English', 'Telugu'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate sentence lengths\n",
    "english_lengths = [len(sentence) for sentence in english_sentences]\n",
    "telugu_lengths = [len(sentence) for sentence in telugu_sentences]\n",
    "\n",
    "# Plot boxplots\n",
    "plt.boxplot([english_lengths, telugu_lengths], labels=['English', 'Telugu'])\n",
    "plt.ylabel('Sentence Length')\n",
    "plt.title('Sentence Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentences under 97th percentile: 179.0\n",
      "Telugu sentences under 97th percentile: 170.0\n"
     ]
    }
   ],
   "source": [
    "# restricting the max length sentences to be allowed to 97 percentile\n",
    "# Calculate sentence lengths\n",
    "en_sentence_lengths = [len(sentence) for sentence in english_sentences]\n",
    "te_sentence_lengths = [len(sentence) for sentence in telugu_sentences]\n",
    "\n",
    "# Calculate desired percentile\n",
    "percentile = 97\n",
    "en_value = np.percentile(en_sentence_lengths, percentile)\n",
    "te_value = np.percentile(te_sentence_lengths, percentile)\n",
    "\n",
    "print(f\"English sentences under {percentile}th percentile: {en_value}\")\n",
    "print(f\"Telugu sentences under {percentile}th percentile: {te_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing sentences only which have characters in vocab\n",
    "\n",
    "# def is_valid_tokens(sentences, vocab):\n",
    "#     for token in list(set(sentences)):\n",
    "#         if token not in vocab:\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "def is_valid_tokens(sentence, vocab, oov_threshold=0.3):\n",
    "    oov_count = sum(1 for token in sentence if token not in vocab)\n",
    "    oov_ratio = oov_count / len(sentence)\n",
    "    return oov_ratio <= oov_threshold\n",
    "\n",
    "\n",
    "# passing sentences only if they have length less then the max sequence length\n",
    "def is_valid_length(sentence, max_seq_length):\n",
    "    return len(list(sentence)) < (max_seq_length - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sentence_indices = []\n",
    "\n",
    "for index in range(len(telugu_sentences)):\n",
    "    telugu_sentence, english_sentence = telugu_sentences[index], english_sentences[index]\n",
    "    if is_valid_length(telugu_sentence, max_seq_length)\\\n",
    "    and is_valid_length(telugu_sentence, max_seq_length)\\\n",
    "        and is_valid_tokens(telugu_sentence, telugu_vocab):\n",
    "        valid_sentence_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2423581"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences taken: 2473018\n",
      "Number of Valid Sentences - 2423581\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of sentences taken: {len(english_sentences)}\")\n",
    "print(f\"Number of Valid Sentences - {len(valid_sentence_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The investigation of the case has been taken up. ',\n",
       "  'The bodies have been taken to Osmania Hospital for post-mortem. ',\n",
       "  'A light rain is falling. '],\n",
       " ['కేసు దర్యాప్తు వేగవంతం చేశారు. ',\n",
       "  'మృతదేహాలను పోస్టుమార్టమ్ నిమిత్తం ఉస్మానియాకు తరలించారు. ',\n",
       "  'చిన్నచిన్నగా వాన చినుకులు పడుతున్నాయి. '])"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_telugu_sentences = [telugu_sentences[i] for i in valid_sentence_indices]\n",
    "valid_english_sentences = [english_sentences[i] for i in valid_sentence_indices]\n",
    "\n",
    "valid_english_sentences[-3:], valid_telugu_sentences[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create passing mask, look ahead mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(sequence_lengths, max_seq_length):\n",
    "    \"\"\"\n",
    "    1. Initialize a tensor mask with shape (batch_size, max_seq_length) filled with True values.\n",
    "    2. Iterate through each sequence length in sequence_lengths.\n",
    "    3. For each sequence, set the first length positions in the corresponding row of mask to False - mask[i, :length] = False\n",
    "    it goes to nth row and from nth row , untill nth column value it marks false indicating valid values.\n",
    "    \"\"\"\n",
    "    batch_size = len(sequence_lengths)\n",
    "    mask = torch.ones((batch_size, max_seq_length), dtype=torch.bool)\n",
    "\n",
    "    for i, length in enumerate(sequence_lengths):\n",
    "        mask[i, :length] = False  # Set valid sequence positions to False\n",
    "\n",
    "    return mask\n",
    "\n",
    "# if max_seq_length = 5, batch_size = 3\n",
    "# seq_length = [3,2,4]\n",
    "# expand(batch_size, max_seq_length)\n",
    "# [\n",
    "#     [0, 1, 2, 3, 4],\n",
    "#     [0, 1, 2, 3, 4],\n",
    "#     [0, 1, 2, 3, 4]\n",
    "# ]\n",
    "\n",
    "# torch.tensor(sequence_lengths).unsqueeze(1)\n",
    "# [\n",
    "#     [3],\n",
    "#     [2],\n",
    "#     [4]\n",
    "# ]\n",
    "# >= compares each sequence index with corresponding sequence length.\n",
    "\n",
    "# [\n",
    "#     [False, False, False, True, True],\n",
    "#     [False, False, True, True, True],\n",
    "#     [False, False, False, False, True]\n",
    "# ]\n",
    "# False represnts actual tokens and True represents padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(max_seq_length):\n",
    "    \"\"\" Creates a look ahead mask to block future tokens in the sequnce\"\"\"\n",
    "    return torch.triu(torch.ones((max_seq_length, max_seq_length), dtype = torch.bool), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, max_seq_length, pad_token_id):\n",
    "\n",
    "    pad_length = max_seq_length - len(sequence)\n",
    "    if pad_length > 0:\n",
    "        return sequence + [pad_token_id] * pad_length\n",
    "    else:\n",
    "        return sequence[:max_seq_length]\n",
    "\n",
    "\n",
    "def prepare_data(data, tokenizer, max_seq_length, pad_token_id):\n",
    "    \"\"\"Prepares data by tokenizing and padding sequences to max_seq_length.\"\"\"\n",
    "    # Tokenize and pad sequences\n",
    "    encoder_inputs = tokenizer(\n",
    "        data[\"source\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_seq_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    decoder_inputs = tokenizer(\n",
    "        data[\"target\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_seq_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Extract inputs and attention masks\n",
    "    encoder_input_ids = encoder_inputs[\"input_ids\"]\n",
    "    encoder_attention_mask = encoder_inputs[\"attention_mask\"]\n",
    "    decoder_input_ids = decoder_inputs[\"input_ids\"]\n",
    "    decoder_attention_mask = decoder_inputs[\"attention_mask\"]\n",
    "\n",
    "    return encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(english_sentences, telugu_sentences, max_seq_length):\n",
    "\n",
    "    \"\"\"\n",
    "    Create padding and look-ahead masks for a batch of sentences.\n",
    "    Args:\n",
    "        english_sentences: List of tokenized English sentences (with padding).\n",
    "        telugu_sentences: List of tokenized Telugu sentences (with padding).\n",
    "        max_seq_len: Maximum sequence length.\n",
    "    Returns:\n",
    "        encoder_mask: Encoder padding mask for self-attention.\n",
    "        decoder_self_attention_mask: Combined padding + look-ahead mask.\n",
    "        cross_attention_mask: Decoder-to-encoder padding mask.\n",
    "    \"\"\"\n",
    "    # calculate the sequnce length\n",
    "    eng_seq_length = [len(seq) for seq in english_sentences]\n",
    "    tel_seq_length = [len(seq) for seq in telugu_sentences]\n",
    "\n",
    "    # Create padding masks\n",
    "    encoder_padding_mask = create_padding_mask(sequence_lengths=eng_seq_length, max_seq_length= max_seq_length) # [batch_size, max_seq_len]\n",
    "    decoder_padding_mask = create_padding_mask(tel_seq_length, max_seq_length) # [batch_size, max_seq_len]\n",
    "\n",
    "    # Expand encoder padding mask to match the shape of input for multiheaded self attention\n",
    "    encoder_mask = encoder_padding_mask.unsqueeze(1).unsqueeze(2) # [batch_size, 1, 1, max_seq_len]\n",
    "\n",
    "    # Create look ahead mask\n",
    "    look_ahead_mask = create_look_ahead_mask(max_seq_length) # [max_seq_len, max_seq_len]\n",
    "\n",
    "    # Combine decoder padding mask with look ahead mask for decoder self attention\n",
    "    decoder_self_attention_mask = (look_ahead_mask.unsqueeze(0) | decoder_padding_mask.unsqueeze(1)) #[batch_size, max_seq_len, max_seq_len]\n",
    "\n",
    "    # Expand decoder padding mask to match for cross attention\n",
    "    cross_attention_mask = decoder_padding_mask.unsqueeze(1).unsqueeze(2) # [batch_size, 1, 1, max_seq_len]\n",
    "\n",
    "    # Apply negative inf values for the masked positions\n",
    "    encoder_mask = encoder_mask.float().masked_fill(mask= encoder_mask, value = NEG_INFTY)\n",
    "    decoder_self_attention_mask = decoder_self_attention_mask.float().masked_fill(mask = decoder_self_attention_mask, value=NEG_INFTY)\n",
    "    cross_attention_mask = cross_attention_mask.float().masked_fill(mask = cross_attention_mask, value= NEG_INFTY)\n",
    "\n",
    "    return encoder_mask, decoder_self_attention_mask, cross_attention_mask\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedTokenizer:\n",
    "    def __init__(self,\n",
    "                 english_vocab,\n",
    "                 telugu_vocab,\n",
    "                 max_seq_length, \n",
    "                 unk_token=UNK_TOKEN, \n",
    "                 padding_token=PADDING_TOKEN, \n",
    "                 start_token=START_TOKEN, \n",
    "                 end_token=END_TOKEN):\n",
    "        self.padding_token = padding_token\n",
    "        self.english_vocab = english_vocab\n",
    "        self.telugu_vocab = telugu_vocab\n",
    "        self.english_vocab_dict = {token: idx for idx, token in enumerate(english_vocab)}\n",
    "        self.telugu_vocab_dict = {token:idx for idx, token in enumerate(telugu_vocab)}\n",
    "        # create a common padding token for both the vocab\n",
    "        shared_padding_index = max(len(self.english_vocab_dict), len(self.telugu_vocab_dict)) + 1\n",
    "        self.english_vocab_dict[self.padding_token] = shared_padding_index\n",
    "        self.telugu_vocab_dict[self.padding_token] = shared_padding_index\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.unk_token = unk_token\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "\n",
    "\n",
    "        # Telugu-specific characters\n",
    "        self.diacritics = [\"ా\", \"ి\", \"ీ\", \"ు\", \"ూ\", \"ె\", \"ే\", \"ై\", \"ొ\", \"ో\", \"ౌ\", \"ం\", \"ః\"]\n",
    "        self.special_chars = [\"౦\", \"౧\", \"౨\", \"౩\", \"౪\", \"౫\", \"౬\", \"౭\", \"౮\", \"౯\"]\n",
    "\n",
    "    def word_decompose(self, word):\n",
    "        \"\"\"Decomposes a Telugu word into its constituent characters.\"\"\"\n",
    "        return list(word)\n",
    "\n",
    "    def reconstruct(self, tokens):\n",
    "        \"\"\"Reconstructs a word from its decomposed tokens.\"\"\"\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    # def tokenize(self, sentence, language, add_start_token=False, add_end_token=False):\n",
    "    #     \"\"\"\n",
    "    #     Tokenizes a sentence, applying language-specific rules.\n",
    "    #     Args:\n",
    "    #         sentence: The sentence to tokenize.\n",
    "    #         language: \"english\" or \"telugu\".\n",
    "    #         add_start_token: Whether to add a start token.\n",
    "    #         add_end_token: Whether to add an end token.\n",
    "    #     Returns:\n",
    "    #         tokens: A list of tokens with special tokens and padding.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     words = sentence.split()\n",
    "    #     tokens = []\n",
    "\n",
    "    #     if language == \"telugu\":\n",
    "    #         # Tokenize Telugu sentence\n",
    "    #         for word in words:\n",
    "    #             if word in self.telugu_vocab_dict:\n",
    "    #                 tokens.append(word)  # Direct match\n",
    "    #             else:\n",
    "    #                 # Decompose and tokenize at the character level\n",
    "    #                 decomposed = self.word_decompose(word)\n",
    "    #                 for char in decomposed:\n",
    "    #                     tokens.append(char if char in self.telugu_vocab_dict else self.unk_token)\n",
    "    #     elif language == \"english\":\n",
    "    #         # Tokenize English sentence (word-level only)\n",
    "    #         words = [word.lower() for word in words]\n",
    "    #         for word in words:\n",
    "    #             tokens.append(word if word in self.english_vocab_dict else self.unk_token)\n",
    "\n",
    "    #     # Add special tokens if specified\n",
    "    #     if add_start_token:\n",
    "    #         tokens.insert(0, self.start_token)\n",
    "    #     if add_end_token:\n",
    "    #         tokens.append(self.end_token)\n",
    "\n",
    "    #     # Add padding tokens to ensure uniform length\n",
    "    #     while len(tokens) < self.max_seq_length:\n",
    "    #         tokens.append(self.padding_token)\n",
    "\n",
    "    #     # Truncate if the length exceeds max_seq_length\n",
    "    #     tokens = tokens[:self.max_seq_length]\n",
    "\n",
    "    #     return tokens\n",
    "    \n",
    "    # def tokenize(self, sentence, language, add_start_token = False, add_end_token = False):\n",
    "    #     # if the input is a list of sentances, tokenize each sentence in the list\n",
    "    #     if isinstance(sentence, list):\n",
    "    #         tokens = []\n",
    "    #         for s in sentence:\n",
    "    #             tokens.append(self.tokenize_single_sentence(s, language, add_start_token, add_end_token))\n",
    "    #         return tokens\n",
    "    #     else:\n",
    "    #         return self.tokenize_single_sentence(sentence, language, add_start_token, add_end_token)\n",
    "\n",
    "\n",
    "    def tokenize(self, sentence, language, add_start_token=False, add_end_token=False):\n",
    "        \n",
    "        tokens = []\n",
    "        words = sentence.split()\n",
    "\n",
    "        if language == \"telugu\":\n",
    "            for word in words:\n",
    "                if word in self.telugu_vocab_dict:\n",
    "                    # Direct match\n",
    "                    tokens.append(word)\n",
    "                else:\n",
    "                    # Decompose the word\n",
    "                    decomposed = self.word_decompose(word)\n",
    "                    char_indices = []\n",
    "                    for char in decomposed:\n",
    "                        if char in self.telugu_vocab_dict:\n",
    "                            char_indices.append(self.telugu_vocab_dict[char])\n",
    "                        else:\n",
    "                            char_indices.append(self.telugu_vocab_dict[self.unk_token])\n",
    "\n",
    "                    # Check if all characters matched\n",
    "                    if all(idx != self.telugu_vocab_dict[self.unk_token] for idx in char_indices):\n",
    "                        # Reconstruct the word by combining indices (optional)\n",
    "                        tokens.extend(decomposed)  # Add decomposed characters as tokens\n",
    "                    else:\n",
    "                        # If even one character didn't match, return `[UNK]` for the word\n",
    "                        tokens.append(self.unk_token)\n",
    "\n",
    "        elif language == \"english\":\n",
    "            # English tokenization remains unchanged\n",
    "            words = [word.lower() for word in words]\n",
    "            for word in words:\n",
    "                tokens.append(word if word in self.english_vocab_dict else self.unk_token)\n",
    "\n",
    "        # Add special tokens\n",
    "        if add_start_token:\n",
    "            tokens.insert(0, self.start_token)\n",
    "        if add_end_token:\n",
    "            tokens.append(self.end_token)\n",
    "\n",
    "        # Add padding\n",
    "        while len(tokens) < self.max_seq_length:\n",
    "            tokens.append(self.padding_token)\n",
    "\n",
    "        # Truncate\n",
    "        tokens = tokens[:self.max_seq_length]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def encode(self, tokens, language):\n",
    "        \"\"\"Encode token indices into embedding\"\"\"\n",
    "\n",
    "        if language == \"english\":\n",
    "            vocab_dict = self.english_vocab_dict\n",
    "        elif language == \"telugu\":\n",
    "            vocab_dict = self.telugu_vocab_dict\n",
    "\n",
    "        return [vocab_dict.get(token, vocab_dict.get(self.unk_token)) for token in tokens]\n",
    "    \n",
    "    # def decode(self, indices, language):\n",
    "    #     \"\"\" Decodes indices back into tokens\"\"\"\n",
    "\n",
    "    #     if language == \"english\":\n",
    "    #         reversed_vocab_dict = {idx:token for token, idx in self.english_vocab_dict.items()}\n",
    "    #         tokens = [reversed_vocab_dict.get(idx, self.unk_token) for idx in indices]\n",
    "    #         # Join English tokens with spaces (skip special tokens)\n",
    "    #         return \" \".join([token for token in tokens if token not in [self.start_token, self.end_token, self.padding_token]]) \n",
    "\n",
    "    #     elif language == \"telugu\":\n",
    "    #         reverse_vocab_dict = {idx: token for token, idx in self.telugu_vocab_dict.items()}\n",
    "    #         tokens = [reverse_vocab_dict.get(idx, self.unk_token) for idx in indices]\n",
    "    #         # Reconstruct Telugu sentence\n",
    "    #         return self.reconstruct(tokens)\n",
    "        \n",
    "    def decode(self, indices, language):\n",
    "\n",
    "        \"\"\" Decodes indices back into tokens \"\"\"\n",
    "        if language == \"english\":\n",
    "           reversed_vocab_dict = {idx: token for token, idx in self.english_vocab_dict.items()}\n",
    "           tokens = [reversed_vocab_dict.get(idx, self.unk_token) for idx in indices]\n",
    "           # Join English tokens with spaces (skip special tokens)\n",
    "           return \" \".join([token for token in tokens if token not in [self.start_token, self.end_token, self.padding_token]])\n",
    "\n",
    "        elif language == \"telugu\":\n",
    "            reverse_vocab_dict = {idx: token for token, idx in self.telugu_vocab_dict.items()}\n",
    "            tokens = [reverse_vocab_dict.get(idx, self.unk_token) for idx in indices]\n",
    "            # Filter out special tokens\n",
    "            tokens = [token for token in tokens if token not in [self.start_token, self.end_token, self.padding_token]]\n",
    "            # Reconstruct with spaces between words\n",
    "            return \" \".join(tokens)  # Telugu-specific reconstruction\n",
    "        \n",
    "        \n",
    "\n",
    "    @property\n",
    "    def pad_token_index(self):\n",
    "        # Return the padding token index for the relevant vocabulary\n",
    "        if self.padding_token in self.english_vocab_dict:\n",
    "            return self.english_vocab_dict[self.padding_token]\n",
    "        elif self.padding_token in self.telugu_vocab_dict:\n",
    "            return self.telugu_vocab_dict[self.padding_token]\n",
    "        else:\n",
    "            raise ValueError(\"Padding token is not in either vocabulary.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom Dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextData(Dataset):\n",
    "    def __init__(self, english_sentences, telugu_sentences, tokenizer, max_seq_length, pad_token_idx):\n",
    "        super().__init__()\n",
    "        self.english_sentences = english_sentences\n",
    "        self.telugu_sentences = telugu_sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.pad_token_idx = pad_token_idx  # Padding token index shared across both vocabularies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        english_sentence = self.english_sentences[idx]\n",
    "        telugu_sentence = self.telugu_sentences[idx]\n",
    "\n",
    "        # Tokenize and pad\n",
    "        english_tokens = self.tokenizer.tokenize(\n",
    "            english_sentence, language=\"english\", add_start_token=False, add_end_token=False\n",
    "        )\n",
    "        telugu_tokens = self.tokenizer.tokenize(\n",
    "            telugu_sentence, language=\"telugu\", add_start_token=True, add_end_token=True\n",
    "        )\n",
    "\n",
    "        # Encode tokens to indices\n",
    "        english_indices = self.tokenizer.encode(english_tokens, language=\"english\")\n",
    "        telugu_indices = self.tokenizer.encode(telugu_tokens, language=\"telugu\")\n",
    "\n",
    "        # Pad sequences to max_seq_length\n",
    "        english_indices = self.pad_sequence(english_indices)\n",
    "        telugu_indices = self.pad_sequence(telugu_indices)\n",
    "\n",
    "        return torch.tensor(english_indices, dtype=torch.long), torch.tensor(telugu_indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "    def pad_sequence(self, tokens):\n",
    "        \"\"\"Pad or truncate tokens to max_seq_length.\"\"\"\n",
    "        if len(tokens) > self.max_seq_length:\n",
    "            tokens = tokens[:self.max_seq_length]  # Truncate if tokens exceed max_seq_length\n",
    "        elif len(tokens) < self.max_seq_length:\n",
    "            tokens += [self.pad_token_idx] * (self.max_seq_length - len(tokens))  # Pad to max_seq_length\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_english_vocab = [\n",
    "    \"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\", \n",
    "    \"hello\", \"this\", \"is\",\n",
    "    \"a\", \"test\", \"example\", \"sentence\", \n",
    "    \"how\", \"are\", \"you\", \"doing\", \n",
    "    \"today\", \"good\", \"morning\", \"night\"\n",
    "]\n",
    "\n",
    "sample_telugu_vocab = [\n",
    "    \"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\", \n",
    "    \"హలో\", \"ఇది\", \"ఒక\",\n",
    "    \"పరీక్ష\", \"ఉదాహరణ\", \"వాక్యం\", \n",
    "    \"ఎలా\", \"ఉన్నావు\", \"నువ్వు\", \n",
    "    \"చేస్తున్నావు\", \"ఈరోజు\", \n",
    "    \"శుభోదయం\", \"శుభరాత్రి\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "SOS_TOKEN = \"[SOS]\"\n",
    "EOS_TOKEN = \"[EOS]\"\n",
    "# Define max sequence length\n",
    "sample_MAX_SEQ_LENGTH = 20\n",
    "\n",
    "# Initialize tokenizer\n",
    "sample_tokenizer = UnifiedTokenizer(\n",
    "    english_vocab=sample_english_vocab,\n",
    "    telugu_vocab=sample_telugu_vocab,\n",
    "    max_seq_length=sample_MAX_SEQ_LENGTH,\n",
    "    unk_token=UNK_TOKEN,\n",
    "    padding_token=PAD_TOKEN,\n",
    "    start_token=SOS_TOKEN,\n",
    "    end_token=EOS_TOKEN\n",
    ")\n",
    "\n",
    "# Sample sentences for testing\n",
    "sample_english_sentence = 'Hello, how are you?'\n",
    "sample_telugu_sentence = 'హలో, మీరు ఎలా ఉన్నారు?'\n",
    "                          \n",
    "\n",
    "\n",
    "dataset = TextData(english_sentences= sample_english_sentence,\n",
    "                   telugu_sentences= sample_telugu_sentence,\n",
    "                   tokenizer= sample_tokenizer,\n",
    "                   max_seq_length= sample_MAX_SEQ_LENGTH,\n",
    "                   pad_token_idx=sample_tokenizer.pad_token_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokens = sample_tokenizer.tokenize(sample_english_sentence, language=\"english\", add_start_token=False, add_end_token=False)\n",
    "telugu_tokens = sample_tokenizer.tokenize(sample_telugu_sentence, language=\"telugu\", add_start_token=True, add_end_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu Tokens: ['[SOS]', '[UNK]', '[UNK]', 'ఎలా', '[UNK]', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Telugu Indices: [2, 1, 1, 10, 1, 3, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "English Indices [1, 11, 12, 1, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "English Tokens ['[UNK]', 'how', 'are', '[UNK]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Decoded Telugu: [UNK] [UNK] ఎలా [UNK]\n",
      "Decoded English: [UNK] how are [UNK]\n",
      "English indices: [1, 11, 12, 1, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "Telugu indices: [2, 1, 1, 10, 1, 3, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[857], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish indices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, english_indices)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTelugu indices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, telugu_indices)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoded English:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [sample_english_vocab[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m english_indices])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoded Telugu:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [sample_telugu_vocab[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m telugu_indices])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "english_indices = sample_tokenizer.encode(english_tokens, language=\"english\")\n",
    "telugu_indices = sample_tokenizer.encode(telugu_tokens, language=\"telugu\")\n",
    "\n",
    "print(\"Telugu Tokens:\", telugu_tokens)\n",
    "print(\"Telugu Indices:\", telugu_indices)\n",
    "print(\"English Indices\", english_indices)\n",
    "print(\"English Tokens\", english_tokens)\n",
    "print(\"Decoded Telugu:\", sample_tokenizer.decode(telugu_indices, language=\"telugu\"))\n",
    "print(\"Decoded English:\", sample_tokenizer.decode(english_indices, language=\"english\"))\n",
    "\n",
    "# Verify padding and decoding\n",
    "assert sample_tokenizer.pad_token_index in english_indices\n",
    "assert sample_tokenizer.pad_token_index in telugu_indices\n",
    "\n",
    "for english, telugu in dataset:\n",
    "    print(\"English indices:\", english_indices)\n",
    "    print(\"Telugu indices:\", telugu_indices)\n",
    "    print(\"Decoded English:\", [sample_english_vocab[idx] for idx in english_indices])\n",
    "    print(\"Decoded Telugu:\", [sample_telugu_vocab[idx] for idx in telugu_indices])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_english_sentence = \"This is a hello sentence\"\n",
    "# sample_sentence_tokens = tokenizer.tokenize(sample_english_sentence, language=\"english\", add_start_token=True, add_end_token=True)\n",
    "# print(sample_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 32\n",
    "max_seq_length = 200\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabularies\n",
    "sample_english_vocab = [\n",
    "    \"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\", \n",
    "    \"hello\", \"this\", \"is\",\n",
    "    \"a\", \"test\", \"example\", \"sentence\", \n",
    "    \"how\", \"are\", \"you\", \"doing\", \n",
    "    \"today\", \"good\", \"morning\", \"night\"\n",
    "]\n",
    "\n",
    "sample_telugu_vocab = [\n",
    "    \"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\", \n",
    "    \"హలో\", \"ఇది\", \"ఒక\",\n",
    "    \"పరీక్ష\", \"ఉదాహరణ\", \"వాక్యం\", \n",
    "    \"ఎలా\", \"ఉన్నావు\", \"నువ్వు\", \n",
    "    \"చేస్తున్నావు\", \"ఈరోజు\", \n",
    "    \"శుభోదయం\", \"శుభరాత్రి\"\n",
    "]\n",
    "\n",
    "# Define special tokens\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "SOS_TOKEN = \"[SOS]\"\n",
    "EOS_TOKEN = \"[EOS]\"\n",
    "\n",
    "# Define max sequence length\n",
    "sample_MAX_SEQ_LENGTH = 20\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = UnifiedTokenizer(\n",
    "    english_vocab=sample_english_vocab,\n",
    "    telugu_vocab=sample_telugu_vocab,\n",
    "    max_seq_length=sample_MAX_SEQ_LENGTH,\n",
    "    unk_token=UNK_TOKEN,\n",
    "    padding_token=PAD_TOKEN,\n",
    "    start_token=SOS_TOKEN,\n",
    "    end_token=EOS_TOKEN\n",
    ")\n",
    "\n",
    "# Sample sentences for testing\n",
    "sample_english_sentence = [\"hello world this is a test\", \"hello world this is a test\", \"hello world this is a test\"]\n",
    "sample_telugu_sentence = [\"హలో ప్రపంచం ఇది ఒక పరీక్ష\", \"హలో ప్రపంచం ఇది ఒక పరీక్ష\", \"హలో ప్రపంచం ఇది ఒక పరీక్ష\"]\n",
    "\n",
    "dataset = TextData(english_sentences= sample_english_sentence,\n",
    "                   telugu_sentences= sample_telugu_sentence,\n",
    "                   tokenizer= tokenizer,\n",
    "                   max_seq_length= sample_MAX_SEQ_LENGTH,\n",
    "                   pad_token_idx=tokenizer.pad_token_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, '[SOS]', '[EOS]', '[UNK]', 20)"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_index, tokenizer.start_token, tokenizer.end_token, tokenizer.unk_token, dataset.pad_token_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English indices: tensor([ 4,  1,  5,  6,  7,  8, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20])\n",
      "Telugu indices: tensor([ 2,  4,  1,  5,  6,  7,  3, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[860], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish indices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, english)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTelugu indices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, telugu)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoded English:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [sample_english_vocab[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m english])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoded Telugu:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [sample_telugu_vocab[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m telugu])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Test the dataset\n",
    "for english, telugu in dataset:\n",
    "    print(\"English indices:\", english)\n",
    "    print(\"Telugu indices:\", telugu)\n",
    "    print(\"Decoded English:\", [sample_english_vocab[idx] for idx in english])\n",
    "    print(\"Decoded Telugu:\", [sample_telugu_vocab[idx] for idx in telugu])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_dataloader = DataLoader(\n",
    "    dataset, batch_size=32, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[862], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize English\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m english_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sample_english_sentence, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, add_end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish Tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, english_tokens)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Encode English\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[827], line 97\u001b[0m, in \u001b[0;36mUnifiedTokenizer.tokenize\u001b[0;34m(self, sentence, language, add_start_token, add_end_token)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence, language, add_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     96\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 97\u001b[0m     words \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtelugu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize English\n",
    "english_tokens = tokenizer.tokenize(sample_english_sentence, language=\"english\", add_start_token=True, add_end_token=True)\n",
    "print(\"English Tokens:\", english_tokens)\n",
    "\n",
    "# Encode English\n",
    "english_encoded = tokenizer.encode(english_tokens, language=\"english\")\n",
    "print(\"Encoded English:\", english_encoded)\n",
    "\n",
    "# Decode English\n",
    "english_decoded = tokenizer.decode(english_encoded, language=\"english\")\n",
    "print(\"Decoded English:\", english_decoded)\n",
    "\n",
    "# Tokenize Telugu\n",
    "telugu_tokens = tokenizer.tokenize(sample_telugu_sentence, language=\"telugu\", add_start_token=True, add_end_token=True)\n",
    "print(\"Telugu Tokens:\", telugu_tokens)\n",
    "\n",
    "# Encode Telugu\n",
    "telugu_encoded = tokenizer.encode(telugu_tokens, language=\"telugu\")\n",
    "print(\"Encoded Telugu:\", telugu_encoded)\n",
    "\n",
    "# Decode Telugu\n",
    "telugu_decoded = tokenizer.decode(telugu_encoded, language=\"telugu\")\n",
    "print(\"Decoded Telugu:\", telugu_decoded)\n",
    "\n",
    "# Define embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(english_vocab + telugu_vocab), embedding_dim=d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1283, 1012, 200, 2423581, 2423581)"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_vocab), len(telugu_vocab), max_seq_length, len(valid_english_sentences), len(valid_telugu_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Have you heard about Foie gras? ',\n",
       "  'I never thought of acting in films. ',\n",
       "  'Installed Software '],\n",
       " ['ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా? ',\n",
       "  'సూర్య సినిమాల్లో నటించాలని ఎప్పుడూ అనుకోలేదు. ',\n",
       "  'స్థాపించబడిన సాఫ్ట్\\u200dవేర్ '])"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_english_sentences = valid_english_sentences[:1000]\n",
    "valid_telugu_sentences = valid_telugu_sentences[:1000]\n",
    "valid_english_sentences[:3], valid_telugu_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialzing for Tokenizer and Dataset\n",
    "\n",
    "final_tokenizer = UnifiedTokenizer(english_vocab= sample_english_vocab, \n",
    "                             telugu_vocab= sample_telugu_vocab, \n",
    "                             max_seq_length= max_seq_length,\n",
    "                             unk_token= UNK_TOKEN,\n",
    "                             padding_token= PADDING_TOKEN,\n",
    "                             start_token= START_TOKEN,\n",
    "                             end_token= END_TOKEN)\n",
    "\n",
    "final_dataset = TextData(english_sentences= sample_english_sentence,\n",
    "                   telugu_sentences= sample_telugu_sentence,\n",
    "                   tokenizer= sample_tokenizer,\n",
    "                   max_seq_length= max_seq_length,\n",
    "                   pad_token_idx=final_tokenizer.pad_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    final_dataset, batch_size=32, drop_last=True\n",
    ")\n",
    "\n",
    "for english_batch, telugu_batch in train_dataloader:\n",
    "    assert english_batch.shape[0] == telugu_batch.shape[0] == 32, \"Batch size mismatch in dataloader!\"\n",
    "\n",
    "\n",
    "# Initiliaze Position Encoder\n",
    "position_encoder = PositionEncoding(d_model= d_model, max_seq_length= max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "for english_batch, telugu_batch in train_dataloader:\n",
    "    english_indices = [sample_tokenizer.encode(tokens, language=\"english\") for tokens in english_batch]\n",
    "    telugu_indices = [sample_tokenizer.encode(tokens, language=\"telugu\") for tokens in telugu_batch]\n",
    "    print(f\"English indices batch size: {len(english_indices)}\")\n",
    "    print(f\"Telugu indices batch size: {len(telugu_indices)}\")\n",
    "    print(f\"Max lengths: English={max(len(seq) for seq in english_indices)}, Telugu={max(len(seq) for seq in telugu_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(english_vocab + telugu_vocab), embedding_dim=d_model)\n",
    "\n",
    "\n",
    "for english_batch, telugu_batch in train_dataloader:\n",
    "    # Encode tokens to indices\n",
    "    english_indices = [sample_tokenizer.encode(tokens, language=\"english\") for tokens in english_batch]\n",
    "    telugu_indices = [sample_tokenizer.encode(tokens, language=\"telugu\") for tokens in telugu_batch]\n",
    "    \n",
    "    # Convert indices into torch tensor\n",
    "    english_tensor = torch.tensor(english_indices, dtype=torch.long)\n",
    "    telugu_tensor = torch.tensor(telugu_indices, dtype=torch.long)\n",
    "\n",
    "    print(f\"English Tensor Shape: {english_tensor.shape}\")\n",
    "    print(f\"Telugu Tensor Shape: {telugu_tensor.shape}\")\n",
    "\n",
    "    # Add embedding layer (convert indices to embeddings)\n",
    "    english_embedded = embedding_layer(english_tensor)  # Shape: [batch_size, seq_len, d_model]\n",
    "    telugu_embedded = embedding_layer(telugu_tensor)  # Shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "    print(f\"Embedded English Shape: {english_embedded.shape}\")  # Expected: [batch_size, seq_len, d_model]\n",
    "    print(f\"Embedded Telugu Shape: {telugu_embedded.shape}\") \n",
    "\n",
    "    # Add Position embeddings\n",
    "    english_position_encoded_tensor = position_encoder(english_embedded)\n",
    "    telugu_position_encoded_tensor = position_encoder(telugu_embedded)\n",
    "\n",
    "    print(f\"Position Encoded English Tensor Shape: {english_position_encoded_tensor.shape}\")\n",
    "    print(f\"Position Encoded Telugu Tensor Shape: {telugu_position_encoded_tensor.shape}\")\n",
    "\n",
    "    # Create Masks\n",
    "    encoder_mask, decoder_self_attention_mask, cross_attention_mask = create_masks(english_sentences=english_tensor,\n",
    "                                                                                   telugu_sentences=telugu_tensor,\n",
    "                                                                                   max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 1, 200]),\n",
       " torch.Size([32, 200, 200]),\n",
       " torch.Size([32, 1, 1, 200]))"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_mask.shape, decoder_self_attention_mask.shape, cross_attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 200]),\n",
       " torch.Size([32, 200]),\n",
       " torch.Size([32, 200, 512]),\n",
       " torch.Size([32, 200, 512]),\n",
       " torch.Size([32, 200, 512]),\n",
       " torch.Size([32, 200, 512]))"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tensor.shape, telugu_tensor.shape, english_embedded.shape, telugu_embedded.shape, english_position_encoded_tensor.shape, telugu_position_encoded_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Token Index: 20\n",
      "Unique Indices in English Tensor: tensor([1282])\n",
      "Unique Indices in Telugu Tensor: tensor([1011])\n"
     ]
    }
   ],
   "source": [
    "print(\"Padding Token Index:\", sample_tokenizer.pad_token_index)\n",
    "print(\"Unique Indices in English Tensor:\", torch.unique(english_tensor[20]))\n",
    "print(\"Unique Indices in Telugu Tensor:\", torch.unique(telugu_tensor[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tensor Sample (first batch):\n",
      "tensor([1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282,\n",
      "        1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282])\n",
      "tensor([1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011,\n",
      "        1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011])\n"
     ]
    }
   ],
   "source": [
    "print(\"English Tensor Sample (first batch):\")\n",
    "print(english_tensor[1])  # Check for presence of padding tokens\n",
    "print(telugu_tensor[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Mask [1]:\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "Cross Attention Mask [1]:\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder Mask [1]:\")\n",
    "print(encoder_mask[1])  # This will show the mask applied for the first sequence\n",
    "\n",
    "print(\"Cross Attention Mask [1]:\")\n",
    "print(cross_attention_mask[1])  # Same for cross attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Indices: tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# Debugging mask logic\n",
    "padding_indices = (english_tensor == tokenizer.pad_token_index)  # Compare with padding token index\n",
    "print(\"Padding Indices:\", padding_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Token Index: 20\n",
      "Unique Indices in Tensor: tensor([1282])\n"
     ]
    }
   ],
   "source": [
    "print(\"Padding Token Index:\", tokenizer.pad_token_index)\n",
    "print(\"Unique Indices in Tensor:\", torch.unique(english_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Indices Shape: torch.Size([32, 200])\n",
      "Number of Padding Tokens: 0\n"
     ]
    }
   ],
   "source": [
    "padding_indices = (english_tensor == tokenizer.pad_token_index)  # Boolean mask for padding\n",
    "print(\"Padding Indices Shape:\", padding_indices.shape)\n",
    "print(\"Number of Padding Tokens:\", padding_indices.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Mask Shape: torch.Size([32, 1, 1, 200])\n",
      "Sample Encoder Mask: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "encoder_mask = (~padding_indices).unsqueeze(1).unsqueeze(2).to(torch.float32)\n",
    "print(\"Encoder Mask Shape:\", encoder_mask.shape)\n",
    "print(\"Sample Encoder Mask:\", encoder_mask[0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Attention Mask Shape: torch.Size([32, 1, 200, 1])\n",
      "Sample Cross Attention Mask: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "cross_attention_mask = (~padding_indices).unsqueeze(1).unsqueeze(3).to(torch.float32)\n",
    "print(\"Cross Attention Mask Shape:\", cross_attention_mask.shape)\n",
    "print(\"Sample Cross Attention Mask:\", cross_attention_mask[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 0 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 1 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 1 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 2 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 2 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 3 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 3 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 4 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 4 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 5 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 5 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 6 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 6 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 7 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 7 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 8 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 8 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 9 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 9 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 10 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 10 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 11 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 11 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 12 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 12 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 13 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 13 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 14 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 14 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 15 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 15 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 16 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 16 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 17 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 17 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 18 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 18 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 19 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 19 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 20 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 20 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 21 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 21 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 22 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 22 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 23 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 23 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 24 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 24 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 25 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 25 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 26 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 26 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 27 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 27 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 28 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 28 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 29 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 29 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 30 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 30 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n",
      "Batch 31 Encoder Mask:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "Batch 31 Cross Attention Mask:\n",
      "torch.Size([1, 200, 1])\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    print(f\"Batch {i} Encoder Mask:\")\n",
    "    print(encoder_mask[i])\n",
    "    print(f\"Batch {i} Cross Attention Mask:\")\n",
    "    print(cross_attention_mask[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Mask Shape: torch.Size([32, 1, 1, 200])\n",
      "Cross Attention Mask Shape: torch.Size([32, 1, 200, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder Mask Shape:\", encoder_mask.shape)  # Expected: [32, 1, 1, 200]\n",
    "print(\"Cross Attention Mask Shape:\", cross_attention_mask.shape)  # Expected: [32, 1, 200, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared Indices (should only include padding token index): set()\n"
     ]
    }
   ],
   "source": [
    "english_unique = torch.unique(english_tensor)\n",
    "telugu_unique = torch.unique(telugu_tensor)\n",
    "shared_indices = set(english_unique.tolist()) & set(telugu_unique.tolist())\n",
    "print(\"Shared Indices (should only include padding token index):\", shared_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared Tokens in English Vocabulary:\n",
      "Index 2: Unknown\n",
      "Index 1146: Unknown\n",
      "Index 95: Unknown\n",
      "\n",
      "Shared Tokens in Telugu Vocabulary:\n",
      "Index 2: Unknown\n",
      "Index 1146: Unknown\n",
      "Index 95: Unknown\n"
     ]
    }
   ],
   "source": [
    "shared_indices = {2, 1146, 95}  # The indices you've identified\n",
    "\n",
    "print(\"Shared Tokens in English Vocabulary:\")\n",
    "for idx in shared_indices:\n",
    "    print(f\"Index {idx}: {tokenizer.english_vocab_dict.get(idx, 'Unknown')}\")\n",
    "\n",
    "print(\"\\nShared Tokens in Telugu Vocabulary:\")\n",
    "for idx in shared_indices:\n",
    "    print(f\"Index {idx}: {tokenizer.telugu_vocab_dict.get(idx, 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English OOV Token Index: 1\n",
      "Telugu OOV Token Index: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"English OOV Token Index:\", tokenizer.english_vocab_dict.get(tokenizer.unk_token, \"Not Found\"))\n",
    "print(\"Telugu OOV Token Index:\", tokenizer.telugu_vocab_dict.get(tokenizer.unk_token, \"Not Found\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw English Tokens for Sample 22: [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "Raw Telugu Tokens for Sample 22: [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "# Check the raw tokenized outputs before conversion to indices\n",
    "print(\"Raw English Tokens for Sample 22:\", tokenizer.decode(english_tensor[22].tolist(), language=\"english\"))\n",
    "print(\"Raw Telugu Tokens for Sample 22:\", tokenizer.decode(telugu_tensor[22].tolist(), language=\"telugu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Transformer Class and defining Training Loop and the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 num_heads, \n",
    "                 ffn_hidden, \n",
    "                 drop_prob, \n",
    "                 num_layers, \n",
    "                 tokenizer, \n",
    "                 max_seq_length):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderBlock(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
    "        self.decoder = DecoderLayer(d_model, ffn_hidden, drop_prob, num_heads)\n",
    "        self.output_layer = nn.Linear(d_model, len(tokenizer.telugu_vocab))  # Project to Telugu vocab size\n",
    "\n",
    "    def forward(self, \n",
    "                src_embedded,  # Position-encoded embeddings of the source (English) sequence\n",
    "                tgt_embedded,  # Position-encoded embeddings of the target (Telugu) sequence\n",
    "                encoder_mask, \n",
    "                decoder_self_attention_mask, \n",
    "                cross_attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            src_embedded: Position-encoded source embeddings (batch_size, seq_len_src, d_model)\n",
    "            tgt_embedded: Position-encoded target embeddings (batch_size, seq_len_tgt, d_model)\n",
    "            encoder_mask: Mask for the encoder (batch_size, 1, 1, seq_len_src)\n",
    "            decoder_self_attention_mask: Mask for decoder's self-attention (batch_size, seq_len_tgt, seq_len_tgt)\n",
    "            cross_attention_mask: Mask for decoder's cross-attention (batch_size, 1, 1, seq_len_src)\n",
    "\n",
    "        Returns:\n",
    "            logits: Unnormalized predictions for the target vocabulary (batch_size, seq_len_tgt, vocab_size)\n",
    "        \"\"\"\n",
    "        # Pass through the encoder\n",
    "        encoder_output = self.encoder(src_embedded, mask=encoder_mask)  # Shape: (batch_size, seq_len_src, d_model)\n",
    "\n",
    "        # Pass through the decoder\n",
    "        decoder_output = self.decoder(\n",
    "            x=encoder_output,  # Encoder output\n",
    "            y=tgt_embedded,    # Target embeddings\n",
    "            decoder_self_attention_mask=decoder_self_attention_mask,\n",
    "            cross_attention_mask=cross_attention_mask\n",
    "        )  # Shape: (batch_size, seq_len_tgt, d_model)\n",
    "\n",
    "        # Project to the output vocabulary space\n",
    "        logits = self.output_layer(decoder_output)  # Shape: (batch_size, seq_len_tgt, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the model\n",
    "model = Transformer(d_model=d_model,\n",
    "                    num_heads=num_heads,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    drop_prob=drop_prob,\n",
    "                    num_layers=num_layers,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_seq_length=max_seq_length)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define loss function (CrossEntropyLoss for sequence-to-sequence)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_index, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 32\n",
    "max_seq_length = 200\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embedded = english_position_encoded_tensor  # Shape: [batch_size, seq_len, d_model]\n",
    "tgt_embedded = telugu_position_encoded_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1, 200])\n",
      "torch.Size([32, 200, 200])\n",
      "torch.Size([32, 1, 200, 1])\n",
      "----- Attention Layer 1 ----\n",
      "x.shape = torch.Size([32, 200, 512])\n",
      "x.size() = torch.Size([32, 200, 512])\n",
      "x after going through qkv layer - torch.Size([32, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([32, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([32, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before broadcasting: torch.Size([32, 1, 1, 200])\n",
      "Scaled tensor shape before adding mask: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before adding: torch.Size([32, 1, 1, 200])\n",
      "Scaled tensor shape after adding mask: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Now we get the Attention Scores and New Values\n",
      "Values shape we get from scaled dot product attention is - torch.Size([32, 8, 200, 64])\n",
      "Now we concatinate the values received from scaled dot product.\n",
      "New Values from Multiheaded attention output shape is - torch.Size([32, 200, 512])\n",
      "Final Output shape from multiheaded attention heads - torch.Size([32, 200, 512])\n",
      "Output shape from Multi Head Attention - torch.Size([32, 200, 512])\n",
      "---- Dropout 1 ----\n",
      "x shape after passing through dropout Layer 1 - torch.Size([32, 200, 512])\n",
      "----- Add and Normalizer 1 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "x shape after passing through layer normalizer 1 - torch.Size([32, 200, 512])\n",
      "---- Attention 2 through ffn ----\n",
      "x after first linear layer: torch.Size([32, 200, 2048])\n",
      "x after activation: torch.Size([32, 200, 2048])\n",
      "x after dropout: torch.Size([32, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
      "----- Dropout 2 -----\n",
      "----- Add and Normalizer 2 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "----- Attention Layer 1 ----\n",
      "x.shape = torch.Size([32, 200, 512])\n",
      "x.size() = torch.Size([32, 200, 512])\n",
      "x after going through qkv layer - torch.Size([32, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([32, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([32, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before broadcasting: torch.Size([32, 1, 1, 200])\n",
      "Scaled tensor shape before adding mask: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before adding: torch.Size([32, 1, 1, 200])\n",
      "Scaled tensor shape after adding mask: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Now we get the Attention Scores and New Values\n",
      "Values shape we get from scaled dot product attention is - torch.Size([32, 8, 200, 64])\n",
      "Now we concatinate the values received from scaled dot product.\n",
      "New Values from Multiheaded attention output shape is - torch.Size([32, 200, 512])\n",
      "Final Output shape from multiheaded attention heads - torch.Size([32, 200, 512])\n",
      "Output shape from Multi Head Attention - torch.Size([32, 200, 512])\n",
      "---- Dropout 1 ----\n",
      "x shape after passing through dropout Layer 1 - torch.Size([32, 200, 512])\n",
      "----- Add and Normalizer 1 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "x shape after passing through layer normalizer 1 - torch.Size([32, 200, 512])\n",
      "---- Attention 2 through ffn ----\n",
      "x after first linear layer: torch.Size([32, 200, 2048])\n",
      "x after activation: torch.Size([32, 200, 2048])\n",
      "x after dropout: torch.Size([32, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
      "----- Dropout 2 -----\n",
      "----- Add and Normalizer 2 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "----- Attention Layer 1 ----\n",
      "x.shape = torch.Size([32, 200, 512])\n",
      "x.size() = torch.Size([32, 200, 512])\n",
      "x after going through qkv layer - torch.Size([32, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([32, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([32, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before broadcasting: torch.Size([32, 1, 1, 200])\n",
      "Scaled tensor shape before adding mask: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before adding: torch.Size([32, 1, 1, 200])\n",
      "Scaled tensor shape after adding mask: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Now we get the Attention Scores and New Values\n",
      "Values shape we get from scaled dot product attention is - torch.Size([32, 8, 200, 64])\n",
      "Now we concatinate the values received from scaled dot product.\n",
      "New Values from Multiheaded attention output shape is - torch.Size([32, 200, 512])\n",
      "Final Output shape from multiheaded attention heads - torch.Size([32, 200, 512])\n",
      "Output shape from Multi Head Attention - torch.Size([32, 200, 512])\n",
      "---- Dropout 1 ----\n",
      "x shape after passing through dropout Layer 1 - torch.Size([32, 200, 512])\n",
      "----- Add and Normalizer 1 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "x shape after passing through layer normalizer 1 - torch.Size([32, 200, 512])\n",
      "---- Attention 2 through ffn ----\n",
      "x after first linear layer: torch.Size([32, 200, 2048])\n",
      "x after activation: torch.Size([32, 200, 2048])\n",
      "x after dropout: torch.Size([32, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
      "----- Dropout 2 -----\n",
      "----- Add and Normalizer 2 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "----- Attention Layer 1 ----\n",
      "x.shape = torch.Size([32, 200, 512])\n",
      "x.size() = torch.Size([32, 200, 512])\n",
      "x after going through qkv layer - torch.Size([32, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([32, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([32, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before broadcasting: torch.Size([32, 1, 1, 200])\n",
      "Scaled tensor shape before adding mask: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before adding: torch.Size([32, 1, 1, 200])\n",
      "Scaled tensor shape after adding mask: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Now we get the Attention Scores and New Values\n",
      "Values shape we get from scaled dot product attention is - torch.Size([32, 8, 200, 64])\n",
      "Now we concatinate the values received from scaled dot product.\n",
      "New Values from Multiheaded attention output shape is - torch.Size([32, 200, 512])\n",
      "Final Output shape from multiheaded attention heads - torch.Size([32, 200, 512])\n",
      "Output shape from Multi Head Attention - torch.Size([32, 200, 512])\n",
      "---- Dropout 1 ----\n",
      "x shape after passing through dropout Layer 1 - torch.Size([32, 200, 512])\n",
      "----- Add and Normalizer 1 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "x shape after passing through layer normalizer 1 - torch.Size([32, 200, 512])\n",
      "---- Attention 2 through ffn ----\n",
      "x after first linear layer: torch.Size([32, 200, 2048])\n",
      "x after activation: torch.Size([32, 200, 2048])\n",
      "x after dropout: torch.Size([32, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
      "----- Dropout 2 -----\n",
      "----- Add and Normalizer 2 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "----- Attention Layer 1 ----\n",
      "x.shape = torch.Size([32, 200, 512])\n",
      "x.size() = torch.Size([32, 200, 512])\n",
      "x after going through qkv layer - torch.Size([32, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([32, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([32, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([32, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([32, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before broadcasting: torch.Size([32, 1, 1, 200])\n",
      "Scaled tensor shape before adding mask: torch.Size([32, 8, 200, 200])\n",
      "Mask shape before adding: torch.Size([32, 1, 1, 200])\n",
      "Scaled tensor shape after adding mask: torch.Size([32, 8, 200, 200])\n",
      "Attention (after softmax) shape: torch.Size([32, 8, 200, 200])\n",
      "Shape of scaled tensor before matmul with values: torch.Size([32, 8, 200, 200])\n",
      "Shape of value tensor (v): torch.Size([32, 8, 200, 64])\n",
      "Output values shape: torch.Size([32, 8, 200, 64])\n",
      "Now we get the Attention Scores and New Values\n",
      "Values shape we get from scaled dot product attention is - torch.Size([32, 8, 200, 64])\n",
      "Now we concatinate the values received from scaled dot product.\n",
      "New Values from Multiheaded attention output shape is - torch.Size([32, 200, 512])\n",
      "Final Output shape from multiheaded attention heads - torch.Size([32, 200, 512])\n",
      "Output shape from Multi Head Attention - torch.Size([32, 200, 512])\n",
      "---- Dropout 1 ----\n",
      "x shape after passing through dropout Layer 1 - torch.Size([32, 200, 512])\n",
      "----- Add and Normalizer 1 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "x shape after passing through layer normalizer 1 - torch.Size([32, 200, 512])\n",
      "---- Attention 2 through ffn ----\n",
      "x after first linear layer: torch.Size([32, 200, 2048])\n",
      "x after activation: torch.Size([32, 200, 2048])\n",
      "x after dropout: torch.Size([32, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
      "----- Dropout 2 -----\n",
      "----- Add and Normalizer 2 ---\n",
      "Shape of the Mean after preserving the dimensions - torch.Size([32, 200, 1])\n",
      "Standard Deviation is torch.Size([32, 200, 1])\n",
      "Normalized value is Multiplied and Added with Learnable Parameters Gamma and Beta\n",
      "Shape of gamma and beta are - torch.Size([512]) and torch.Size([512])\n",
      "Layer Normalized Output shape is torch.Size([32, 200, 512])\n",
      "----ENTERING MASKED SELF ATTENTION----\n",
      "x.shape = torch.Size([31, 200, 512])\n",
      "x.size() = torch.Size([31, 200, 512])\n",
      "x after going through qkv layer - torch.Size([31, 200, 1536])\n",
      "--- Entering MultiHeadedAttention ---\n",
      "qkv reshaped shape is torch.Size([31, 200, 8, 192])\n",
      "QKV Permuted Shape is - torch.Size([31, 8, 200, 192])\n",
      "--- Dividing QKV into individual tensors ---\n",
      "Now q, k, v tensors are passed into the Attention Calculation\n",
      "Initial Query shape: torch.Size([31, 8, 200, 64])\n",
      "Initial Key shape: torch.Size([31, 8, 200, 64])\n",
      "Initial Value shape: torch.Size([31, 8, 200, 64])\n",
      "Scaled dot product (q * k^T) shape: torch.Size([31, 8, 200, 200])\n",
      "Mask shape before broadcasting: torch.Size([32, 200, 200])\n",
      "Mask has 3 dimensions, adding additional singleton dimensions for multi-head attention.\n",
      "Mask shape after unsqueeze(1): torch.Size([32, 1, 200, 200])\n",
      "Scaled tensor shape before adding mask: torch.Size([31, 8, 200, 200])\n",
      "Mask shape before adding: torch.Size([32, 1, 200, 200])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (31) must match the size of tensor b (32) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[477], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     16\u001b[0m     src_embedded,                \u001b[38;5;66;03m# Position-encoded source embeddings\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     tgt_embedded[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],           \u001b[38;5;66;03m# Exclude the last token for teacher forcing\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     encoder_mask,\n\u001b[1;32m     19\u001b[0m     decoder_self_attention_mask,\n\u001b[1;32m     20\u001b[0m     cross_attention_mask\n\u001b[1;32m     21\u001b[0m     )  \u001b[38;5;66;03m# logits: [batch_size, seq_len - 1, vocab_size]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Reshape logits and target for loss calculation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# [batch_size * seq_len - 1, vocab_size]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[473], line 38\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src_embedded, tgt_embedded, encoder_mask, decoder_self_attention_mask, cross_attention_mask)\u001b[0m\n\u001b[1;32m     35\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src_embedded, mask\u001b[38;5;241m=\u001b[39mencoder_mask)  \u001b[38;5;66;03m# Shape: (batch_size, seq_len_src, d_model)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Pass through the decoder\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m     39\u001b[0m     x\u001b[38;5;241m=\u001b[39mencoder_output,  \u001b[38;5;66;03m# Encoder output\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     y\u001b[38;5;241m=\u001b[39mtgt_embedded,    \u001b[38;5;66;03m# Target embeddings\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     decoder_self_attention_mask\u001b[38;5;241m=\u001b[39mdecoder_self_attention_mask,\n\u001b[1;32m     42\u001b[0m     cross_attention_mask\u001b[38;5;241m=\u001b[39mcross_attention_mask\n\u001b[1;32m     43\u001b[0m )  \u001b[38;5;66;03m# Shape: (batch_size, seq_len_tgt, d_model)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Project to the output vocabulary space\u001b[39;00m\n\u001b[1;32m     46\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(decoder_output)  \u001b[38;5;66;03m# Shape: (batch_size, seq_len_tgt, vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[313], line 17\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, y, decoder_self_attention_mask, cross_attention_mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m residual_y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;66;03m# 30 x 200 x 512\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----ENTERING MASKED SELF ATTENTION----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention(y, mask \u001b[38;5;241m=\u001b[39m decoder_self_attention_mask)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---ENTERING DROPOUT 1---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(y) \u001b[38;5;66;03m# 30 x 200 x 512\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[304], line 26\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv_permute\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNow q, k, v tensors are passed into the Attention Calculation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m attention, values \u001b[38;5;241m=\u001b[39m scaled_dot_product(q, k, v, mask \u001b[38;5;241m=\u001b[39m mask)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNow we get the Attention Scores and New Values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValues shape we get from scaled dot product attention is - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[396], line 23\u001b[0m, in \u001b[0;36mscaled_dot_product\u001b[0;34m(q, k, v, mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScaled tensor shape before adding mask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscaled\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask shape before adding: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     scaled \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mask  \u001b[38;5;66;03m# Broadcast the mask to the scaled tensor\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScaled tensor shape after adding mask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscaled\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Apply softmax to the scaled tensor\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (31) must match the size of tensor b (32) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 3  # You can adjust this based on your requirements\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for english_batch, telugu_batch in train_dataloader:\n",
    "        \n",
    "        print(encoder_mask.shape)\n",
    "        print(decoder_self_attention_mask.shape)\n",
    "        print(cross_attention_mask.shape)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(\n",
    "            src_embedded,                # Position-encoded source embeddings\n",
    "            tgt_embedded[:-1],           # Exclude the last token for teacher forcing\n",
    "            encoder_mask,\n",
    "            decoder_self_attention_mask,\n",
    "            cross_attention_mask\n",
    "            )  # logits: [batch_size, seq_len - 1, vocab_size]\n",
    "\n",
    "        # Reshape logits and target for loss calculation\n",
    "        logits = logits.reshape(-1, logits.size(-1))  # [batch_size * seq_len - 1, vocab_size]\n",
    "        target = telugu_tensor[1:].reshape(-1)  # Shift target to exclude <sos>, [batch_size * seq_len - 1]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
